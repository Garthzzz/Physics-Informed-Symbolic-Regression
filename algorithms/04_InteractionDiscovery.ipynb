{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_InteractionDiscovery - Physics-SR Framework v4.1\n",
    "\n",
    "## Stage 1.4: Adaptive Interaction Discovery with TreeSHAP\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Contact:** zz3239@columbia.edu  \n",
    "**Date:** January 2026  \n",
    "**Version:** 4.1.1 (TreeSHAP-based Adaptive Strategy)\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Discover high-order feature interactions using an adaptive strategy:\n",
    "- **Low-dimensional (n_features <= 10):** Direct enumeration of all pairwise interactions\n",
    "- **High-dimensional (n_features > 10):** TreeSHAP interaction values with liberal threshold\n",
    "\n",
    "### Key Innovation (v4.1.1)\n",
    "\n",
    "**Problem with previous iRF approach:**\n",
    "- Original iRF: O(trees x paths) complexity, 25+ minutes for 8 features\n",
    "- EBM: Unreliable interaction ranking, cannot distinguish true vs spurious\n",
    "\n",
    "**TreeSHAP Solution:**\n",
    "- O(MTLD^2) polynomial complexity (M=features, T=trees, L=leaves, D=depth)\n",
    "- 83-100% recall on moderate-to-large interactions\n",
    "- Theoretically grounded (Shapley values)\n",
    "- Mature implementation (`shap` package)\n",
    "\n",
    "### Adaptive Strategy\n",
    "\n",
    "| Features | Strategy | Rationale |\n",
    "|----------|----------|----------|\n",
    "| <= 10 | Direct enumeration | C(10,2)=45 pairs, no filtering needed |\n",
    "| > 10 | TreeSHAP + 25th percentile | Filter dummy-related interactions |\n",
    "\n",
    "### Reference\n",
    "\n",
    "- Lundberg, S. M., et al. (2020). From local explanations to global understanding with explainable AI for trees. *Nature Machine Intelligence*, 2(1), 56-67."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "04_InteractionDiscovery.ipynb - Adaptive Interaction Discovery\n",
    "===============================================================\n",
    "\n",
    "Three-Stage Physics-Informed Symbolic Regression Framework v4.1.1\n",
    "\n",
    "This module provides:\n",
    "- AdaptiveInteractionDiscoverer: Adaptive interaction discovery\n",
    "  - Low-dim (<=10 features): Direct pairwise enumeration\n",
    "  - High-dim (>10 features): TreeSHAP interaction values\n",
    "- Softmax soft threshold for importance-weighted feature selection\n",
    "- High-recall design: prioritizes not missing true interactions\n",
    "\n",
    "Algorithm:\n",
    "    1. Train Random Forest on data\n",
    "    2. Extract Gini importance -> soft_weights via softmax\n",
    "    3. If n_features <= 10: enumerate all pairwise\n",
    "    4. If n_features > 10: compute TreeSHAP interactions, filter by threshold\n",
    "    5. Return stable_interactions for Feature Library\n",
    "\n",
    "Output Dictionary Keys (v4.1 compatible):\n",
    "    - soft_weights: Dict of softmax-transformed importance weights\n",
    "    - selected_features: List of features above selection threshold\n",
    "    - stable_interactions: List of high-confidence interactions\n",
    "    - interaction_stability: Dict mapping interactions to scores\n",
    "    - raw_importance: Dict of raw Gini importance\n",
    "    - suggested_terms: List of feature product strings for library\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "Contact: zz3239@columbia.edu\n",
    "\"\"\"\n",
    "\n",
    "# Import core module\n",
    "%run 00_Core.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for Interaction Discovery\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from typing import Dict, List, Tuple, Optional, Any, Set, FrozenSet\n",
    "\n",
    "# TreeSHAP import (optional, falls back to enumeration if unavailable)\n",
    "try:\n",
    "    import shap\n",
    "    _SHAP_AVAILABLE = True\n",
    "    print(f\"04_InteractionDiscovery v4.1.1: SHAP {shap.__version__} available.\")\n",
    "except ImportError:\n",
    "    _SHAP_AVAILABLE = False\n",
    "    print(\"04_InteractionDiscovery v4.1.1: SHAP not available, using enumeration fallback.\")\n",
    "\n",
    "print(\"04_InteractionDiscovery v4.1.1: Additional imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ADAPTIVE INTERACTION DISCOVERER CLASS (v4.1.1 - TreeSHAP)\n",
    "# ==============================================================================\n",
    "\n",
    "class AdaptiveInteractionDiscoverer:\n",
    "    \"\"\"\n",
    "    Adaptive Interaction Discovery with TreeSHAP.\n",
    "    \n",
    "    This discoverer uses an adaptive strategy based on feature count:\n",
    "    - Low-dimensional (<=10 features): Direct pairwise enumeration\n",
    "    - High-dimensional (>10 features): TreeSHAP interaction values\n",
    "    \n",
    "    The design prioritizes HIGH RECALL (not missing true interactions)\n",
    "    over precision, since downstream E-WSINDy will filter false positives.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    temperature : float\n",
    "        Softmax temperature for feature importance (default: 0.5)\n",
    "    selection_threshold : float\n",
    "        Minimum softmax weight for feature selection (default: 0.1)\n",
    "    stability_threshold : float\n",
    "        For TreeSHAP: percentile threshold (default: 0.5 = top 50%)\n",
    "    max_interaction_order : int\n",
    "        Maximum order of interactions (default: 2 for pairwise)\n",
    "    n_estimators : int\n",
    "        Number of trees in Random Forest (default: 200)\n",
    "    low_dim_threshold : int\n",
    "        Features below this use direct enumeration (default: 10)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    discover(X, y, feature_names) -> Dict\n",
    "        Discover feature interactions\n",
    "    get_stable_interactions() -> List[Tuple[str, ...]]\n",
    "        Get list of stable interactions as tuples\n",
    "    get_interaction_matrix(X) -> Tuple[np.ndarray, List[str]]\n",
    "        Compute interaction features from stable interactions\n",
    "    print_interaction_report() -> None\n",
    "        Print detailed discovery report\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Lundberg et al. (2020). Nature Machine Intelligence, 2(1), 56-67.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> discoverer = AdaptiveInteractionDiscoverer()\n",
    "    >>> result = discoverer.discover(X, y, feature_names)\n",
    "    >>> print(result['stable_interactions'])\n",
    "    [frozenset({'x0', 'x1'}), frozenset({'x1', 'x2'})]  # Discovered interactions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature: float = DEFAULT_SOFTMAX_TEMPERATURE,\n",
    "        selection_threshold: float = DEFAULT_IMPORTANCE_THRESHOLD,\n",
    "        stability_threshold: float = DEFAULT_STABILITY_THRESHOLD,\n",
    "        max_interaction_order: int = 2,\n",
    "        n_estimators: int = 200,\n",
    "        n_bootstrap: int = 50,  # Kept for API compatibility, not used in v4.1.1\n",
    "        low_dim_threshold: int = 10,\n",
    "        shap_percentile: float = 25.0,  # Liberal threshold for high recall\n",
    "        random_state: int = RANDOM_SEED\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize AdaptiveInteractionDiscoverer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        temperature : float\n",
    "            Softmax temperature for feature importance.\n",
    "            Default: 0.5 (recommended balanced value)\n",
    "        selection_threshold : float\n",
    "            Minimum softmax weight for a feature to be considered.\n",
    "            Default: 0.1\n",
    "        stability_threshold : float\n",
    "            Minimum score for stable interaction (used differently in v4.1.1).\n",
    "            Default: 0.5\n",
    "        max_interaction_order : int\n",
    "            Maximum number of features in an interaction.\n",
    "            Default: 2 (pairwise interactions)\n",
    "        n_estimators : int\n",
    "            Number of trees in Random Forest.\n",
    "            Default: 200\n",
    "        n_bootstrap : int\n",
    "            Kept for API compatibility. Not used in v4.1.1.\n",
    "            Default: 50\n",
    "        low_dim_threshold : int\n",
    "            Feature count below which direct enumeration is used.\n",
    "            Default: 10\n",
    "        shap_percentile : float\n",
    "            Percentile threshold for SHAP filtering (lower = more recall).\n",
    "            Default: 25.0 (keep top 75% of interactions)\n",
    "        random_state : int\n",
    "            Random seed for reproducibility.\n",
    "            Default: 42\n",
    "        \"\"\"\n",
    "        self.temperature = temperature\n",
    "        self.selection_threshold = selection_threshold\n",
    "        self.stability_threshold = stability_threshold\n",
    "        self.max_interaction_order = max_interaction_order\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_bootstrap = n_bootstrap  # Kept for API compatibility\n",
    "        self.low_dim_threshold = low_dim_threshold\n",
    "        self.shap_percentile = shap_percentile\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Internal state\n",
    "        self._feature_names = None\n",
    "        self._rf_model = None\n",
    "        self._raw_importance = None\n",
    "        self._soft_weights = None\n",
    "        self._selected_features = None\n",
    "        self._all_interactions = None\n",
    "        self._interaction_stability = None\n",
    "        self._stable_interactions = None\n",
    "        self._discovery_complete = False\n",
    "        self._method_used = None  # 'enumeration' or 'treeshap'\n",
    "    \n",
    "    def discover(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Discover feature interactions using adaptive strategy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Feature matrix of shape (n_samples, n_features)\n",
    "        y : np.ndarray\n",
    "            Target vector of shape (n_samples,)\n",
    "        feature_names : List[str]\n",
    "            Names of features corresponding to columns of X\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Dictionary containing (v4.1 compatible keys):\n",
    "            - soft_weights: Dict of softmax-transformed importance weights\n",
    "            - selected_features: List of features above selection threshold\n",
    "            - stable_interactions: List of high-confidence interactions\n",
    "            - interaction_stability: Dict mapping interactions to scores\n",
    "            - raw_importance: Dict of raw Gini importance\n",
    "            - all_interactions: List of all candidate interactions\n",
    "            - suggested_terms: List of feature product strings for library\n",
    "            - n_stable_interactions: Number of stable interactions\n",
    "            - method_used: 'enumeration' or 'treeshap'\n",
    "        \"\"\"\n",
    "        self._feature_names = list(feature_names)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Step 1: Train Random Forest for importance\n",
    "        self._rf_model = self._fit_random_forest(X, y)\n",
    "        \n",
    "        # Step 2: Extract and transform importance\n",
    "        self._raw_importance = self._rf_model.feature_importances_\n",
    "        self._soft_weights = self._softmax_transform(self._raw_importance)\n",
    "        \n",
    "        # Step 3: Select features above threshold\n",
    "        self._selected_features = [\n",
    "            self._feature_names[i] for i in range(n_features)\n",
    "            if self._soft_weights[i] > self.selection_threshold\n",
    "        ]\n",
    "        \n",
    "        # Step 4: Adaptive interaction discovery\n",
    "        if n_features <= self.low_dim_threshold:\n",
    "            # Low-dimensional: direct enumeration\n",
    "            self._method_used = 'enumeration'\n",
    "            self._all_interactions, self._interaction_stability = \\\n",
    "                self._enumerate_all_interactions(n_features)\n",
    "        else:\n",
    "            # High-dimensional: TreeSHAP\n",
    "            if _SHAP_AVAILABLE:\n",
    "                self._method_used = 'treeshap'\n",
    "                self._all_interactions, self._interaction_stability = \\\n",
    "                    self._treeshap_interactions(X)\n",
    "            else:\n",
    "                # Fallback to enumeration if SHAP unavailable\n",
    "                self._method_used = 'enumeration_fallback'\n",
    "                self._all_interactions, self._interaction_stability = \\\n",
    "                    self._enumerate_all_interactions(n_features)\n",
    "        \n",
    "        # Step 5: Filter stable interactions based on method\n",
    "        if self._method_used == 'enumeration' or self._method_used == 'enumeration_fallback':\n",
    "            # For enumeration: all interactions are \"stable\" (score = 1.0)\n",
    "            self._stable_interactions = list(self._all_interactions)\n",
    "        else:\n",
    "            # For TreeSHAP: filter by percentile threshold\n",
    "            if self._interaction_stability:\n",
    "                scores = list(self._interaction_stability.values())\n",
    "                threshold = np.percentile(scores, self.shap_percentile)\n",
    "                self._stable_interactions = [\n",
    "                    interaction for interaction, score\n",
    "                    in self._interaction_stability.items()\n",
    "                    if score >= threshold\n",
    "                ]\n",
    "            else:\n",
    "                self._stable_interactions = []\n",
    "        \n",
    "        self._discovery_complete = True\n",
    "        \n",
    "        # Build result dictionary (v4.1 compatible)\n",
    "        raw_importance_dict = {\n",
    "            name: float(self._raw_importance[i])\n",
    "            for i, name in enumerate(self._feature_names)\n",
    "        }\n",
    "        \n",
    "        soft_weights_dict = {\n",
    "            name: float(self._soft_weights[i])\n",
    "            for i, name in enumerate(self._feature_names)\n",
    "        }\n",
    "        \n",
    "        # Convert frozenset keys to tuple for JSON compatibility in some contexts\n",
    "        interaction_stability_dict = {\n",
    "            interaction: float(score)\n",
    "            for interaction, score in self._interaction_stability.items()\n",
    "        }\n",
    "        \n",
    "        # Build suggested terms for feature library\n",
    "        suggested_terms = self._build_suggested_terms()\n",
    "        \n",
    "        return {\n",
    "            # v4.1 primary keys\n",
    "            'soft_weights': soft_weights_dict,\n",
    "            'selected_features': self._selected_features,\n",
    "            'stable_interactions': self._stable_interactions,\n",
    "            'interaction_stability': interaction_stability_dict,\n",
    "            # Additional useful keys\n",
    "            'raw_importance': raw_importance_dict,\n",
    "            'all_interactions': list(self._all_interactions),\n",
    "            'suggested_terms': suggested_terms,\n",
    "            'n_stable_interactions': len(self._stable_interactions),\n",
    "            'temperature': self.temperature,\n",
    "            'stability_threshold': self.stability_threshold,\n",
    "            # v4.1.1 additions\n",
    "            'method_used': self._method_used,\n",
    "            'n_features': n_features,\n",
    "            'low_dim_threshold': self.low_dim_threshold,\n",
    "            # Backward compatibility alias\n",
    "            'softmax_weights': soft_weights_dict\n",
    "        }\n",
    "    \n",
    "    def _fit_random_forest(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray\n",
    "    ) -> RandomForestRegressor:\n",
    "        \"\"\"\n",
    "        Fit Random Forest regressor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Feature matrix\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        RandomForestRegressor\n",
    "            Fitted Random Forest model\n",
    "        \"\"\"\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_features='sqrt',\n",
    "            max_depth=8,  # Shallower for faster SHAP computation\n",
    "            min_samples_leaf=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        rf.fit(X, y)\n",
    "        return rf\n",
    "    \n",
    "    def _softmax_transform(\n",
    "        self,\n",
    "        importance: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply softmax transformation to importance scores.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        importance : np.ndarray\n",
    "            Raw Gini importance scores\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Softmax-transformed weights (sum to 1)\n",
    "        \"\"\"\n",
    "        # Scale by temperature\n",
    "        scaled = importance / self.temperature\n",
    "        \n",
    "        # Numerical stability: subtract max before exp\n",
    "        scaled = scaled - np.max(scaled)\n",
    "        \n",
    "        # Compute softmax\n",
    "        exp_scaled = np.exp(scaled)\n",
    "        weights = exp_scaled / np.sum(exp_scaled)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _enumerate_all_interactions(\n",
    "        self,\n",
    "        n_features: int\n",
    "    ) -> Tuple[Set[FrozenSet[str]], Dict[FrozenSet[str], float]]:\n",
    "        \"\"\"\n",
    "        Enumerate all pairwise interactions (for low-dimensional data).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features : int\n",
    "            Number of features\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Set[FrozenSet[str]], Dict[FrozenSet[str], float]]\n",
    "            - Set of all pairwise interactions\n",
    "            - Dictionary mapping each interaction to score 1.0\n",
    "        \"\"\"\n",
    "        all_interactions = set()\n",
    "        interaction_scores = {}\n",
    "        \n",
    "        # Generate all pairwise combinations\n",
    "        for i, j in combinations(range(n_features), 2):\n",
    "            interaction = frozenset([self._feature_names[i], self._feature_names[j]])\n",
    "            all_interactions.add(interaction)\n",
    "            interaction_scores[interaction] = 1.0  # All equally valid\n",
    "        \n",
    "        # Optionally add 3-way interactions if max_order allows\n",
    "        if self.max_interaction_order >= 3 and n_features <= 6:\n",
    "            for combo in combinations(range(n_features), 3):\n",
    "                interaction = frozenset(self._feature_names[i] for i in combo)\n",
    "                all_interactions.add(interaction)\n",
    "                interaction_scores[interaction] = 1.0\n",
    "        \n",
    "        return all_interactions, interaction_scores\n",
    "    \n",
    "    def _treeshap_interactions(\n",
    "        self,\n",
    "        X: np.ndarray\n",
    "    ) -> Tuple[Set[FrozenSet[str]], Dict[FrozenSet[str], float]]:\n",
    "        \"\"\"\n",
    "        Compute TreeSHAP interaction values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Feature matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Set[FrozenSet[str]], Dict[FrozenSet[str], float]]\n",
    "            - Set of all detected interactions\n",
    "            - Dictionary mapping interactions to SHAP interaction scores\n",
    "        \"\"\"\n",
    "        # Use subset of samples for speed if dataset is large\n",
    "        n_samples = X.shape[0]\n",
    "        if n_samples > 500:\n",
    "            np.random.seed(self.random_state)\n",
    "            sample_idx = np.random.choice(n_samples, 500, replace=False)\n",
    "            X_sample = X[sample_idx]\n",
    "        else:\n",
    "            X_sample = X\n",
    "        \n",
    "        # Compute TreeSHAP interaction values\n",
    "        explainer = shap.TreeExplainer(self._rf_model)\n",
    "        shap_interactions = explainer.shap_interaction_values(X_sample)\n",
    "        \n",
    "        # shap_interactions shape: (n_samples, n_features, n_features)\n",
    "        # Average absolute interaction values across samples\n",
    "        mean_interactions = np.abs(shap_interactions).mean(axis=0)\n",
    "        \n",
    "        # Zero out diagonal (self-interactions)\n",
    "        np.fill_diagonal(mean_interactions, 0)\n",
    "        \n",
    "        # Build interaction dictionary\n",
    "        all_interactions = set()\n",
    "        interaction_scores = {}\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            for j in range(i + 1, n_features):\n",
    "                interaction = frozenset([self._feature_names[i], self._feature_names[j]])\n",
    "                # Use symmetric average\n",
    "                score = (mean_interactions[i, j] + mean_interactions[j, i]) / 2\n",
    "                \n",
    "                if score > 0:  # Only include non-zero interactions\n",
    "                    all_interactions.add(interaction)\n",
    "                    interaction_scores[interaction] = float(score)\n",
    "        \n",
    "        return all_interactions, interaction_scores\n",
    "    \n",
    "    def _build_suggested_terms(\n",
    "        self\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Build suggested interaction terms for feature library.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of feature product strings (e.g., \"x0 * x1\")\n",
    "        \"\"\"\n",
    "        terms = []\n",
    "        for interaction in self._stable_interactions:\n",
    "            # Sort for consistent ordering\n",
    "            sorted_features = sorted(interaction)\n",
    "            term = \" * \".join(sorted_features)\n",
    "            terms.append(term)\n",
    "        return terms\n",
    "    \n",
    "    def get_stable_interactions(\n",
    "        self\n",
    "    ) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"\n",
    "        Get list of stable interactions as tuples.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[str, ...]]\n",
    "            List of stable interactions\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If discovery has not been performed\n",
    "        \"\"\"\n",
    "        if not self._discovery_complete:\n",
    "            raise RuntimeError(\"Must run discover() before getting interactions\")\n",
    "        \n",
    "        return [tuple(sorted(interaction))\n",
    "                for interaction in self._stable_interactions]\n",
    "    \n",
    "    def get_interaction_matrix(\n",
    "        self,\n",
    "        X: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Compute interaction features from stable interactions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Original feature matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, List[str]]\n",
    "            - Interaction feature matrix\n",
    "            - Names of interaction features\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If discovery has not been performed\n",
    "        \"\"\"\n",
    "        if not self._discovery_complete:\n",
    "            raise RuntimeError(\"Must run discover() before computing interaction matrix\")\n",
    "        \n",
    "        if len(self._stable_interactions) == 0:\n",
    "            return np.empty((X.shape[0], 0)), []\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        interaction_features = []\n",
    "        interaction_names = []\n",
    "        \n",
    "        # Create mapping from feature name to column index\n",
    "        name_to_idx = {name: i for i, name in enumerate(self._feature_names)}\n",
    "        \n",
    "        for interaction in self._stable_interactions:\n",
    "            # Compute product of features in interaction\n",
    "            product = np.ones(n_samples)\n",
    "            sorted_features = sorted(interaction)\n",
    "            \n",
    "            for feat_name in sorted_features:\n",
    "                idx = name_to_idx[feat_name]\n",
    "                product *= X[:, idx]\n",
    "            \n",
    "            interaction_features.append(product)\n",
    "            interaction_names.append(\"*\".join(sorted_features))\n",
    "        \n",
    "        return np.column_stack(interaction_features), interaction_names\n",
    "    \n",
    "    def print_interaction_report(self) -> None:\n",
    "        \"\"\"\n",
    "        Print a detailed interaction discovery report.\n",
    "        \"\"\"\n",
    "        if not self._discovery_complete:\n",
    "            print(\"Discovery not yet performed. Run discover() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"=== Interaction Discovery Results (v4.1.1 Adaptive) ===\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(\"Configuration:\")\n",
    "        print(f\"  Method used: {self._method_used}\")\n",
    "        print(f\"  Temperature: {self.temperature}\")\n",
    "        print(f\"  Selection threshold: {self.selection_threshold}\")\n",
    "        print(f\"  Low-dim threshold: {self.low_dim_threshold}\")\n",
    "        if self._method_used == 'treeshap':\n",
    "            print(f\"  SHAP percentile: {self.shap_percentile}\")\n",
    "        print()\n",
    "        print(\"-\" * 70)\n",
    "        print(\" Feature Importance (Softmax Weights):\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Feature':<20} {'Raw Importance':<15} {'Soft Weight':<15} {'Selected'}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Sort by softmax weight\n",
    "        sorted_indices = np.argsort(self._soft_weights)[::-1]\n",
    "        for idx in sorted_indices:\n",
    "            name = self._feature_names[idx]\n",
    "            raw = self._raw_importance[idx]\n",
    "            soft = self._soft_weights[idx]\n",
    "            selected = \"YES\" if name in self._selected_features else \"no\"\n",
    "            print(f\"{name:<20} {raw:<15.4f} {soft:<15.4f} {selected}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"-\" * 70)\n",
    "        print(\" Stable Interactions:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        if len(self._stable_interactions) == 0:\n",
    "            print(\"  No stable interactions found.\")\n",
    "        else:\n",
    "            # Sort by score\n",
    "            sorted_interactions = sorted(\n",
    "                self._stable_interactions,\n",
    "                key=lambda x: self._interaction_stability.get(x, 0),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            print(f\"{'Interaction':<30} {'Score':<15}\")\n",
    "            print(\"-\" * 50)\n",
    "            for interaction in sorted_interactions[:20]:  # Show top 20\n",
    "                name = \" * \".join(sorted(interaction))\n",
    "                score = self._interaction_stability.get(interaction, 0)\n",
    "                print(f\"{name:<30} {score:<15.4f}\")\n",
    "            \n",
    "            if len(sorted_interactions) > 20:\n",
    "                print(f\"  ... and {len(sorted_interactions) - 20} more interactions\")\n",
    "        \n",
    "        print()\n",
    "        print(f\"Total stable interactions: {len(self._stable_interactions)}\")\n",
    "        print(f\"Total candidate interactions: {len(self._all_interactions)}\")\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BACKWARD COMPATIBILITY ALIAS\n",
    "# ==============================================================================\n",
    "# Keep IRFInteractionDiscoverer as an alias for backward compatibility\n",
    "# This allows old code that uses IRFInteractionDiscoverer to still work\n",
    "IRFInteractionDiscoverer = AdaptiveInteractionDiscoverer\n",
    "\n",
    "print(\"AdaptiveInteractionDiscoverer class defined (v4.1.1 - TreeSHAP Adaptive).\")\n",
    "print(\"IRFInteractionDiscoverer is now an alias for AdaptiveInteractionDiscoverer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Internal Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST CONTROL FLAG\n",
    "# ==============================================================================\n",
    "\n",
    "_RUN_TESTS = False  # Set to True to run internal tests\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" RUNNING INTERNAL TESTS FOR 04_InteractionDiscovery v4.1.1\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 1: Low-Dimensional Enumeration (n_features <= 10)\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 1: Low-Dimensional Enumeration\")\n",
    "    \n",
    "    # Generate data with 5 features (should use enumeration)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    x0 = np.random.uniform(0.1, 1, n_samples)\n",
    "    x1 = np.random.uniform(0.1, 1, n_samples)\n",
    "    x2 = np.random.uniform(0.1, 1, n_samples)\n",
    "    x3 = np.random.randn(n_samples)  # Noise\n",
    "    x4 = np.random.randn(n_samples)  # Noise\n",
    "    \n",
    "    # True equation: y = 3*x0*x1 + x2\n",
    "    y = 3 * x0 * x1 + x2 + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    X = np.column_stack([x0, x1, x2, x3, x4])\n",
    "    feature_names = ['x0', 'x1', 'x2', 'x3', 'x4']\n",
    "    \n",
    "    print(f\"True equation: y = 3*x0*x1 + x2\")\n",
    "    print(f\"Number of features: {X.shape[1]} (should use enumeration)\")\n",
    "    print()\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    discoverer = AdaptiveInteractionDiscoverer(low_dim_threshold=10)\n",
    "    result = discoverer.discover(X, y, feature_names)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Method used: {result['method_used']}\")\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "    print(f\"Total interactions: {len(result['stable_interactions'])}\")\n",
    "    print(f\"Expected: C(5,2) = 10 pairwise interactions\")\n",
    "    print()\n",
    "    \n",
    "    # Check x0*x1 is included\n",
    "    x0_x1_found = any(\n",
    "        frozenset(['x0', 'x1']) == interaction\n",
    "        for interaction in result['stable_interactions']\n",
    "    )\n",
    "    \n",
    "    if x0_x1_found:\n",
    "        print(\"[PASS] True interaction (x0*x1) included in results\")\n",
    "    else:\n",
    "        print(\"[FAIL] True interaction (x0*x1) NOT found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 2: Backward Compatibility (IRFInteractionDiscoverer alias)\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 2: Backward Compatibility\")\n",
    "    \n",
    "    # Test that IRFInteractionDiscoverer works as an alias\n",
    "    discoverer_old_name = IRFInteractionDiscoverer()\n",
    "    \n",
    "    # Check it has the new attributes\n",
    "    if hasattr(discoverer_old_name, 'low_dim_threshold'):\n",
    "        print(f\"[PASS] IRFInteractionDiscoverer has low_dim_threshold: {discoverer_old_name.low_dim_threshold}\")\n",
    "    else:\n",
    "        print(\"[FAIL] IRFInteractionDiscoverer missing low_dim_threshold\")\n",
    "    \n",
    "    if hasattr(discoverer_old_name, 'shap_percentile'):\n",
    "        print(f\"[PASS] IRFInteractionDiscoverer has shap_percentile: {discoverer_old_name.shap_percentile}\")\n",
    "    else:\n",
    "        print(\"[FAIL] IRFInteractionDiscoverer missing shap_percentile\")\n",
    "    \n",
    "    # Check class identity\n",
    "    if IRFInteractionDiscoverer is AdaptiveInteractionDiscoverer:\n",
    "        print(\"[PASS] IRFInteractionDiscoverer is AdaptiveInteractionDiscoverer\")\n",
    "    else:\n",
    "        print(\"[FAIL] Classes are not the same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Module Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODULE SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" 04_InteractionDiscovery.ipynb v4.1.1 - Module Summary\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"CLASS: AdaptiveInteractionDiscoverer (v4.1.1 - TreeSHAP Adaptive)\")\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(\"Purpose:\")\n",
    "print(\"  Discover feature interactions using adaptive strategy:\")\n",
    "print(\"  - Low-dim (<=10 features): Direct pairwise enumeration\")\n",
    "print(\"  - High-dim (>10 features): TreeSHAP interaction values\")\n",
    "print()\n",
    "print(\"Key Innovation (v4.1.1):\")\n",
    "print(\"  - Replaces slow iRF with fast adaptive strategy\")\n",
    "print(\"  - Prioritizes HIGH RECALL (not missing true interactions)\")\n",
    "print(\"  - Fast: <1s for low-dim, 1-5min for high-dim\")\n",
    "print(\"  - False positives handled by downstream E-WSINDy\")\n",
    "print()\n",
    "print(\"Backward Compatibility:\")\n",
    "print(\"  IRFInteractionDiscoverer = AdaptiveInteractionDiscoverer (alias)\")\n",
    "print()\n",
    "print(\"Main Methods:\")\n",
    "print(\"  discover(X, y, feature_names)\")\n",
    "print(\"      Discover feature interactions\")\n",
    "print(\"      Returns: dict with soft_weights, stable_interactions, etc.\")\n",
    "print()\n",
    "print(\"  get_stable_interactions()\")\n",
    "print(\"      Get list of stable interactions as tuples\")\n",
    "print()\n",
    "print(\"  get_interaction_matrix(X)\")\n",
    "print(\"      Compute interaction features from stable interactions\")\n",
    "print()\n",
    "print(\"  print_interaction_report()\")\n",
    "print(\"      Print detailed discovery report\")\n",
    "print()\n",
    "print(\"Output Dictionary Keys (v4.1 compatible):\")\n",
    "print(\"  - soft_weights          : Dict of softmax-transformed weights\")\n",
    "print(\"  - selected_features     : List of features above threshold\")\n",
    "print(\"  - stable_interactions   : List of high-confidence interactions\")\n",
    "print(\"  - interaction_stability : Dict mapping interactions to scores\")\n",
    "print(\"  - raw_importance        : Dict of raw Gini importance\")\n",
    "print(\"  - suggested_terms       : List of feature product strings\")\n",
    "print(\"  - method_used           : 'enumeration' or 'treeshap'\")\n",
    "print()\n",
    "print(\"Key Parameters:\")\n",
    "print(\"  low_dim_threshold: Features below this use enumeration (default: 10)\")\n",
    "print(\"  shap_percentile: For TreeSHAP, percentile threshold (default: 25)\")\n",
    "print(\"  temperature: Softmax temperature for importance (default: 0.5)\")\n",
    "print()\n",
    "print(\"Usage Example:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "# Create discoverer (new name)\n",
    "discoverer = AdaptiveInteractionDiscoverer(\n",
    "    low_dim_threshold=10,\n",
    "    shap_percentile=25\n",
    ")\n",
    "\n",
    "# Or use old name (backward compatible)\n",
    "discoverer = IRFInteractionDiscoverer()  # Same as above\n",
    "\n",
    "# Run discovery\n",
    "result = discoverer.discover(X, y, feature_names)\n",
    "\n",
    "# Get stable interactions (v4.1 key)\n",
    "interactions = result['stable_interactions']\n",
    "print(f\"Found {len(interactions)} interactions using {result['method_used']}\")\n",
    "\"\"\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"Module loaded successfully. Import via: %run 04_InteractionDiscovery.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
