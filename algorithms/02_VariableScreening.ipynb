{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_VariableScreening - Physics-SR Framework v4.1\n",
    "\n",
    "## Stage 1.2: PAN+SR Nonlinear Variable Screening\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Date:** January 2026  \n",
    "**Version:** 4.1 (Structure-Guided Feature Library Enhancement + Computational Optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Select important variables using nonlinear importance measures, addressing the limitation of LARS which relies on linear correlation.\n",
    "\n",
    "### Problem with LARS\n",
    "\n",
    "LARS selects features based on linear correlation:\n",
    "$$\\hat{j} = \\arg\\max_j |\\text{corr}(X_j, r)|$$\n",
    "\n",
    "This fails for nonlinear dependencies. For example, if $y = \\sin(x)$ with symmetric $x \\in [-\\pi, \\pi]$, then $\\text{corr}(x, y) \\approx 0$.\n",
    "\n",
    "### Solution: Random Forest Permutation Importance\n",
    "\n",
    "Permutation importance measures how much the prediction error increases when a feature is randomly shuffled:\n",
    "\n",
    "$$I_j = \\frac{\\text{MSE}_{\\text{permuted}_j} - \\text{MSE}_{\\text{baseline}}}{\\text{MSE}_{\\text{baseline}}}$$\n",
    "\n",
    "This captures nonlinear dependencies because Random Forest can model arbitrary functions.\n",
    "\n",
    "### Reference\n",
    "\n",
    "- Breiman, L. (2001). Random Forests. *Machine Learning*, 45(1), 5-32.\n",
    "- Strobl, C., et al. (2007). Bias in random forest variable importance measures. *BMC Bioinformatics*, 8, 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "02_VariableScreening.ipynb - PAN+SR Nonlinear Variable Screening\n",
    "=================================================================\n",
    "\n",
    "Three-Stage Physics-Informed Symbolic Regression Framework v4.1\n",
    "\n",
    "This module provides:\n",
    "- PANSRVariableScreener: Variable selection using Random Forest permutation importance\n",
    "- Nonlinear importance detection that captures dependencies LARS misses\n",
    "- Normalized importance scores and automatic threshold selection\n",
    "\n",
    "Algorithm:\n",
    "    1. Fit Random Forest with n_estimators=500, max_features='sqrt'\n",
    "    2. Compute permutation importance for each feature\n",
    "    3. Normalize importance scores to sum to 1\n",
    "    4. Select features with normalized importance > threshold\n",
    "\n",
    "Output Dictionary Keys (v4.1):\n",
    "    - selected_features: List of selected feature names\n",
    "    - selected_indices: List of selected feature indices\n",
    "    - importance_scores: Dict mapping feature names to raw importance\n",
    "    - normalized_importance: Dict mapping feature names to normalized importance\n",
    "    - importance_ranking: List of (name, importance) sorted by importance\n",
    "    - rf_r2: R-squared of Random Forest fit\n",
    "    - n_features: Total number of input features\n",
    "    - n_selected: Number of selected features\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "Contact: zz3239@columbia.edu\n",
    "\"\"\"\n",
    "\n",
    "# Import core module\n",
    "%run 00_Core.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for Variable Screening\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "print(\"02_VariableScreening v4.1: Additional imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PAN+SR VARIABLE SCREENER CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class PANSRVariableScreener:\n",
    "    \"\"\"\n",
    "    PAN+SR Nonlinear Variable Screening using Random Forest Permutation Importance.\n",
    "    \n",
    "    This screener addresses the limitation of LARS (Least Angle Regression) which\n",
    "    relies on linear correlation and fails for nonlinear dependencies. Random Forest\n",
    "    permutation importance can detect arbitrary nonlinear relationships.\n",
    "    \n",
    "    The algorithm:\n",
    "    1. Fits a Random Forest regressor to capture nonlinear patterns\n",
    "    2. Computes permutation importance by measuring MSE increase when each\n",
    "       feature is randomly shuffled\n",
    "    3. Normalizes importance scores to create a probability distribution\n",
    "    4. Selects features exceeding the importance threshold\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    n_estimators : int\n",
    "        Number of trees in the Random Forest (default: 500)\n",
    "    importance_threshold : float\n",
    "        Minimum normalized importance for feature selection (default: 0.01)\n",
    "    n_permutations : int\n",
    "        Number of permutations for importance estimation (default: 10)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    rf_model : RandomForestRegressor\n",
    "        Fitted Random Forest model (public, v4.1)\n",
    "    importance_scores : Dict[str, float]\n",
    "        Raw importance scores (public, v4.1)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    screen(X, y, feature_names) -> Dict\n",
    "        Perform nonlinear variable screening\n",
    "    get_selected_features() -> List[str]\n",
    "        Get list of selected feature names\n",
    "    get_importance_ranking() -> List[Tuple[str, float]]\n",
    "        Get features ranked by importance\n",
    "    get_gini_importance() -> Dict[str, float]\n",
    "        Get Gini importance from Random Forest\n",
    "    print_screening_report() -> None\n",
    "        Print detailed screening report\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> screener = PANSRVariableScreener(importance_threshold=0.01)\n",
    "    >>> result = screener.screen(X, y, feature_names)\n",
    "    >>> print(result['selected_features'])\n",
    "    ['q_c', 'N_d']  # Features in the true equation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 500,\n",
    "        importance_threshold: float = DEFAULT_IMPORTANCE_THRESHOLD,\n",
    "        n_permutations: int = 10,\n",
    "        random_state: int = RANDOM_SEED\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize PANSRVariableScreener.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int\n",
    "            Number of trees in Random Forest. More trees give more stable\n",
    "            importance estimates but increase computation time.\n",
    "            Default: 500\n",
    "        importance_threshold : float\n",
    "            Minimum normalized importance for a feature to be selected.\n",
    "            Features with importance < threshold are considered unimportant.\n",
    "            Default: 0.01 (1%)\n",
    "        n_permutations : int\n",
    "            Number of times to permute each feature when computing importance.\n",
    "            More permutations give more stable estimates.\n",
    "            Default: 10\n",
    "        random_state : int\n",
    "            Random seed for reproducibility.\n",
    "            Default: 42\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.importance_threshold = importance_threshold\n",
    "        self.n_permutations = n_permutations\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Public attributes (v4.1 naming convention)\n",
    "        self.rf_model = None\n",
    "        self.importance_scores = None\n",
    "        \n",
    "        # Internal state (private, prefixed with underscore)\n",
    "        self._raw_importance = None\n",
    "        self._normalized_importance = None\n",
    "        self._importance_std = None\n",
    "        self._feature_names = None\n",
    "        self._selected_indices = None\n",
    "        self._selected_features = None\n",
    "        self._baseline_mse = None\n",
    "        self._rf_r2 = None\n",
    "        self._screening_complete = False\n",
    "    \n",
    "    def screen(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform variable screening using Random Forest permutation importance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Feature matrix of shape (n_samples, n_features)\n",
    "        y : np.ndarray\n",
    "            Target vector of shape (n_samples,)\n",
    "        feature_names : List[str]\n",
    "            Names of features corresponding to columns of X\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Dictionary containing (v4.1 keys):\n",
    "            - selected_features: List of selected feature names\n",
    "            - selected_indices: List of selected feature indices\n",
    "            - importance_scores: Dict mapping feature names to raw importance\n",
    "            - normalized_importance: Dict mapping feature names to normalized importance\n",
    "            - importance_ranking: List of (name, importance) sorted by importance\n",
    "            - rf_r2: R-squared of Random Forest fit\n",
    "            - n_features: Total number of input features\n",
    "            - n_selected: Number of selected features\n",
    "            - baseline_mse: Baseline MSE before permutation\n",
    "            - threshold_used: Importance threshold used for selection\n",
    "        \"\"\"\n",
    "        self._feature_names = list(feature_names)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Step 1: Fit Random Forest\n",
    "        self.rf_model = self._fit_random_forest(X, y)\n",
    "        \n",
    "        # Compute baseline metrics\n",
    "        y_pred = self.rf_model.predict(X)\n",
    "        self._baseline_mse = compute_mse(y, y_pred)\n",
    "        self._rf_r2 = compute_r2(y, y_pred)\n",
    "        \n",
    "        # Step 2: Compute permutation importance\n",
    "        self._raw_importance, self._importance_std = self._compute_permutation_importance(X, y)\n",
    "        \n",
    "        # Step 3: Normalize importance\n",
    "        self._normalized_importance = self._normalize_importance(self._raw_importance)\n",
    "        \n",
    "        # Step 4: Select features\n",
    "        self._selected_indices = [\n",
    "            i for i in range(n_features)\n",
    "            if self._normalized_importance[i] > self.importance_threshold\n",
    "        ]\n",
    "        self._selected_features = [self._feature_names[i] for i in self._selected_indices]\n",
    "        \n",
    "        self._screening_complete = True\n",
    "        \n",
    "        # Build importance_scores dict (raw importance, public attribute)\n",
    "        self.importance_scores = {\n",
    "            name: float(self._raw_importance[i])\n",
    "            for i, name in enumerate(self._feature_names)\n",
    "        }\n",
    "        \n",
    "        # Build normalized_importance dict (v4.1 key)\n",
    "        normalized_importance = {\n",
    "            name: float(self._normalized_importance[i])\n",
    "            for i, name in enumerate(self._feature_names)\n",
    "        }\n",
    "        \n",
    "        # Build ranking\n",
    "        importance_ranking = sorted(\n",
    "            normalized_importance.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            # v4.1 primary keys\n",
    "            'selected_features': self._selected_features,\n",
    "            'selected_indices': self._selected_indices,\n",
    "            'importance_scores': self.importance_scores,\n",
    "            'normalized_importance': normalized_importance,\n",
    "            # Additional useful keys\n",
    "            'importance_ranking': importance_ranking,\n",
    "            'rf_r2': self._rf_r2,\n",
    "            'n_features': n_features,\n",
    "            'n_selected': len(self._selected_indices),\n",
    "            'baseline_mse': self._baseline_mse,\n",
    "            'threshold_used': self.importance_threshold,\n",
    "            # Backward compatibility alias\n",
    "            'selected_names': self._selected_features\n",
    "        }\n",
    "    \n",
    "    def _fit_random_forest(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray\n",
    "    ) -> RandomForestRegressor:\n",
    "        \"\"\"\n",
    "        Fit Random Forest regressor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Feature matrix\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        RandomForestRegressor\n",
    "            Fitted Random Forest model\n",
    "        \"\"\"\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_features='sqrt',\n",
    "            min_samples_leaf=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        rf.fit(X, y)\n",
    "        return rf\n",
    "    \n",
    "    def _compute_permutation_importance(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute permutation importance for all features.\n",
    "        \n",
    "        For each feature j:\n",
    "        1. Compute baseline MSE\n",
    "        2. Shuffle column j and recompute MSE\n",
    "        3. Importance = (MSE_permuted - MSE_baseline) / MSE_baseline\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Feature matrix\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "            - importance_mean: Mean importance for each feature\n",
    "            - importance_std: Standard deviation of importance\n",
    "        \"\"\"\n",
    "        result = permutation_importance(\n",
    "            self.rf_model,\n",
    "            X,\n",
    "            y,\n",
    "            n_repeats=self.n_permutations,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        # permutation_importance returns negative values for neg_mse\n",
    "        # Higher (less negative) = more important\n",
    "        # We want: importance = increase in error when permuted\n",
    "        importance_mean = -result.importances_mean  # Convert to positive\n",
    "        importance_std = result.importances_std\n",
    "        \n",
    "        # Ensure non-negative (some features may have negative importance due to noise)\n",
    "        importance_mean = np.maximum(importance_mean, 0)\n",
    "        \n",
    "        return importance_mean, importance_std\n",
    "    \n",
    "    def _normalize_importance(\n",
    "        self,\n",
    "        importance: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize importance scores to sum to 1.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        importance : np.ndarray\n",
    "            Raw importance scores\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Normalized importance (sums to 1)\n",
    "        \"\"\"\n",
    "        total = np.sum(importance)\n",
    "        if total < EPS_DIV:\n",
    "            # All importances are zero - return uniform distribution\n",
    "            return np.ones_like(importance) / len(importance)\n",
    "        return importance / total\n",
    "    \n",
    "    def get_selected_features(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of selected feature names.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            Names of selected features\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If screening has not been performed\n",
    "        \"\"\"\n",
    "        if not self._screening_complete:\n",
    "            raise RuntimeError(\"Must run screen() before getting selected features\")\n",
    "        return self._selected_features.copy()\n",
    "    \n",
    "    def get_importance_ranking(self) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get features ranked by importance.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[str, float]]\n",
    "            List of (feature_name, normalized_importance) sorted descending\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If screening has not been performed\n",
    "        \"\"\"\n",
    "        if not self._screening_complete:\n",
    "            raise RuntimeError(\"Must run screen() before getting ranking\")\n",
    "        \n",
    "        ranking = [\n",
    "            (name, float(self._normalized_importance[i]))\n",
    "            for i, name in enumerate(self._feature_names)\n",
    "        ]\n",
    "        return sorted(ranking, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def get_gini_importance(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get Gini importance (mean decrease in impurity) from Random Forest.\n",
    "        \n",
    "        This is a faster alternative to permutation importance, though\n",
    "        potentially biased for features with many categories.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, float]\n",
    "            Feature names to Gini importance\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If screening has not been performed\n",
    "        \"\"\"\n",
    "        if self.rf_model is None:\n",
    "            raise RuntimeError(\"Must run screen() before getting Gini importance\")\n",
    "        \n",
    "        gini_importance = self.rf_model.feature_importances_\n",
    "        return {\n",
    "            name: float(gini_importance[i])\n",
    "            for i, name in enumerate(self._feature_names)\n",
    "        }\n",
    "    \n",
    "    def print_screening_report(self) -> None:\n",
    "        \"\"\"\n",
    "        Print a detailed screening report in v4.1 format.\n",
    "        \"\"\"\n",
    "        if not self._screening_complete:\n",
    "            print(\"Screening not yet performed. Run screen() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"=== Variable Screening Results (PAN+SR) ===\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(\"Random Forest Configuration:\")\n",
    "        print(f\"  n_estimators: {self.n_estimators}\")\n",
    "        print(f\"  n_permutations: {self.n_permutations}\")\n",
    "        print(f\"  importance_threshold: {self.importance_threshold}\")\n",
    "        print()\n",
    "        print(\"Random Forest Performance:\")\n",
    "        print(f\"  R-squared: {self._rf_r2:.4f}\")\n",
    "        print(f\"  Baseline MSE: {self._baseline_mse:.6e}\")\n",
    "        print()\n",
    "        print(\"-\" * 70)\n",
    "        print(\" Feature Importance Ranking:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Rank':<6} {'Feature':<20} {'Importance':<15} {'Selected'}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        ranking = self.get_importance_ranking()\n",
    "        for rank, (name, importance) in enumerate(ranking, 1):\n",
    "            selected = \"YES\" if name in self._selected_features else \"no\"\n",
    "            print(f\"{rank:<6} {name:<20} {importance:<15.4f} {selected}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"-\" * 70)\n",
    "        print(f\" Selected Features: {len(self._selected_features)} / {len(self._feature_names)}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"  {self._selected_features}\")\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "print(\"PANSRVariableScreener class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Internal Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST CONTROL FLAG\n",
    "# ==============================================================================\n",
    "\n",
    "_RUN_TESTS = False  # Set to True to run internal tests\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" RUNNING INTERNAL TESTS FOR 02_VariableScreening v4.1\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 1: Warm Rain Data\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 1: Warm Rain Data\")\n",
    "    \n",
    "    # Generate warm rain data\n",
    "    # True equation: dq_r/dt = 0.89 * q_c^2.47 * N_d^(-1.79)\n",
    "    X, y, feature_names, user_inputs = generate_warm_rain_data(\n",
    "        n_samples=500, noise_level=0.01, seed=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Features: {feature_names}\")\n",
    "    print(f\"True relevant features: q_c, N_d\")\n",
    "    print()\n",
    "    \n",
    "    # Screen variables\n",
    "    screener = PANSRVariableScreener(\n",
    "        n_estimators=200,\n",
    "        importance_threshold=0.05\n",
    "    )\n",
    "    result = screener.screen(X, y, feature_names)\n",
    "    \n",
    "    # Verify output keys match v4.1 specification\n",
    "    print(\"Output Dictionary Keys:\")\n",
    "    for key in result.keys():\n",
    "        print(f\"  {key}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Selected features (v4.1 key): {result['selected_features']}\")\n",
    "    print(f\"RF R-squared: {result['rf_r2']:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Print importance ranking\n",
    "    print(\"Importance Ranking:\")\n",
    "    for name, imp in result['importance_ranking']:\n",
    "        marker = \" <-- TRUE\" if name in ['q_c', 'N_d'] else \"\"\n",
    "        print(f\"  {name}: {imp:.4f}{marker}\")\n",
    "    \n",
    "    # Verify true features are selected\n",
    "    if 'q_c' in result['selected_features'] and 'N_d' in result['selected_features']:\n",
    "        print(\"\\n[PASS] Both true features selected\")\n",
    "    else:\n",
    "        print(f\"\\n[WARNING] Not all true features selected: {result['selected_features']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 2: Noise vs True Features\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 2: Noise vs True Features\")\n",
    "    \n",
    "    # Generate data with noise features\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500\n",
    "    \n",
    "    # True features\n",
    "    x1 = np.random.uniform(0.1, 1.0, n_samples)\n",
    "    x2 = np.random.uniform(0.1, 1.0, n_samples)\n",
    "    \n",
    "    # Noise features\n",
    "    noise1 = np.random.randn(n_samples)\n",
    "    noise2 = np.random.randn(n_samples)\n",
    "    noise3 = np.random.randn(n_samples)\n",
    "    \n",
    "    # True equation: y = x1^2 + x2\n",
    "    y = x1**2 + x2 + 0.01 * np.random.randn(n_samples)\n",
    "    \n",
    "    X = np.column_stack([x1, x2, noise1, noise2, noise3])\n",
    "    feature_names = ['x1', 'x2', 'noise1', 'noise2', 'noise3']\n",
    "    \n",
    "    print(f\"True equation: y = x1^2 + x2\")\n",
    "    print(f\"Features: {feature_names}\")\n",
    "    print()\n",
    "    \n",
    "    # Screen\n",
    "    screener = PANSRVariableScreener(\n",
    "        n_estimators=200,\n",
    "        importance_threshold=0.05\n",
    "    )\n",
    "    result = screener.screen(X, y, feature_names)\n",
    "    \n",
    "    # Print ranking\n",
    "    print(\"Importance Ranking:\")\n",
    "    for name, imp in result['importance_ranking']:\n",
    "        marker = \" <-- TRUE\" if name in ['x1', 'x2'] else \"\"\n",
    "        print(f\"  {name}: {imp:.4f}{marker}\")\n",
    "    \n",
    "    print(f\"\\nSelected: {result['selected_features']}\")\n",
    "    \n",
    "    # Verify\n",
    "    noise_selected = [n for n in result['selected_features'] if 'noise' in n]\n",
    "    if len(noise_selected) == 0:\n",
    "        print(\"[PASS] No noise features selected\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Noise features selected: {noise_selected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 3: Nonlinear Relationship Detection\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 3: Nonlinear Relationship Detection\")\n",
    "    \n",
    "    # Generate data with nonlinear relationships that LARS would miss\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500\n",
    "    \n",
    "    # x1 with symmetric range - correlation with sin(x1) would be ~0\n",
    "    x1 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    x2 = np.random.uniform(0, 1, n_samples)\n",
    "    noise_feat = np.random.randn(n_samples)\n",
    "    \n",
    "    # True equation: y = sin(x1) + x2^2\n",
    "    y = np.sin(x1) + x2**2 + 0.01 * np.random.randn(n_samples)\n",
    "    \n",
    "    X = np.column_stack([x1, x2, noise_feat])\n",
    "    feature_names = ['x1', 'x2', 'noise']\n",
    "    \n",
    "    print(f\"True equation: y = sin(x1) + x2^2\")\n",
    "    print(f\"Note: x1 has symmetric range, so corr(x1, sin(x1)) ~ 0\")\n",
    "    print(f\"LARS would fail to detect x1's importance\")\n",
    "    print()\n",
    "    \n",
    "    # Compute linear correlation (what LARS would use)\n",
    "    corr_x1 = np.corrcoef(x1, y)[0, 1]\n",
    "    corr_x2 = np.corrcoef(x2, y)[0, 1]\n",
    "    print(f\"Linear correlations (LARS perspective):\")\n",
    "    print(f\"  corr(x1, y) = {corr_x1:.4f}\")\n",
    "    print(f\"  corr(x2, y) = {corr_x2:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Screen with RF\n",
    "    screener = PANSRVariableScreener(\n",
    "        n_estimators=200,\n",
    "        importance_threshold=0.05\n",
    "    )\n",
    "    result = screener.screen(X, y, feature_names)\n",
    "    \n",
    "    print(\"RF Permutation Importance:\")\n",
    "    for name, imp in result['importance_ranking']:\n",
    "        print(f\"  {name}: {imp:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSelected: {result['selected_features']}\")\n",
    "    \n",
    "    # Verify\n",
    "    if 'x1' in result['selected_features'] and 'x2' in result['selected_features']:\n",
    "        print(\"[PASS] RF correctly detected both nonlinear relationships\")\n",
    "    else:\n",
    "        print(\"[WARNING] RF did not detect both true features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 4: Threshold Sensitivity\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 4: Threshold Sensitivity\")\n",
    "    \n",
    "    # Use warm rain data\n",
    "    X, y, feature_names, _ = generate_warm_rain_data(\n",
    "        n_samples=500, noise_level=0.01\n",
    "    )\n",
    "    \n",
    "    thresholds = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "    \n",
    "    print(f\"Testing different importance thresholds:\")\n",
    "    print(f\"{'Threshold':<12} {'N Selected':<12} {'Selected Features'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        screener = PANSRVariableScreener(\n",
    "            n_estimators=100,\n",
    "            importance_threshold=thresh\n",
    "        )\n",
    "        result = screener.screen(X, y, feature_names)\n",
    "        print(f\"{thresh:<12} {result['n_selected']:<12} {result['selected_features']}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Note: Threshold 0.05-0.1 typically works well for physics problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 5: Full Report Output\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 5: Full Screening Report\")\n",
    "    \n",
    "    # Use warm rain data\n",
    "    X, y, feature_names, _ = generate_warm_rain_data(\n",
    "        n_samples=500, noise_level=0.01, seed=42\n",
    "    )\n",
    "    \n",
    "    screener = PANSRVariableScreener(\n",
    "        n_estimators=200,\n",
    "        importance_threshold=0.05\n",
    "    )\n",
    "    result = screener.screen(X, y, feature_names)\n",
    "    \n",
    "    # Print full report\n",
    "    screener.print_screening_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Module Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODULE SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" 02_VariableScreening.ipynb v4.1 - Module Summary\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"CLASS: PANSRVariableScreener\")\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(\"Purpose:\")\n",
    "print(\"  Select important variables using Random Forest permutation importance.\")\n",
    "print(\"  Detects nonlinear dependencies that LARS misses.\")\n",
    "print()\n",
    "print(\"Public Attributes (v4.1):\")\n",
    "print(\"  rf_model           - Fitted Random Forest model\")\n",
    "print(\"  importance_scores  - Dict of raw importance scores\")\n",
    "print()\n",
    "print(\"Main Methods:\")\n",
    "print(\"  screen(X, y, feature_names)\")\n",
    "print(\"      Perform variable screening\")\n",
    "print(\"      Returns: dict with selected_features, importance_scores, etc.\")\n",
    "print()\n",
    "print(\"  get_selected_features()\")\n",
    "print(\"      Get list of selected feature names\")\n",
    "print()\n",
    "print(\"  get_importance_ranking()\")\n",
    "print(\"      Get features ranked by importance\")\n",
    "print()\n",
    "print(\"  get_gini_importance()\")\n",
    "print(\"      Get Gini importance (faster alternative)\")\n",
    "print()\n",
    "print(\"  print_screening_report()\")\n",
    "print(\"      Print detailed screening report\")\n",
    "print()\n",
    "print(\"Output Dictionary Keys (v4.1):\")\n",
    "print(\"  - selected_features    : List of selected feature names\")\n",
    "print(\"  - selected_indices     : List of selected feature indices\")\n",
    "print(\"  - importance_scores    : Dict of raw importance\")\n",
    "print(\"  - normalized_importance: Dict of normalized importance\")\n",
    "print(\"  - importance_ranking   : List of (name, importance) tuples\")\n",
    "print()\n",
    "print(\"Usage Example:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "# Create screener\n",
    "screener = PANSRVariableScreener(\n",
    "    n_estimators=500,\n",
    "    importance_threshold=0.01\n",
    ")\n",
    "\n",
    "# Run screening\n",
    "result = screener.screen(X, y, feature_names)\n",
    "\n",
    "# Get selected features (v4.1 key)\n",
    "selected = result['selected_features']\n",
    "print(f\"Selected: {selected}\")\n",
    "\n",
    "# View importance ranking\n",
    "for name, importance in result['importance_ranking']:\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "\"\"\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"Module loaded successfully. Import via: %run 02_VariableScreening.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
