{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00_Core - Physics-SR Framework v3.0\n",
    "\n",
    "## Foundation Module: DataClasses, Utilities, and Test Data Generators\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook provides the foundational components for the Three-Stage Physics-Informed Symbolic Regression Framework:\n",
    "\n",
    "1. **DataClasses**: `UserInputs`, `Stage1Results`, `Stage2Results`, `Stage3Results`\n",
    "2. **Utility Functions**: Safe math operations, formatting, metrics\n",
    "3. **Test Data Generators**: Warm rain microphysics, polynomial, trigonometric\n",
    "4. **Global Configuration Constants**\n",
    "\n",
    "### Usage\n",
    "\n",
    "This module is imported by all other notebooks via:\n",
    "```python\n",
    "%run 00_Core.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "00_Core.ipynb - Foundation Module\n",
    "==================================\n",
    "\n",
    "Three-Stage Physics-Informed Symbolic Regression Framework v3.0\n",
    "\n",
    "This module provides:\n",
    "- DataClasses for user inputs and stage results\n",
    "- Utility functions for safe numerical operations\n",
    "- Test data generators for algorithm validation\n",
    "- Global configuration constants\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Symbolic computation\n",
    "import sympy as sp\n",
    "\n",
    "print(\"00_Core: All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GLOBAL CONFIGURATION CONSTANTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Stage 1 defaults\n",
    "DEFAULT_MAX_EXPONENT = 4              # Buckingham pi search range\n",
    "DEFAULT_IMPORTANCE_THRESHOLD = 0.01  # Variable screening threshold\n",
    "DEFAULT_POWERLAW_R2_THRESHOLD = 0.9  # Power-law detection R^2 threshold\n",
    "DEFAULT_SOFTMAX_TEMPERATURE = 0.5    # iRF soft reweighting temperature\n",
    "DEFAULT_STABILITY_THRESHOLD = 0.5    # Interaction stability threshold\n",
    "\n",
    "# Stage 2 defaults\n",
    "DEFAULT_MAX_POLY_DEGREE = 3          # Feature library polynomial degree\n",
    "DEFAULT_STLSQ_THRESHOLD = 0.1        # STLSQ sparsity threshold\n",
    "DEFAULT_STLSQ_MAX_ITER = 20          # STLSQ maximum iterations\n",
    "DEFAULT_ALASSO_GAMMA = 1.0           # Adaptive Lasso gamma parameter\n",
    "DEFAULT_ALASSO_EPS = 1e-6            # Adaptive Lasso stabilization constant\n",
    "\n",
    "# Stage 3 defaults\n",
    "DEFAULT_CV_FOLDS = 5                 # Cross-validation folds\n",
    "DEFAULT_EBIC_GAMMA = 0.5             # EBIC gamma parameter\n",
    "DEFAULT_N_BOOTSTRAP = 200            # Number of bootstrap samples\n",
    "DEFAULT_CONFIDENCE_LEVEL = 0.95      # Confidence interval level\n",
    "DEFAULT_DIM_TOLERANCE = 0.05         # Dimensional check tolerance\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Numerical stability constants\n",
    "EPS_LOG = 1e-10                      # Epsilon for log safety\n",
    "EPS_DIV = 1e-10                      # Epsilon for division safety\n",
    "\n",
    "print(\"Global configuration constants defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: DataClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# USER INPUTS DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class UserInputs:\n",
    "    \"\"\"\n",
    "    User-defined inputs required for the Physics-SR Framework.\n",
    "    \n",
    "    These inputs must be prepared before running the pipeline and require\n",
    "    domain knowledge about the physical system being modeled.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    variable_dimensions : Dict[str, List[float]]\n",
    "        Dictionary mapping variable names to their dimensional exponents [M, L, T, Theta].\n",
    "        M = Mass, L = Length, T = Time, Theta = Temperature.\n",
    "        Example: {'velocity': [0, 1, -1, 0]}  # m/s has L^1 * T^-1\n",
    "        \n",
    "    target_dimensions : List[float]\n",
    "        Dimensional exponents [M, L, T, Theta] for the target variable.\n",
    "        Example: [0, 0, -1, 0] for a rate with units s^-1\n",
    "        \n",
    "    physical_bounds : Dict[str, Dict[str, Optional[float]]]\n",
    "        Physical constraints for variables and target.\n",
    "        Format: {var_name: {'min': float or None, 'max': float or None}}\n",
    "        Example: {'target': {'min': 0, 'max': None}}  # Non-negative target\n",
    "        \n",
    "    variable_mapping : Optional[Dict[str, str]]\n",
    "        Maps data column names to standardized physical variable names.\n",
    "        Example: {'cloud_water_mixing_ratio': 'q_c'}\n",
    "        \n",
    "    unit_conversions : Optional[Dict[str, float]]\n",
    "        Conversion factors to convert data to SI units.\n",
    "        Example: {'radius_um': 1e-6}  # Convert micrometers to meters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Required fields\n",
    "    variable_dimensions: Dict[str, List[float]]\n",
    "    target_dimensions: List[float]\n",
    "    physical_bounds: Dict[str, Dict[str, Optional[float]]]\n",
    "    \n",
    "    # Optional fields with defaults\n",
    "    variable_mapping: Optional[Dict[str, str]] = None\n",
    "    unit_conversions: Optional[Dict[str, float]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate inputs after initialization.\"\"\"\n",
    "        # Validate dimensional exponents have length 4\n",
    "        for var_name, dims in self.variable_dimensions.items():\n",
    "            if len(dims) != 4:\n",
    "                raise ValueError(\n",
    "                    f\"Variable '{var_name}' has {len(dims)} dimensional exponents, \"\n",
    "                    f\"expected 4 [M, L, T, Theta]\"\n",
    "                )\n",
    "        \n",
    "        if len(self.target_dimensions) != 4:\n",
    "            raise ValueError(\n",
    "                f\"Target dimensions has {len(self.target_dimensions)} exponents, \"\n",
    "                f\"expected 4 [M, L, T, Theta]\"\n",
    "            )\n",
    "    \n",
    "    def get_variable_names(self) -> List[str]:\n",
    "        \"\"\"Return list of variable names.\"\"\"\n",
    "        return list(self.variable_dimensions.keys())\n",
    "    \n",
    "    def get_dimension_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Return dimensional matrix D where D[i,j] = exponent of dimension i for variable j.\"\"\"\n",
    "        var_names = self.get_variable_names()\n",
    "        n_vars = len(var_names)\n",
    "        D = np.zeros((4, n_vars))\n",
    "        for j, var_name in enumerate(var_names):\n",
    "            D[:, j] = self.variable_dimensions[var_name]\n",
    "        return D\n",
    "\n",
    "print(\"UserInputs dataclass defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STAGE 1 RESULTS DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Stage1Results:\n",
    "    \"\"\"\n",
    "    Results from Stage 1: Variable Selection & Preprocessing.\n",
    "    \n",
    "    Contains outputs from:\n",
    "    - 1.1 Buckingham Pi Dimensional Analysis\n",
    "    - 1.2 PAN+SR Variable Screening\n",
    "    - 1.3 Symmetry Analysis\n",
    "    - 1.4 iRF Interaction Discovery\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    pi_groups : Optional[Dict[str, np.ndarray]]\n",
    "        Dictionary mapping pi-group names to their exponent vectors\n",
    "    pi_exponents : Optional[np.ndarray]\n",
    "        Matrix of exponents for selected pi-groups (n_groups x n_variables)\n",
    "    pi_group_names : Optional[List[str]]\n",
    "        Human-readable names for each pi-group\n",
    "    X_transformed : Optional[np.ndarray]\n",
    "        Data transformed to dimensionless pi-groups\n",
    "    selected_indices : Optional[List[int]]\n",
    "        Indices of variables selected by screening\n",
    "    selected_names : Optional[List[str]]\n",
    "        Names of selected variables\n",
    "    importance_scores : Optional[Dict[str, float]]\n",
    "        RF permutation importance for each variable\n",
    "    is_power_law : bool\n",
    "        Whether power-law relationship was detected\n",
    "    estimated_exponents : Optional[Dict[str, float]]\n",
    "        Estimated power-law exponents for each variable\n",
    "    power_law_r2 : Optional[float]\n",
    "        R-squared of log-log regression\n",
    "    structural_hints : Optional[Dict]\n",
    "        Hints about equation structure for Stage 2\n",
    "    stable_interactions : Optional[List[Tuple[int, ...]]]\n",
    "        List of stable feature interactions (as tuples of indices)\n",
    "    interaction_stability : Optional[Dict[Tuple, float]]\n",
    "        Stability scores for each interaction\n",
    "    soft_weights : Optional[np.ndarray]\n",
    "        Soft reweighting weights from iRF\n",
    "    \"\"\"\n",
    "    \n",
    "    # Buckingham Pi results\n",
    "    pi_groups: Optional[Dict[str, np.ndarray]] = None\n",
    "    pi_exponents: Optional[np.ndarray] = None\n",
    "    pi_group_names: Optional[List[str]] = None\n",
    "    X_transformed: Optional[np.ndarray] = None\n",
    "    \n",
    "    # Variable screening results\n",
    "    selected_indices: Optional[List[int]] = None\n",
    "    selected_names: Optional[List[str]] = None\n",
    "    importance_scores: Optional[Dict[str, float]] = None\n",
    "    \n",
    "    # Symmetry analysis results\n",
    "    is_power_law: bool = False\n",
    "    estimated_exponents: Optional[Dict[str, float]] = None\n",
    "    power_law_r2: Optional[float] = None\n",
    "    structural_hints: Optional[Dict] = None\n",
    "    \n",
    "    # Interaction discovery results\n",
    "    stable_interactions: Optional[List[Tuple[int, ...]]] = None\n",
    "    interaction_stability: Optional[Dict[Tuple, float]] = None\n",
    "    soft_weights: Optional[np.ndarray] = None\n",
    "\n",
    "print(\"Stage1Results dataclass defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STAGE 2 RESULTS DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Stage2Results:\n",
    "    \"\"\"\n",
    "    Results from Stage 2: Structure Discovery.\n",
    "    \n",
    "    Contains outputs from:\n",
    "    - 2.1 Feature Library Construction\n",
    "    - 2.2a PySR Genetic Programming\n",
    "    - 2.2b E-WSINDy with STLSQ\n",
    "    - 2.2c Adaptive Lasso\n",
    "    - 2.3 Structure Parsing\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    feature_library : Optional[np.ndarray]\n",
    "        Feature library matrix Phi (n_samples x n_features)\n",
    "    feature_names : Optional[List[str]]\n",
    "        Names of features in the library\n",
    "    scaler : Optional[StandardScaler]\n",
    "        Fitted scaler for feature normalization\n",
    "    pysr_equations : Optional[List[str]]\n",
    "        List of equations discovered by PySR\n",
    "    pysr_pareto : Optional[pd.DataFrame]\n",
    "        Pareto front of complexity vs accuracy\n",
    "    best_pysr_equation : Optional[str]\n",
    "        Best equation from PySR\n",
    "    best_pysr_sympy : Optional[sp.Expr]\n",
    "        Best PySR equation as SymPy expression\n",
    "    refined_features : Optional[np.ndarray]\n",
    "        Features extracted from PySR equation structure\n",
    "    refined_feature_names : Optional[List[str]]\n",
    "        Names of refined features\n",
    "    stlsq_coefficients : Optional[np.ndarray]\n",
    "        Coefficients from STLSQ\n",
    "    stlsq_support : Optional[np.ndarray]\n",
    "        Boolean mask of selected features from STLSQ\n",
    "    stlsq_equation : Optional[str]\n",
    "        Equation string from STLSQ\n",
    "    weak_form_Q : Optional[np.ndarray]\n",
    "        Weak-form feature matrix\n",
    "    weak_form_b : Optional[np.ndarray]\n",
    "        Weak-form target vector\n",
    "    alasso_coefficients : Optional[np.ndarray]\n",
    "        Coefficients from Adaptive Lasso\n",
    "    alasso_support : Optional[np.ndarray]\n",
    "        Boolean mask of selected features from Adaptive Lasso\n",
    "    alasso_equation : Optional[str]\n",
    "        Equation string from Adaptive Lasso\n",
    "    alasso_lambda : Optional[float]\n",
    "        Optimal lambda from Adaptive Lasso\n",
    "    \"\"\"\n",
    "    \n",
    "    # Feature library\n",
    "    feature_library: Optional[np.ndarray] = None\n",
    "    feature_names: Optional[List[str]] = None\n",
    "    scaler: Optional[Any] = None  # StandardScaler, but using Any for flexibility\n",
    "    \n",
    "    # PySR results\n",
    "    pysr_equations: Optional[List[str]] = None\n",
    "    pysr_pareto: Optional[pd.DataFrame] = None\n",
    "    best_pysr_equation: Optional[str] = None\n",
    "    best_pysr_sympy: Optional[Any] = None  # sp.Expr\n",
    "    refined_features: Optional[np.ndarray] = None\n",
    "    refined_feature_names: Optional[List[str]] = None\n",
    "    \n",
    "    # E-WSINDy/STLSQ results\n",
    "    stlsq_coefficients: Optional[np.ndarray] = None\n",
    "    stlsq_support: Optional[np.ndarray] = None\n",
    "    stlsq_equation: Optional[str] = None\n",
    "    weak_form_Q: Optional[np.ndarray] = None\n",
    "    weak_form_b: Optional[np.ndarray] = None\n",
    "    \n",
    "    # Adaptive Lasso results\n",
    "    alasso_coefficients: Optional[np.ndarray] = None\n",
    "    alasso_support: Optional[np.ndarray] = None\n",
    "    alasso_equation: Optional[str] = None\n",
    "    alasso_lambda: Optional[float] = None\n",
    "\n",
    "print(\"Stage2Results dataclass defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STAGE 3 RESULTS DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Stage3Results:\n",
    "    \"\"\"\n",
    "    Results from Stage 3: Validation & Uncertainty Quantification.\n",
    "    \n",
    "    Contains outputs from:\n",
    "    - 3.1 Model Selection (CV + EBIC)\n",
    "    - 3.2 Physics Verification (Dimensional + Bounds)\n",
    "    - 3.3 Three-Layer UQ (Structural, Parametric, Predictive)\n",
    "    - 3.4 Statistical Inference\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    cv_scores : Optional[Dict[str, Tuple[float, float]]]\n",
    "        Cross-validation scores {model_name: (mean, std)}\n",
    "    ebic_scores : Optional[Dict[str, float]]\n",
    "        EBIC scores for each model\n",
    "    best_model : Optional[str]\n",
    "        Name of selected best model\n",
    "    dim_consistent : bool\n",
    "        Whether equation is dimensionally consistent\n",
    "    dim_details : Optional[Dict]\n",
    "        Detailed dimensional analysis per term\n",
    "    bounds_violations : Optional[Dict]\n",
    "        Physical bounds violation statistics\n",
    "    physics_score : Optional[float]\n",
    "        Overall physics verification score\n",
    "    inclusion_probabilities : Optional[np.ndarray]\n",
    "        Bootstrap inclusion probability for each feature\n",
    "    structural_confidence : Optional[Dict[str, str]]\n",
    "        Confidence classification per feature (HIGH/MEDIUM/LOW)\n",
    "    coefficient_estimates : Optional[np.ndarray]\n",
    "        Point estimates for coefficients\n",
    "    coefficient_CI : Optional[np.ndarray]\n",
    "        95% confidence intervals (n_features x 2)\n",
    "    coefficient_SE : Optional[np.ndarray]\n",
    "        Standard errors for coefficients\n",
    "    prediction_intervals : Optional[Tuple[np.ndarray, np.ndarray]]\n",
    "        Prediction intervals (lower, upper)\n",
    "    pi_coverage : Optional[float]\n",
    "        Empirical coverage of prediction intervals\n",
    "    p_values : Optional[Dict[str, float]]\n",
    "        P-values from hypothesis tests\n",
    "    significant_terms : Optional[List[str]]\n",
    "        Terms that are statistically significant\n",
    "    final_equation : Optional[str]\n",
    "        Final equation string\n",
    "    final_coefficients : Optional[Dict[str, float]]\n",
    "        Final coefficient values by term name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model selection\n",
    "    cv_scores: Optional[Dict[str, Tuple[float, float]]] = None\n",
    "    ebic_scores: Optional[Dict[str, float]] = None\n",
    "    best_model: Optional[str] = None\n",
    "    \n",
    "    # Physics verification\n",
    "    dim_consistent: bool = False\n",
    "    dim_details: Optional[Dict] = None\n",
    "    bounds_violations: Optional[Dict] = None\n",
    "    physics_score: Optional[float] = None\n",
    "    \n",
    "    # Structural UQ (Layer 1)\n",
    "    inclusion_probabilities: Optional[np.ndarray] = None\n",
    "    structural_confidence: Optional[Dict[str, str]] = None\n",
    "    \n",
    "    # Parametric UQ (Layer 2)\n",
    "    coefficient_estimates: Optional[np.ndarray] = None\n",
    "    coefficient_CI: Optional[np.ndarray] = None\n",
    "    coefficient_SE: Optional[np.ndarray] = None\n",
    "    \n",
    "    # Predictive UQ (Layer 3)\n",
    "    prediction_intervals: Optional[Tuple[np.ndarray, np.ndarray]] = None\n",
    "    pi_coverage: Optional[float] = None\n",
    "    \n",
    "    # Hypothesis testing\n",
    "    p_values: Optional[Dict[str, float]] = None\n",
    "    significant_terms: Optional[List[str]] = None\n",
    "    \n",
    "    # Final equation\n",
    "    final_equation: Optional[str] = None\n",
    "    final_coefficients: Optional[Dict[str, float]] = None\n",
    "\n",
    "print(\"Stage3Results dataclass defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# NUMERICAL SAFETY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def safe_log(x: np.ndarray, eps: float = EPS_LOG) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute logarithm with numerical safety.\n",
    "    \n",
    "    Prevents log(0) by adding a small epsilon to values below threshold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input array (should be positive)\n",
    "    eps : float\n",
    "        Small constant to prevent log(0)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        log(max(x, eps))\n",
    "    \"\"\"\n",
    "    return np.log(np.maximum(np.abs(x), eps))\n",
    "\n",
    "\n",
    "def safe_divide(a: np.ndarray, b: np.ndarray, eps: float = EPS_DIV) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute division with numerical safety.\n",
    "    \n",
    "    Prevents division by zero by ensuring denominator is at least eps.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : np.ndarray\n",
    "        Numerator\n",
    "    b : np.ndarray\n",
    "        Denominator\n",
    "    eps : float\n",
    "        Small constant to prevent division by zero\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        a / (b + sign(b) * eps), preserving sign of b\n",
    "    \"\"\"\n",
    "    # Handle both positive and negative denominators\n",
    "    sign_b = np.sign(b)\n",
    "    sign_b[sign_b == 0] = 1  # Default to positive for zero\n",
    "    safe_b = np.maximum(np.abs(b), eps) * sign_b\n",
    "    return a / safe_b\n",
    "\n",
    "\n",
    "def safe_power(base: np.ndarray, exponent: float, eps: float = EPS_LOG) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute power with numerical safety for non-integer exponents.\n",
    "    \n",
    "    For non-integer exponents, ensures base is positive.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base : np.ndarray\n",
    "        Base values\n",
    "    exponent : float\n",
    "        Exponent\n",
    "    eps : float\n",
    "        Small constant for numerical safety\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        base ** exponent, with numerical safety\n",
    "    \"\"\"\n",
    "    if exponent == int(exponent):\n",
    "        # Integer exponent: direct computation is safe\n",
    "        return np.power(base, exponent)\n",
    "    else:\n",
    "        # Non-integer exponent: ensure positive base\n",
    "        safe_base = np.maximum(np.abs(base), eps)\n",
    "        return np.power(safe_base, exponent)\n",
    "\n",
    "print(\"Numerical safety functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METRIC FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_r2(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute coefficient of determination (R-squared).\n",
    "    \n",
    "    R^2 = 1 - SS_res / SS_tot\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True target values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        R-squared value\n",
    "    \"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    if ss_tot < EPS_DIV:\n",
    "        return 0.0\n",
    "    return 1.0 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    MSE = (1/n) * sum((y_true - y_pred)^2)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True target values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean squared error\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def compute_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Root Mean Squared Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True target values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Root mean squared error\n",
    "    \"\"\"\n",
    "    return np.sqrt(compute_mse(y_true, y_pred))\n",
    "\n",
    "\n",
    "def compute_mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True target values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean absolute error\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "print(\"Metric functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FORMATTING FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def format_equation(\n",
    "    coefficients: np.ndarray,\n",
    "    feature_names: List[str],\n",
    "    threshold: float = 1e-6,\n",
    "    precision: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Format coefficients and feature names into an equation string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefficients : np.ndarray\n",
    "        Coefficient values\n",
    "    feature_names : List[str]\n",
    "        Names of features\n",
    "    threshold : float\n",
    "        Coefficients below this threshold are treated as zero\n",
    "    precision : int\n",
    "        Number of decimal places for coefficients\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Formatted equation string\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    \n",
    "    for coef, name in zip(coefficients, feature_names):\n",
    "        if np.abs(coef) > threshold:\n",
    "            if name == '1' or name == 'intercept':\n",
    "                terms.append(f\"{coef:.{precision}f}\")\n",
    "            else:\n",
    "                if coef > 0 and len(terms) > 0:\n",
    "                    terms.append(f\"+ {coef:.{precision}f}*{name}\")\n",
    "                elif coef < 0:\n",
    "                    terms.append(f\"- {abs(coef):.{precision}f}*{name}\")\n",
    "                else:\n",
    "                    terms.append(f\"{coef:.{precision}f}*{name}\")\n",
    "    \n",
    "    if len(terms) == 0:\n",
    "        return \"0\"\n",
    "    \n",
    "    equation = \" \".join(terms)\n",
    "    # Clean up leading +\n",
    "    if equation.startswith(\"+ \"):\n",
    "        equation = equation[2:]\n",
    "    \n",
    "    return equation\n",
    "\n",
    "\n",
    "def print_section_header(title: str, width: int = 70, char: str = \"=\") -> None:\n",
    "    \"\"\"\n",
    "    Print a formatted section header.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        Section title\n",
    "    width : int\n",
    "        Total width of header\n",
    "    char : str\n",
    "        Character to use for border\n",
    "    \"\"\"\n",
    "    print(char * width)\n",
    "    print(f\" {title}\")\n",
    "    print(char * width)\n",
    "\n",
    "\n",
    "def print_subsection_header(title: str, width: int = 70, char: str = \"-\") -> None:\n",
    "    \"\"\"\n",
    "    Print a formatted subsection header.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        Subsection title\n",
    "    width : int\n",
    "        Total width of header\n",
    "    char : str\n",
    "        Character to use for border\n",
    "    \"\"\"\n",
    "    print(char * width)\n",
    "    print(f\" {title}\")\n",
    "    print(char * width)\n",
    "\n",
    "print(\"Formatting functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA PREPROCESSING FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def normalize_features(\n",
    "    X: np.ndarray,\n",
    "    scaler: Optional[StandardScaler] = None\n",
    ") -> Tuple[np.ndarray, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Normalize features to zero mean and unit variance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    scaler : Optional[StandardScaler]\n",
    "        Pre-fitted scaler. If None, a new scaler is fitted.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, StandardScaler]\n",
    "        Normalized features and fitted scaler\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_normalized = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_normalized = scaler.transform(X)\n",
    "    \n",
    "    return X_normalized, scaler\n",
    "\n",
    "\n",
    "def check_data_validity(X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Check data for common issues.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary with validity checks:\n",
    "        - 'valid': bool, overall validity\n",
    "        - 'n_samples': int\n",
    "        - 'n_features': int\n",
    "        - 'has_nan': bool\n",
    "        - 'has_inf': bool\n",
    "        - 'n_nan': int\n",
    "        - 'n_inf': int\n",
    "        - 'warnings': List[str]\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'valid': True,\n",
    "        'n_samples': X.shape[0],\n",
    "        'n_features': X.shape[1],\n",
    "        'has_nan': False,\n",
    "        'has_inf': False,\n",
    "        'n_nan': 0,\n",
    "        'n_inf': 0,\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # Check X\n",
    "    n_nan_X = np.sum(np.isnan(X))\n",
    "    n_inf_X = np.sum(np.isinf(X))\n",
    "    \n",
    "    # Check y\n",
    "    n_nan_y = np.sum(np.isnan(y))\n",
    "    n_inf_y = np.sum(np.isinf(y))\n",
    "    \n",
    "    result['n_nan'] = n_nan_X + n_nan_y\n",
    "    result['n_inf'] = n_inf_X + n_inf_y\n",
    "    \n",
    "    if result['n_nan'] > 0:\n",
    "        result['has_nan'] = True\n",
    "        result['valid'] = False\n",
    "        result['warnings'].append(f\"Data contains {result['n_nan']} NaN values\")\n",
    "    \n",
    "    if result['n_inf'] > 0:\n",
    "        result['has_inf'] = True\n",
    "        result['valid'] = False\n",
    "        result['warnings'].append(f\"Data contains {result['n_inf']} Inf values\")\n",
    "    \n",
    "    # Check sample size\n",
    "    if result['n_samples'] < 10:\n",
    "        result['warnings'].append(f\"Very small sample size: {result['n_samples']}\")\n",
    "    \n",
    "    # Check feature count\n",
    "    if result['n_features'] > result['n_samples']:\n",
    "        result['warnings'].append(\n",
    "            f\"More features ({result['n_features']}) than samples ({result['n_samples']})\"\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Data preprocessing functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Test Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# WARM RAIN MICROPHYSICS DATA GENERATOR\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_warm_rain_data(\n",
    "    n_samples: int = 1000,\n",
    "    noise_level: float = 0.01,\n",
    "    seed: int = RANDOM_SEED,\n",
    "    include_irrelevant: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str], UserInputs]:\n",
    "    \"\"\"\n",
    "    Generate synthetic warm rain microphysics data.\n",
    "    \n",
    "    True equation: dq_r/dt = 0.89 * q_c^2.47 * N_d^(-1.79)\n",
    "    \n",
    "    This is a Kessler-type autoconversion parameterization where:\n",
    "    - q_c: cloud water mixing ratio (kg/kg) - dimensionless\n",
    "    - N_d: droplet number concentration (m^-3)\n",
    "    - dq_r/dt: rain water autoconversion rate (s^-1)\n",
    "    \n",
    "    Additional variables (r_eff, LWC) are included but do not appear in\n",
    "    the true equation, serving as irrelevant features for testing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    noise_level : float\n",
    "        Relative noise standard deviation (0.01 = 1% noise)\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    include_irrelevant : bool\n",
    "        Whether to include irrelevant features (r_eff, LWC)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target vector (n_samples,) - autoconversion rate\n",
    "    feature_names : List[str]\n",
    "        List of feature names\n",
    "    user_inputs : UserInputs\n",
    "        Complete UserInputs with dimensional information\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # True equation coefficients\n",
    "    C = 0.89       # Leading coefficient\n",
    "    alpha = 2.47   # Exponent for q_c\n",
    "    beta = -1.79   # Exponent for N_d\n",
    "    \n",
    "    # Generate physically realistic ranges\n",
    "    # q_c: cloud water mixing ratio (0.1 - 5 g/kg = 1e-4 to 5e-3 kg/kg)\n",
    "    q_c = np.random.uniform(1e-4, 5e-3, n_samples)\n",
    "    \n",
    "    # N_d: droplet number concentration (10 - 500 cm^-3 = 1e7 to 5e8 m^-3)\n",
    "    N_d = np.random.uniform(1e7, 5e8, n_samples)\n",
    "    \n",
    "    # Additional variables (not in true equation)\n",
    "    # r_eff: effective radius (5 - 25 um = 5e-6 to 25e-6 m)\n",
    "    r_eff = np.random.uniform(5e-6, 25e-6, n_samples)\n",
    "    \n",
    "    # LWC: liquid water content (0.1 - 2.0 g/m^3 = 0.1 - 2.0 kg/m^3 for this scale)\n",
    "    LWC = np.random.uniform(0.1, 2.0, n_samples)\n",
    "    \n",
    "    # True autoconversion rate (Kessler-type parameterization)\n",
    "    y_true = C * (q_c ** alpha) * (N_d ** beta)\n",
    "    \n",
    "    # Add multiplicative log-normal noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.exp(np.random.normal(0, noise_level, n_samples))\n",
    "        y = y_true * noise\n",
    "    else:\n",
    "        y = y_true.copy()\n",
    "    \n",
    "    # Ensure physical constraints (non-negative)\n",
    "    y = np.maximum(y, 1e-30)\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    if include_irrelevant:\n",
    "        X = np.column_stack([q_c, N_d, r_eff, LWC])\n",
    "        feature_names = ['q_c', 'N_d', 'r_eff', 'LWC']\n",
    "        variable_dimensions = {\n",
    "            'q_c':   [0, 0, 0, 0],     # kg/kg (dimensionless)\n",
    "            'N_d':   [0, -3, 0, 0],    # m^-3\n",
    "            'r_eff': [0, 1, 0, 0],     # m\n",
    "            'LWC':   [1, -3, 0, 0],    # kg/m^3\n",
    "        }\n",
    "    else:\n",
    "        X = np.column_stack([q_c, N_d])\n",
    "        feature_names = ['q_c', 'N_d']\n",
    "        variable_dimensions = {\n",
    "            'q_c':   [0, 0, 0, 0],     # kg/kg (dimensionless)\n",
    "            'N_d':   [0, -3, 0, 0],    # m^-3\n",
    "        }\n",
    "    \n",
    "    # Create UserInputs with complete dimensional information\n",
    "    user_inputs = UserInputs(\n",
    "        variable_dimensions=variable_dimensions,\n",
    "        target_dimensions=[0, 0, -1, 0],  # s^-1 (rate)\n",
    "        physical_bounds={\n",
    "            'target': {'min': 0, 'max': None},\n",
    "            'q_c': {'min': 0, 'max': 0.1},\n",
    "            'N_d': {'min': 0, 'max': 1e10},\n",
    "            'r_eff': {'min': 0, 'max': 1e-3},\n",
    "            'LWC': {'min': 0, 'max': 10},\n",
    "        },\n",
    "        variable_mapping=None,\n",
    "        unit_conversions=None\n",
    "    )\n",
    "    \n",
    "    return X, y, feature_names, user_inputs\n",
    "\n",
    "\n",
    "def get_warm_rain_ground_truth() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Return ground truth information for warm rain data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Ground truth information including:\n",
    "        - equation: str, equation string\n",
    "        - coefficients: dict of coefficient values\n",
    "        - active_features: list of features in true equation\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'equation': 'dq_r/dt = 0.89 * q_c^2.47 * N_d^(-1.79)',\n",
    "        'coefficients': {\n",
    "            'C': 0.89,\n",
    "            'alpha_q_c': 2.47,\n",
    "            'beta_N_d': -1.79\n",
    "        },\n",
    "        'active_features': ['q_c', 'N_d'],\n",
    "        'inactive_features': ['r_eff', 'LWC'],\n",
    "        'equation_type': 'power_law'\n",
    "    }\n",
    "\n",
    "print(\"Warm rain data generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# POLYNOMIAL DATA GENERATOR\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_polynomial_data(\n",
    "    n_samples: int = 1000,\n",
    "    noise_level: float = 0.01,\n",
    "    seed: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Generate synthetic polynomial regression data.\n",
    "    \n",
    "    True equation: y = 0.5*x1^2 + 0.3*x2*x3 - 0.1*x1*x2^2 + 0.8\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    noise_level : float\n",
    "        Additive noise standard deviation\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, 5) with x1, x2, x3 and 2 noise features\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "    feature_names : List[str]\n",
    "        List of feature names\n",
    "    true_coefficients : Dict[str, float]\n",
    "        True coefficient values\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate base features\n",
    "    x1 = np.random.uniform(-2, 2, n_samples)\n",
    "    x2 = np.random.uniform(-2, 2, n_samples)\n",
    "    x3 = np.random.uniform(-2, 2, n_samples)\n",
    "    \n",
    "    # Noise features (should not be selected)\n",
    "    noise1 = np.random.uniform(-2, 2, n_samples)\n",
    "    noise2 = np.random.uniform(-2, 2, n_samples)\n",
    "    \n",
    "    # True equation: y = 0.5*x1^2 + 0.3*x2*x3 - 0.1*x1*x2^2 + 0.8\n",
    "    y_true = 0.5 * x1**2 + 0.3 * x2 * x3 - 0.1 * x1 * x2**2 + 0.8\n",
    "    \n",
    "    # Add noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.random.normal(0, noise_level * np.std(y_true), n_samples)\n",
    "        y = y_true + noise\n",
    "    else:\n",
    "        y = y_true.copy()\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    X = np.column_stack([x1, x2, x3, noise1, noise2])\n",
    "    feature_names = ['x1', 'x2', 'x3', 'noise1', 'noise2']\n",
    "    \n",
    "    true_coefficients = {\n",
    "        'intercept': 0.8,\n",
    "        'x1^2': 0.5,\n",
    "        'x2*x3': 0.3,\n",
    "        'x1*x2^2': -0.1\n",
    "    }\n",
    "    \n",
    "    return X, y, feature_names, true_coefficients\n",
    "\n",
    "print(\"Polynomial data generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRIGONOMETRIC DATA GENERATOR\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_trigonometric_data(\n",
    "    n_samples: int = 1000,\n",
    "    noise_level: float = 0.01,\n",
    "    seed: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Generate synthetic trigonometric regression data.\n",
    "    \n",
    "    True equation: y = sin(x1) + 0.5*cos(x2) + 0.3*x1*x2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    noise_level : float\n",
    "        Additive noise standard deviation\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, 4) with x1, x2 and 2 noise features\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "    feature_names : List[str]\n",
    "        List of feature names\n",
    "    true_coefficients : Dict[str, float]\n",
    "        True coefficient values\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate base features (in range suitable for trig functions)\n",
    "    x1 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    x2 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    \n",
    "    # Noise features (should not be selected)\n",
    "    noise1 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    noise2 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    \n",
    "    # True equation: y = sin(x1) + 0.5*cos(x2) + 0.3*x1*x2\n",
    "    y_true = np.sin(x1) + 0.5 * np.cos(x2) + 0.3 * x1 * x2\n",
    "    \n",
    "    # Add noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.random.normal(0, noise_level * np.std(y_true), n_samples)\n",
    "        y = y_true + noise\n",
    "    else:\n",
    "        y = y_true.copy()\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    X = np.column_stack([x1, x2, noise1, noise2])\n",
    "    feature_names = ['x1', 'x2', 'noise1', 'noise2']\n",
    "    \n",
    "    true_coefficients = {\n",
    "        'sin(x1)': 1.0,\n",
    "        'cos(x2)': 0.5,\n",
    "        'x1*x2': 0.3\n",
    "    }\n",
    "    \n",
    "    return X, y, feature_names, true_coefficients\n",
    "\n",
    "print(\"Trigonometric data generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PENDULUM DATA GENERATOR (FOR BUCKINGHAM PI TESTING)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_pendulum_data(\n",
    "    n_samples: int = 500,\n",
    "    noise_level: float = 0.01,\n",
    "    seed: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str], UserInputs]:\n",
    "    \"\"\"\n",
    "    Generate synthetic simple pendulum period data.\n",
    "    \n",
    "    True equation: T = 2*pi * sqrt(L/g)\n",
    "    \n",
    "    Variables:\n",
    "    - L: pendulum length (m)\n",
    "    - m: mass (kg) - does not appear in true equation\n",
    "    - g: gravitational acceleration (m/s^2)\n",
    "    - T: period (s)\n",
    "    \n",
    "    This is a classic dimensional analysis example where:\n",
    "    T * sqrt(g/L) is the dimensionless group.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    noise_level : float\n",
    "        Relative noise standard deviation\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, 3) with L, m, g\n",
    "    y : np.ndarray\n",
    "        Target vector - period T\n",
    "    feature_names : List[str]\n",
    "        List of feature names\n",
    "    user_inputs : UserInputs\n",
    "        Complete UserInputs with dimensional information\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate physically realistic ranges\n",
    "    L = np.random.uniform(0.1, 2.0, n_samples)      # Length (m)\n",
    "    m = np.random.uniform(0.01, 1.0, n_samples)     # Mass (kg) - irrelevant\n",
    "    g = np.random.uniform(9.7, 10.0, n_samples)     # Gravity (m/s^2)\n",
    "    \n",
    "    # True period: T = 2*pi * sqrt(L/g)\n",
    "    T_true = 2 * np.pi * np.sqrt(L / g)\n",
    "    \n",
    "    # Add multiplicative noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.exp(np.random.normal(0, noise_level, n_samples))\n",
    "        T = T_true * noise\n",
    "    else:\n",
    "        T = T_true.copy()\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    X = np.column_stack([L, m, g])\n",
    "    feature_names = ['L', 'm', 'g']\n",
    "    \n",
    "    # Create UserInputs with dimensional information\n",
    "    user_inputs = UserInputs(\n",
    "        variable_dimensions={\n",
    "            'L': [0, 1, 0, 0],      # Length: m\n",
    "            'm': [1, 0, 0, 0],      # Mass: kg\n",
    "            'g': [0, 1, -2, 0],     # Acceleration: m/s^2\n",
    "        },\n",
    "        target_dimensions=[0, 0, 1, 0],  # Time: s\n",
    "        physical_bounds={\n",
    "            'target': {'min': 0, 'max': None},\n",
    "            'L': {'min': 0, 'max': None},\n",
    "            'm': {'min': 0, 'max': None},\n",
    "            'g': {'min': 0, 'max': None},\n",
    "        },\n",
    "        variable_mapping=None,\n",
    "        unit_conversions=None\n",
    "    )\n",
    "    \n",
    "    return X, T, feature_names, user_inputs\n",
    "\n",
    "print(\"Pendulum data generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Module Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODULE SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" 00_Core.ipynb - Module Summary\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"DATACLASSES:\")\n",
    "print(\"  - UserInputs: User-defined inputs (dimensions, bounds, mappings)\")\n",
    "print(\"  - Stage1Results: Variable selection & preprocessing results\")\n",
    "print(\"  - Stage2Results: Structure discovery results\")\n",
    "print(\"  - Stage3Results: Validation & UQ results\")\n",
    "print()\n",
    "print(\"UTILITY FUNCTIONS:\")\n",
    "print(\"  - safe_log, safe_divide, safe_power: Numerical safety\")\n",
    "print(\"  - compute_r2, compute_mse, compute_rmse, compute_mae: Metrics\")\n",
    "print(\"  - format_equation: Equation string formatting\")\n",
    "print(\"  - print_section_header, print_subsection_header: Output formatting\")\n",
    "print(\"  - normalize_features: StandardScaler wrapper\")\n",
    "print(\"  - check_data_validity: Data validation\")\n",
    "print()\n",
    "print(\"DATA GENERATORS:\")\n",
    "print(\"  - generate_warm_rain_data: dq_r/dt = 0.89 * q_c^2.47 * N_d^(-1.79)\")\n",
    "print(\"  - generate_polynomial_data: y = 0.5*x1^2 + 0.3*x2*x3 - 0.1*x1*x2^2 + 0.8\")\n",
    "print(\"  - generate_trigonometric_data: y = sin(x1) + 0.5*cos(x2) + 0.3*x1*x2\")\n",
    "print(\"  - generate_pendulum_data: T = 2*pi * sqrt(L/g)\")\n",
    "print()\n",
    "print(\"CONFIGURATION CONSTANTS:\")\n",
    "print(f\"  - RANDOM_SEED: {RANDOM_SEED}\")\n",
    "print(f\"  - DEFAULT_N_BOOTSTRAP: {DEFAULT_N_BOOTSTRAP}\")\n",
    "print(f\"  - DEFAULT_CONFIDENCE_LEVEL: {DEFAULT_CONFIDENCE_LEVEL}\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"Module loaded successfully. Import via: %run 00_Core.ipynb\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
