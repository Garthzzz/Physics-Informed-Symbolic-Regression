{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00_Core - Physics-SR Framework v4.1\n",
    "\n",
    "## Foundation Module: DataClasses, Utilities, TimeBudgetManager, and Test Data Generators\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Date:** January 2026  \n",
    "**Version:** 4.1 (Structure-Guided Feature Library Enhancement + Computational Optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook provides the foundational components for the Three-Stage Physics-Informed Symbolic Regression Framework:\n",
    "\n",
    "1. **DataClasses**: `UserInputs`, `Stage1Results`, `Stage2Results`, `Stage3Results`\n",
    "2. **TimeBudgetManager**: Adaptive time allocation for computational optimization (NEW v4.1)\n",
    "3. **Utility Functions**: Safe math operations, formatting, metrics, memory management\n",
    "4. **Test Data Generators**: Warm rain microphysics, polynomial, trigonometric, pendulum\n",
    "5. **Global Configuration Constants**: Including PYSR_MODES (v4.1)\n",
    "\n",
    "### Usage\n",
    "\n",
    "This module is imported by all other notebooks via:\n",
    "```python\n",
    "%run 00_Core.ipynb\n",
    "```\n",
    "\n",
    "### Changelog v4.1\n",
    "\n",
    "- Added TimeBudgetManager for adaptive time allocation\n",
    "- Added PYSR_MODES configuration dictionary\n",
    "- Added Float32 conversion and memory cleanup utilities\n",
    "- Updated Stage2Results with parsed_terms, detected_operators, augmented_library\n",
    "- Added timing field to all Stage dataclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "00_Core.ipynb - Foundation Module v4.1\n",
    "======================================\n",
    "\n",
    "Three-Stage Physics-Informed Symbolic Regression Framework v4.1\n",
    "\n",
    "This module provides:\n",
    "- DataClasses for user inputs and stage results\n",
    "- TimeBudgetManager for computational optimization (NEW v4.1)\n",
    "- Utility functions for safe numerical operations\n",
    "- Test data generators for algorithm validation\n",
    "- Global configuration constants including PYSR_MODES\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "Contact: zz3239@columbia.edu\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "from scipy.integrate import simpson\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Symbolic computation\n",
    "import sympy as sp\n",
    "from sympy import symbols, sympify, expand, Add, Mul, Pow\n",
    "\n",
    "# Optional imports (with graceful fallback)\n",
    "try:\n",
    "    from pysr import PySRRegressor\n",
    "    PYSR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYSR_AVAILABLE = False\n",
    "    warnings.warn(\"PySR not installed. PySR pathway will be disabled.\")\n",
    "\n",
    "# Parallel computing\n",
    "try:\n",
    "    from joblib import Parallel, delayed\n",
    "    JOBLIB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    JOBLIB_AVAILABLE = False\n",
    "\n",
    "print(\"00_Core v4.1: All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PySR available: {PYSR_AVAILABLE}\")\n",
    "print(f\"Joblib available: {JOBLIB_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GLOBAL CONFIGURATION CONSTANTS v4.1\n",
    "# ==============================================================================\n",
    "\n",
    "# Stage 1 defaults\n",
    "DEFAULT_MAX_EXPONENT = 4              # Buckingham pi search range\n",
    "DEFAULT_IMPORTANCE_THRESHOLD = 0.01   # Variable screening threshold\n",
    "DEFAULT_POWERLAW_R2_THRESHOLD = 0.9   # Power-law detection R^2 threshold\n",
    "DEFAULT_SOFTMAX_TEMPERATURE = 0.5     # iRF soft reweighting temperature\n",
    "DEFAULT_STABILITY_THRESHOLD = 0.5     # Interaction stability threshold\n",
    "\n",
    "# Stage 2 defaults\n",
    "DEFAULT_MAX_POLY_DEGREE = 3           # Feature library polynomial degree\n",
    "DEFAULT_STLSQ_THRESHOLD = 0.1         # STLSQ sparsity threshold\n",
    "DEFAULT_STLSQ_MAX_ITER = 20           # STLSQ maximum iterations\n",
    "DEFAULT_ALASSO_GAMMA = 1.0            # Adaptive Lasso gamma parameter\n",
    "DEFAULT_ALASSO_EPS = 1e-6             # Adaptive Lasso stabilization constant\n",
    "\n",
    "# Stage 3 defaults\n",
    "DEFAULT_CV_FOLDS = 5                  # Cross-validation folds\n",
    "DEFAULT_EBIC_GAMMA = 0.5              # EBIC gamma parameter\n",
    "DEFAULT_N_BOOTSTRAP = 200             # Number of bootstrap samples\n",
    "DEFAULT_CONFIDENCE_LEVEL = 0.95       # Confidence interval level\n",
    "DEFAULT_DIM_TOLERANCE = 0.05          # Dimensional check tolerance\n",
    "\n",
    "# v4.1 Computational Optimization Constants\n",
    "DEFAULT_RUNTIME_BUDGET = 180          # Total runtime budget (seconds)\n",
    "DEFAULT_PYSR_TIMEOUT = 100            # PySR timeout (seconds)\n",
    "DEFAULT_PROCS = 2                     # Number of parallel processes\n",
    "DEFAULT_PRECISION = 32                # Float precision (32 or 64)\n",
    "\n",
    "# PySR Mode Configurations (v4.1)\n",
    "PYSR_MODES = {\n",
    "    'fast': {\n",
    "        'niterations': 20,\n",
    "        'maxsize': 18,\n",
    "        'maxdepth': 8,\n",
    "        'populations': 8,\n",
    "        'population_size': 33,\n",
    "        'ncycles_per_iteration': 350,\n",
    "        'timeout_in_seconds': 60\n",
    "    },\n",
    "    'standard': {\n",
    "        'niterations': 40,\n",
    "        'maxsize': 20,\n",
    "        'maxdepth': 10,\n",
    "        'populations': 15,\n",
    "        'population_size': 33,\n",
    "        'ncycles_per_iteration': 400,\n",
    "        'timeout_in_seconds': 100\n",
    "    },\n",
    "    'thorough': {\n",
    "        'niterations': 80,\n",
    "        'maxsize': 25,\n",
    "        'maxdepth': 12,\n",
    "        'populations': 20,\n",
    "        'population_size': 50,\n",
    "        'ncycles_per_iteration': 550,\n",
    "        'timeout_in_seconds': 150\n",
    "    }\n",
    "}\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Numerical stability constants\n",
    "EPS_LOG = 1e-10                       # Epsilon for log safety\n",
    "EPS_DIV = 1e-10                       # Epsilon for division safety\n",
    "EPS_EXP_CLIP = 20                     # Clip for exp to prevent overflow\n",
    "\n",
    "# ==============================================================================\n",
    "# v4.7 DUAL-TRACK SELECTION CONSTANTS\n",
    "# ==============================================================================\n",
    "DEFAULT_PYSR_TRUST_THRESHOLD = 0.70   # Above this, trust PySR structure\n",
    "DEFAULT_PYSR_SKIP_THRESHOLD = 0.95    # Above this, HIGH TRUST mode\n",
    "DEFAULT_MAX_PYSR_TERMS = 12           # Cap on PySR terms even in HIGH TRUST\n",
    "DEFAULT_MAX_TOTAL_TERMS = 15          # Cap on total selected terms\n",
    "\n",
    "print(\"Global configuration constants v4.7 defined.\")\n",
    "print(f\"PYSR_MODES available: {list(PYSR_MODES.keys())}\")\n",
    "print(f\"v4.7 Dual-Track: TRUST_THRESHOLD={DEFAULT_PYSR_TRUST_THRESHOLD}, MAX_PYSR_TERMS={DEFAULT_MAX_PYSR_TERMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: DataClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# USER INPUTS DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class UserInputs:\n",
    "    \"\"\"\n",
    "    User-defined inputs required for the Physics-SR Framework.\n",
    "    \n",
    "    These inputs must be prepared before running the pipeline and require\n",
    "    domain knowledge about the physical system being modeled.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    variable_dimensions : Dict[str, List[float]]\n",
    "        Dictionary mapping variable names to their dimensional exponents [M, L, T, Theta].\n",
    "        M = Mass, L = Length, T = Time, Theta = Temperature.\n",
    "        Example: {'velocity': [0, 1, -1, 0]}  # m/s has L^1 * T^-1\n",
    "        \n",
    "    target_dimensions : List[float]\n",
    "        Dimensional exponents [M, L, T, Theta] for the target variable.\n",
    "        Example: [0, 0, -1, 0] for a rate with units s^-1\n",
    "        \n",
    "    physical_bounds : Dict[str, Dict[str, Optional[float]]]\n",
    "        Physical constraints for variables and target.\n",
    "        Format: {var_name: {'min': float or None, 'max': float or None}}\n",
    "        Example: {'target': {'min': 0, 'max': None}}  # Non-negative target\n",
    "        \n",
    "    variable_mapping : Optional[Dict[str, str]]\n",
    "        Maps data column names to standardized physical variable names.\n",
    "        Example: {'cloud_water_mixing_ratio': 'q_c'}\n",
    "        \n",
    "    unit_conversions : Optional[Dict[str, float]]\n",
    "        Conversion factors to convert data to SI units.\n",
    "        Example: {'radius_um': 1e-6}  # Convert micrometers to meters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Required fields\n",
    "    variable_dimensions: Dict[str, List[float]]\n",
    "    target_dimensions: List[float]\n",
    "    physical_bounds: Dict[str, Dict[str, Optional[float]]]\n",
    "    \n",
    "    # Optional fields with defaults\n",
    "    variable_mapping: Optional[Dict[str, str]] = None\n",
    "    unit_conversions: Optional[Dict[str, float]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate inputs after initialization.\"\"\"\n",
    "        # Validate dimensional exponents have length 4\n",
    "        for var_name, dims in self.variable_dimensions.items():\n",
    "            if len(dims) != 4:\n",
    "                raise ValueError(\n",
    "                    f\"Variable '{var_name}' has {len(dims)} dimensional exponents, \"\n",
    "                    f\"expected 4 [M, L, T, Theta]\"\n",
    "                )\n",
    "        \n",
    "        if len(self.target_dimensions) != 4:\n",
    "            raise ValueError(\n",
    "                f\"Target dimensions has {len(self.target_dimensions)} exponents, \"\n",
    "                f\"expected 4 [M, L, T, Theta]\"\n",
    "            )\n",
    "    \n",
    "    def get_variable_names(self) -> List[str]:\n",
    "        \"\"\"Return list of variable names.\"\"\"\n",
    "        return list(self.variable_dimensions.keys())\n",
    "    \n",
    "    def get_dimension_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Return dimensional matrix D where D[i,j] = exponent of dimension i for variable j.\"\"\"\n",
    "        var_names = self.get_variable_names()\n",
    "        n_vars = len(var_names)\n",
    "        D = np.zeros((4, n_vars))\n",
    "        for j, var_name in enumerate(var_names):\n",
    "            D[:, j] = self.variable_dimensions[var_name]\n",
    "        return D\n",
    "\n",
    "print(\"UserInputs dataclass defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STAGE 1 RESULTS DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Stage1Results:\n",
    "    \"\"\"\n",
    "    Results from Stage 1: Variable Selection & Preprocessing.\n",
    "    \n",
    "    Contains outputs from:\n",
    "    - 1.1 Buckingham Pi Dimensional Analysis\n",
    "    - 1.2 PAN+SR Variable Screening\n",
    "    - 1.3 Symmetry Analysis\n",
    "    - 1.4 iRF Interaction Discovery\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    # Buckingham Pi results\n",
    "    pi_groups : Optional[Dict[str, np.ndarray]]\n",
    "        Dictionary mapping pi-group names to their exponent vectors\n",
    "    pi_exponents : Optional[np.ndarray]\n",
    "        Matrix of exponents for selected pi-groups (n_groups x n_variables)\n",
    "    pi_group_names : Optional[List[str]]\n",
    "        Human-readable names for each pi-group\n",
    "    X_transformed : Optional[np.ndarray]\n",
    "        Data transformed to dimensionless pi-groups\n",
    "    all_pi_candidates : Optional[List[Dict]]\n",
    "        All candidate pi-groups with complexity scores\n",
    "        \n",
    "    # Variable screening results\n",
    "    selected_indices : Optional[List[int]]\n",
    "        Indices of variables selected by screening\n",
    "    selected_names : Optional[List[str]]\n",
    "        Names of selected variables\n",
    "    importance_scores : Optional[Dict[str, float]]\n",
    "        RF permutation importance for each variable\n",
    "        \n",
    "    # Symmetry analysis results\n",
    "    is_power_law : bool\n",
    "        Whether power-law relationship was detected\n",
    "    estimated_exponents : Optional[Dict[str, float]]\n",
    "        Estimated power-law exponents for each variable\n",
    "    power_law_r2 : Optional[float]\n",
    "        R-squared of log-log regression\n",
    "    structural_hints : Optional[Dict]\n",
    "        Hints about equation structure for Stage 2\n",
    "        \n",
    "    # Interaction discovery results\n",
    "    stable_interactions : Optional[List[Tuple[int, ...]]]\n",
    "        List of stable feature interactions (as tuples of indices)\n",
    "    interaction_stability : Optional[Dict[Tuple, float]]\n",
    "        Stability scores for each interaction\n",
    "    soft_weights : Optional[np.ndarray]\n",
    "        Soft reweighting weights from iRF\n",
    "        \n",
    "    # Timing (v4.1)\n",
    "    timing : Optional[Dict[str, float]]\n",
    "        Execution time for each sub-stage\n",
    "    \"\"\"\n",
    "    \n",
    "    # Buckingham Pi results\n",
    "    pi_groups: Optional[Dict[str, np.ndarray]] = None\n",
    "    pi_exponents: Optional[np.ndarray] = None\n",
    "    pi_group_names: Optional[List[str]] = None\n",
    "    X_transformed: Optional[np.ndarray] = None\n",
    "    all_pi_candidates: Optional[List[Dict]] = None\n",
    "    \n",
    "    # Variable screening results\n",
    "    selected_indices: Optional[List[int]] = None\n",
    "    selected_names: Optional[List[str]] = None\n",
    "    importance_scores: Optional[Dict[str, float]] = None\n",
    "    \n",
    "    # Symmetry analysis results\n",
    "    is_power_law: bool = False\n",
    "    estimated_exponents: Optional[Dict[str, float]] = None\n",
    "    power_law_r2: Optional[float] = None\n",
    "    structural_hints: Optional[Dict] = None\n",
    "    \n",
    "    # Interaction discovery results\n",
    "    stable_interactions: Optional[List[Tuple[int, ...]]] = None\n",
    "    interaction_stability: Optional[Dict[Tuple, float]] = None\n",
    "    soft_weights: Optional[np.ndarray] = None\n",
    "    \n",
    "    # Timing (v4.1)\n",
    "    timing: Optional[Dict[str, float]] = None\n",
    "\n",
    "print(\"Stage1Results dataclass defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STAGE 2 RESULTS DATACLASS (v4.1 Enhanced)\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Stage2Results:\n",
    "    \"\"\"\n",
    "    Results from Stage 2: Structure-Guided Discovery (v4.1).\n",
    "    \n",
    "    Contains outputs from:\n",
    "    - 2.1 PySR Structure Exploration\n",
    "    - 2.2 Structure Parsing (NEW v4.0)\n",
    "    - 2.3 Augmented Library Construction (NEW v4.0)\n",
    "    - 2.4 E-WSINDy Sparse Selection\n",
    "    - 2.5 Adaptive Lasso Verification (optional)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    # PySR results\n",
    "    pysr_equations : Optional[List[str]]\n",
    "        List of equations discovered by PySR\n",
    "    pysr_pareto : Optional[pd.DataFrame]\n",
    "        Pareto front of complexity vs accuracy\n",
    "    best_pysr_equation : Optional[str]\n",
    "        Best equation from PySR\n",
    "    best_pysr_sympy : Optional[Any]\n",
    "        Best PySR equation as SymPy expression\n",
    "    best_pysr_r2 : Optional[float]\n",
    "        R-squared of best PySR equation\n",
    "    pysr_elapsed_time : Optional[float]\n",
    "        PySR execution time (v4.1)\n",
    "        \n",
    "    # Structure Parsing results (NEW v4.0)\n",
    "    parsed_terms : Optional[List[Tuple]]\n",
    "        List of (expr, name, func) tuples from parsing\n",
    "    detected_operators : Optional[set]\n",
    "        Set of operators found in PySR equations {'sin', 'cos', 'exp', ...}\n",
    "    term_to_equation_map : Optional[Dict]\n",
    "        Mapping from terms to source equations\n",
    "        \n",
    "    # Augmented Library (NEW v4.0)\n",
    "    augmented_library : Optional[np.ndarray]\n",
    "        5-layer augmented feature matrix Phi_aug\n",
    "    library_names : Optional[List[str]]\n",
    "        Feature names with source tags [PowLaw], [PySR], [Var], [Poly], [Op]\n",
    "    library_info : Optional[Dict]\n",
    "        Library composition statistics\n",
    "    library_builder : Optional[AugmentedLibraryBuilder]\n",
    "        Builder object for transforming new data (v4.1, for test predictions)\n",
    "        \n",
    "    # E-WSINDy results\n",
    "    ewsindy_coefficients : Optional[np.ndarray]\n",
    "        Coefficient vector from E-WSINDy\n",
    "    ewsindy_support : Optional[np.ndarray]\n",
    "        Boolean mask of selected features\n",
    "    ewsindy_equation : Optional[str]\n",
    "        Equation string from E-WSINDy\n",
    "    ewsindy_r2 : Optional[float]\n",
    "        R-squared of E-WSINDy fit\n",
    "    selection_analysis : Optional[Dict]\n",
    "        Analysis of which sources contributed selected terms (v4.1)\n",
    "        \n",
    "    # Adaptive Lasso results (optional)\n",
    "    alasso_coefficients : Optional[np.ndarray]\n",
    "        Coefficient vector from Adaptive Lasso\n",
    "    alasso_support : Optional[np.ndarray]\n",
    "        Boolean mask of selected features\n",
    "    alasso_r2 : Optional[float]\n",
    "        R-squared of Adaptive Lasso fit\n",
    "        \n",
    "    # Timing (v4.1)\n",
    "    timing : Optional[Dict[str, float]]\n",
    "        Execution time for each sub-stage\n",
    "        \n",
    "    # Backward compatibility aliases (v4.1)\n",
    "    @property\n",
    "    def feature_library(self):\n",
    "        return self.augmented_library\n",
    "    \n",
    "    @property\n",
    "    def feature_names(self):\n",
    "        return self.library_names\n",
    "    \"\"\"\n",
    "    \n",
    "    # PySR results\n",
    "    pysr_equations: Optional[List[str]] = None\n",
    "    pysr_pareto: Optional[pd.DataFrame] = None\n",
    "    best_pysr_equation: Optional[str] = None\n",
    "    best_pysr_sympy: Optional[Any] = None\n",
    "    best_pysr_r2: Optional[float] = None\n",
    "    pysr_elapsed_time: Optional[float] = None\n",
    "    pysr_model: Optional[Any] = None  # v4.1.2: Store PySR model for test predictions\n",
    "    \n",
    "    # Structure Parsing results (NEW v4.0)\n",
    "    parsed_terms: Optional[List[Tuple]] = None\n",
    "    detected_operators: Optional[set] = None\n",
    "    term_to_equation_map: Optional[Dict] = None\n",
    "    \n",
    "    # Augmented Library (NEW v4.0)\n",
    "    augmented_library: Optional[np.ndarray] = None\n",
    "    library_names: Optional[List[str]] = None\n",
    "    library_info: Optional[Dict] = None\n",
    "    library_builder: Optional[Any] = None  # v4.1: For test set transformation\n",
    "    \n",
    "    # E-WSINDy results\n",
    "    ewsindy_coefficients: Optional[np.ndarray] = None\n",
    "    ewsindy_support: Optional[np.ndarray] = None\n",
    "    ewsindy_equation: Optional[str] = None\n",
    "    ewsindy_r2: Optional[float] = None\n",
    "    selection_analysis: Optional[Dict] = None\n",
    "    \n",
    "    # Adaptive Lasso results (optional)\n",
    "    alasso_coefficients: Optional[np.ndarray] = None\n",
    "    alasso_support: Optional[np.ndarray] = None\n",
    "    alasso_r2: Optional[float] = None\n",
    "    \n",
    "    # Timing (v4.1)\n",
    "    timing: Optional[Dict[str, float]] = None\n",
    "    \n",
    "    # v4.7 Dual-Track results\n",
    "    ewsindy_intercept: Optional[float] = None\n",
    "    final_method: Optional[str] = None  # 'pysr_refined' or 'ewsindy'\n",
    "    pysr_refined_equation: Optional[str] = None\n",
    "    pysr_refined_r2: Optional[float] = None\n",
    "    curve_fit_success: Optional[bool] = None\n",
    "    \n",
    "    # Backward compatibility properties\n",
    "    @property\n",
    "    def feature_library(self):\n",
    "        \"\"\"Alias for augmented_library (backward compatibility).\"\"\"\n",
    "        return self.augmented_library\n",
    "    \n",
    "    @property\n",
    "    def feature_names(self):\n",
    "        \"\"\"Alias for library_names (backward compatibility).\"\"\"\n",
    "        return self.library_names\n",
    "\n",
    "print(\"Stage2Results dataclass v4.7 defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STAGE 3 RESULTS DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Stage3Results:\n",
    "    \"\"\"\n",
    "    Results from Stage 3: Validation & Uncertainty Quantification.\n",
    "    \n",
    "    Contains outputs from:\n",
    "    - 3.1 Model Selection (CV + EBIC)\n",
    "    - 3.2 Physics Verification\n",
    "    - 3.3 Uncertainty Quantification (Three-Layer)\n",
    "    - 3.4 Formal Statistical Inference\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    # Model selection\n",
    "    cv_scores : Optional[Dict[str, Tuple[float, float]]]\n",
    "        CV scores for candidate models {model_id: (mean, std)}\n",
    "    ebic_scores : Optional[Dict[str, float]]\n",
    "        EBIC scores for candidate models\n",
    "    best_model : Optional[str]\n",
    "        Identifier of best model\n",
    "        \n",
    "    # Physics verification\n",
    "    dim_consistent : bool\n",
    "        Whether equation is dimensionally consistent\n",
    "    dim_details : Optional[Dict]\n",
    "        Details of dimensional analysis\n",
    "    bounds_violations : Optional[Dict]\n",
    "        Physical bounds violations detected\n",
    "    physics_score : Optional[float]\n",
    "        Overall physics verification score [0-1]\n",
    "        \n",
    "    # Structural UQ (Layer 1)\n",
    "    inclusion_probabilities : Optional[np.ndarray]\n",
    "        Bootstrap inclusion probabilities for each term\n",
    "    structural_confidence : Optional[Dict[str, str]]\n",
    "        Confidence classification (HIGH/MODERATE/LOW) per term\n",
    "    bootstrap_supports : Optional[np.ndarray]\n",
    "        Support patterns across bootstrap samples\n",
    "        \n",
    "    # Parametric UQ (Layer 2)\n",
    "    coefficient_estimates : Optional[np.ndarray]\n",
    "        Point estimates of coefficients\n",
    "    coefficient_CI : Optional[np.ndarray]\n",
    "        Confidence intervals (n_coef x 2)\n",
    "    coefficient_SE : Optional[np.ndarray]\n",
    "        Standard errors of coefficients\n",
    "    bootstrap_coefficients : Optional[np.ndarray]\n",
    "        Bootstrap coefficient samples (B x n_coef)\n",
    "        \n",
    "    # Predictive UQ (Layer 3)\n",
    "    prediction_intervals : Optional[Tuple[np.ndarray, np.ndarray]]\n",
    "        Lower and upper prediction interval bounds\n",
    "    pi_coverage : Optional[float]\n",
    "        Empirical coverage of prediction intervals\n",
    "    model_variance : Optional[np.ndarray]\n",
    "        Model uncertainty component\n",
    "    residual_variance : Optional[float]\n",
    "        Residual uncertainty component\n",
    "        \n",
    "    # Hypothesis testing\n",
    "    p_values : Optional[Dict[str, float]]\n",
    "        P-values from hypothesis tests\n",
    "    significant_terms : Optional[List[str]]\n",
    "        Terms that are statistically significant\n",
    "    test_statistics : Optional[Dict[str, float]]\n",
    "        Test statistics for each term\n",
    "        \n",
    "    # Final equation\n",
    "    final_equation : Optional[str]\n",
    "        Final equation string\n",
    "    final_coefficients : Optional[Dict[str, float]]\n",
    "        Final coefficient values by term name\n",
    "        \n",
    "    # Timing (v4.1)\n",
    "    timing : Optional[Dict[str, float]]\n",
    "        Execution time for each sub-stage\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model selection\n",
    "    cv_scores: Optional[Dict[str, Tuple[float, float]]] = None\n",
    "    ebic_scores: Optional[Dict[str, float]] = None\n",
    "    best_model: Optional[str] = None\n",
    "    \n",
    "    # Physics verification\n",
    "    dim_consistent: bool = False\n",
    "    dim_details: Optional[Dict] = None\n",
    "    bounds_violations: Optional[Dict] = None\n",
    "    physics_score: Optional[float] = None\n",
    "    \n",
    "    # Structural UQ (Layer 1)\n",
    "    inclusion_probabilities: Optional[np.ndarray] = None\n",
    "    structural_confidence: Optional[Dict[str, str]] = None\n",
    "    bootstrap_supports: Optional[np.ndarray] = None\n",
    "    \n",
    "    # Parametric UQ (Layer 2)\n",
    "    coefficient_estimates: Optional[np.ndarray] = None\n",
    "    coefficient_CI: Optional[np.ndarray] = None\n",
    "    coefficient_SE: Optional[np.ndarray] = None\n",
    "    bootstrap_coefficients: Optional[np.ndarray] = None\n",
    "    \n",
    "    # Predictive UQ (Layer 3)\n",
    "    prediction_intervals: Optional[Tuple[np.ndarray, np.ndarray]] = None\n",
    "    pi_coverage: Optional[float] = None\n",
    "    model_variance: Optional[np.ndarray] = None\n",
    "    residual_variance: Optional[float] = None\n",
    "    \n",
    "    # Hypothesis testing\n",
    "    p_values: Optional[Dict[str, float]] = None\n",
    "    significant_terms: Optional[List[str]] = None\n",
    "    test_statistics: Optional[Dict[str, float]] = None\n",
    "    \n",
    "    # Final equation\n",
    "    final_equation: Optional[str] = None\n",
    "    final_coefficients: Optional[Dict[str, float]] = None\n",
    "    \n",
    "    # Timing (v4.1)\n",
    "    timing: Optional[Dict[str, float]] = None\n",
    "\n",
    "print(\"Stage3Results dataclass defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: TimeBudgetManager (NEW v4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TIME BUDGET MANAGER (NEW v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "class TimeBudgetManager:\n",
    "    \"\"\"\n",
    "    Manage runtime budget across pipeline stages.\n",
    "    \n",
    "    Provides adaptive time allocation for computational optimization.\n",
    "    Designed for Google Colab Pro with 180-second total budget.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    total_budget : float\n",
    "        Total runtime budget in seconds\n",
    "    start_time : float\n",
    "        Pipeline start time\n",
    "    stage_times : Dict[str, float]\n",
    "        Cumulative time at each checkpoint\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> budget = TimeBudgetManager(total_budget_seconds=180)\n",
    "    >>> pysr_timeout = budget.allocate_pysr_time()\n",
    "    >>> # ... run PySR ...\n",
    "    >>> budget.record_stage('PySR')\n",
    "    >>> n_bootstrap = budget.allocate_bootstrap_count()\n",
    "    >>> print(budget.report())\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, total_budget_seconds: float = DEFAULT_RUNTIME_BUDGET):\n",
    "        \"\"\"\n",
    "        Initialize TimeBudgetManager.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        total_budget_seconds : float\n",
    "            Total runtime budget (default: 180 for Colab Pro)\n",
    "        \"\"\"\n",
    "        self.total_budget = total_budget_seconds\n",
    "        self.start_time = time.time()\n",
    "        self.stage_times = {}\n",
    "    \n",
    "    def elapsed(self) -> float:\n",
    "        \"\"\"Return elapsed time since start.\"\"\"\n",
    "        return time.time() - self.start_time\n",
    "    \n",
    "    def remaining(self) -> float:\n",
    "        \"\"\"Return remaining time in budget.\"\"\"\n",
    "        return max(0, self.total_budget - self.elapsed())\n",
    "    \n",
    "    def allocate_pysr_time(self, reserve_for_stage3: float = 40) -> int:\n",
    "        \"\"\"\n",
    "        Calculate PySR timeout based on remaining budget.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        reserve_for_stage3 : float\n",
    "            Time to reserve for Stage 3 (default: 40s)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            PySR timeout in seconds\n",
    "        \"\"\"\n",
    "        available = self.remaining() - reserve_for_stage3\n",
    "        # Give 70% to PySR, cap at 120s\n",
    "        pysr_time = max(30, min(available * 0.7, 120))\n",
    "        return int(pysr_time)\n",
    "    \n",
    "    def allocate_bootstrap_count(self, time_per_bootstrap: float = 0.2) -> int:\n",
    "        \"\"\"\n",
    "        Calculate bootstrap count based on remaining budget.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        time_per_bootstrap : float\n",
    "            Estimated time per bootstrap sample\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of bootstrap samples\n",
    "        \"\"\"\n",
    "        available = self.remaining() - 10  # Reserve 10s for output\n",
    "        max_bootstraps = int(available / time_per_bootstrap)\n",
    "        return max(50, min(max_bootstraps, 200))\n",
    "    \n",
    "    def should_skip_optional(self, min_required: float = 30) -> bool:\n",
    "        \"\"\"\n",
    "        Decide whether to skip optional components.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        min_required : float\n",
    "            Minimum time required for optional component\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if should skip\n",
    "        \"\"\"\n",
    "        return self.remaining() < min_required\n",
    "    \n",
    "    def record_stage(self, stage_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Record time for a stage.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stage_name : str\n",
    "            Name of the stage\n",
    "        \"\"\"\n",
    "        self.stage_times[stage_name] = self.elapsed()\n",
    "    \n",
    "    def get_stage_duration(self, stage_name: str) -> float:\n",
    "        \"\"\"Get duration of a specific stage.\"\"\"\n",
    "        times = list(self.stage_times.values())\n",
    "        keys = list(self.stage_times.keys())\n",
    "        if stage_name not in keys:\n",
    "            return 0.0\n",
    "        idx = keys.index(stage_name)\n",
    "        if idx == 0:\n",
    "            return times[0]\n",
    "        return times[idx] - times[idx-1]\n",
    "    \n",
    "    def report(self) -> str:\n",
    "        \"\"\"Generate timing report.\"\"\"\n",
    "        lines = [\"=== Timing Report ===\"]\n",
    "        prev = 0\n",
    "        for stage, cumulative in self.stage_times.items():\n",
    "            duration = cumulative - prev\n",
    "            lines.append(f\"  {stage}: {duration:.1f}s\")\n",
    "            prev = cumulative\n",
    "        lines.append(f\"  ---\")\n",
    "        lines.append(f\"  Total: {self.elapsed():.1f}s / {self.total_budget}s\")\n",
    "        lines.append(f\"  Remaining: {self.remaining():.1f}s\")\n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, float]:\n",
    "        \"\"\"Return timing as dictionary.\"\"\"\n",
    "        result = {}\n",
    "        prev = 0\n",
    "        for stage, cumulative in self.stage_times.items():\n",
    "            result[stage] = cumulative - prev\n",
    "            prev = cumulative\n",
    "        result['total'] = self.elapsed()\n",
    "        result['remaining'] = self.remaining()\n",
    "        return result\n",
    "\n",
    "print(\"TimeBudgetManager class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAFE NUMERICAL OPERATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def safe_log(x: np.ndarray, eps: float = EPS_LOG) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe logarithm that handles zero and negative values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input array\n",
    "    eps : float\n",
    "        Small constant to add before taking log\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        log(|x| + eps)\n",
    "    \"\"\"\n",
    "    return np.log(np.abs(x) + eps)\n",
    "\n",
    "\n",
    "def safe_exp(x: np.ndarray, clip: float = EPS_EXP_CLIP) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe exponential that prevents overflow.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input array\n",
    "    clip : float\n",
    "        Maximum absolute value for clipping\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        exp(clip(x, -clip, clip))\n",
    "    \"\"\"\n",
    "    return np.exp(np.clip(x, -clip, clip))\n",
    "\n",
    "\n",
    "def safe_divide(num: np.ndarray, denom: np.ndarray, eps: float = EPS_DIV) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe division that handles zero denominators.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num : np.ndarray\n",
    "        Numerator\n",
    "    denom : np.ndarray\n",
    "        Denominator\n",
    "    eps : float\n",
    "        Small constant to add to denominator\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        num / (denom + eps * sign(denom))\n",
    "    \"\"\"\n",
    "    sign = np.sign(denom)\n",
    "    sign[sign == 0] = 1\n",
    "    return num / (denom + eps * sign)\n",
    "\n",
    "# Alias for backward compatibility\n",
    "safe_div = safe_divide\n",
    "\n",
    "\n",
    "def safe_sqrt(x: np.ndarray, eps: float = EPS_LOG) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe square root that handles negative values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input array\n",
    "    eps : float\n",
    "        Small constant to add\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        sqrt(|x| + eps)\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.abs(x) + eps)\n",
    "\n",
    "\n",
    "def safe_power(base: np.ndarray, exp: float, eps: float = EPS_LOG) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe power function that handles negative bases with non-integer exponents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base : np.ndarray\n",
    "        Base values\n",
    "    exp : float\n",
    "        Exponent\n",
    "    eps : float\n",
    "        Small constant to add\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        (|base| + eps) ** exp * sign(base)\n",
    "    \"\"\"\n",
    "    abs_base = np.abs(base) + eps\n",
    "    sign = np.sign(base)\n",
    "    sign[sign == 0] = 1\n",
    "    return (abs_base ** exp) * sign\n",
    "\n",
    "print(\"Safe numerical operations defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METRICS AND EVALUATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_r2(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute R-squared coefficient of determination.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        R-squared value\n",
    "    \"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        MSE value\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def compute_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Root Mean Squared Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSE value\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "def compute_mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        MAE value\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "print(\"Metrics and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FORMATTING AND OUTPUT FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def format_equation(coefficients: np.ndarray, \n",
    "                   feature_names: List[str],\n",
    "                   threshold: float = 1e-10,\n",
    "                   precision: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Format sparse coefficients as equation string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefficients : np.ndarray\n",
    "        Coefficient vector\n",
    "    feature_names : List[str]\n",
    "        Names of features\n",
    "    threshold : float\n",
    "        Minimum coefficient magnitude to include\n",
    "    precision : int\n",
    "        Decimal precision for coefficients\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Formatted equation string\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for coef, name in zip(coefficients, feature_names):\n",
    "        if np.abs(coef) > threshold:\n",
    "            if name == '1' or name == '[Poly] 1':\n",
    "                terms.append(f\"{coef:.{precision}f}\")\n",
    "            else:\n",
    "                # Remove source tags for cleaner display\n",
    "                clean_name = name\n",
    "                for tag in ['[PySR] ', '[Var] ', '[Poly] ', '[Op] ']:\n",
    "                    clean_name = clean_name.replace(tag, '')\n",
    "                terms.append(f\"{coef:.{precision}f} * {clean_name}\")\n",
    "    \n",
    "    if not terms:\n",
    "        return \"0\"\n",
    "    \n",
    "    equation = terms[0]\n",
    "    for term in terms[1:]:\n",
    "        if term.startswith('-'):\n",
    "            equation += f\" {term}\"\n",
    "        else:\n",
    "            equation += f\" + {term}\"\n",
    "    \n",
    "    return equation\n",
    "\n",
    "\n",
    "def print_section_header(title: str, width: int = 70) -> None:\n",
    "    \"\"\"\n",
    "    Print formatted section header.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        Section title\n",
    "    width : int\n",
    "        Total width of header line\n",
    "    \"\"\"\n",
    "    print(\"=\" * width)\n",
    "    print(f\" {title}\")\n",
    "    print(\"=\" * width)\n",
    "\n",
    "\n",
    "def print_subsection_header(title: str, width: int = 70) -> None:\n",
    "    \"\"\"\n",
    "    Print formatted subsection header.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        Subsection title\n",
    "    width : int\n",
    "        Total width of header line\n",
    "    \"\"\"\n",
    "    print(\"-\" * width)\n",
    "    print(f\" {title}\")\n",
    "    print(\"-\" * width)\n",
    "\n",
    "print(\"Formatting and output functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA PREPROCESSING FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def normalize_features(X: np.ndarray) -> Tuple[np.ndarray, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Normalize features using StandardScaler.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_normalized : np.ndarray\n",
    "        Normalized feature matrix\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler for inverse transform\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    return X_normalized, scaler\n",
    "\n",
    "\n",
    "def check_data_validity(X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Check data for common issues.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary with validity checks:\n",
    "        - is_valid: bool\n",
    "        - n_samples: int\n",
    "        - n_features: int\n",
    "        - has_nan: bool\n",
    "        - has_inf: bool\n",
    "        - constant_features: List[int]\n",
    "        - warnings: List[str]\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'is_valid': True,\n",
    "        'n_samples': X.shape[0],\n",
    "        'n_features': X.shape[1],\n",
    "        'has_nan': False,\n",
    "        'has_inf': False,\n",
    "        'constant_features': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        result['has_nan'] = True\n",
    "        result['is_valid'] = False\n",
    "        result['warnings'].append(\"Data contains NaN values\")\n",
    "    \n",
    "    # Check for Inf values\n",
    "    if np.any(np.isinf(X)) or np.any(np.isinf(y)):\n",
    "        result['has_inf'] = True\n",
    "        result['is_valid'] = False\n",
    "        result['warnings'].append(\"Data contains Inf values\")\n",
    "    \n",
    "    # Check for constant features\n",
    "    for j in range(X.shape[1]):\n",
    "        if np.std(X[:, j]) < 1e-10:\n",
    "            result['constant_features'].append(j)\n",
    "            result['warnings'].append(f\"Feature {j} is constant\")\n",
    "    \n",
    "    # Check sample size\n",
    "    if X.shape[0] < 50:\n",
    "        result['warnings'].append(f\"Small sample size ({X.shape[0]}), results may be unreliable\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Data preprocessing functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# v4.1 MEMORY AND OPTIMIZATION UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def cleanup_memory() -> None:\n",
    "    \"\"\"Force garbage collection to free memory.\"\"\"\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def convert_to_float32(X: np.ndarray, y: np.ndarray = None) -> Tuple:\n",
    "    \"\"\"\n",
    "    Convert arrays to Float32 for memory efficiency.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix\n",
    "    y : np.ndarray, optional\n",
    "        Target vector\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "        Converted arrays\n",
    "    \"\"\"\n",
    "    X_32 = np.asarray(X, dtype=np.float32)\n",
    "    if y is not None:\n",
    "        y_32 = np.asarray(y, dtype=np.float32)\n",
    "        return X_32, y_32\n",
    "    return X_32\n",
    "\n",
    "\n",
    "def is_valid_feature(values: np.ndarray, \n",
    "                    existing_columns: List[np.ndarray] = None,\n",
    "                    corr_threshold: float = 0.9999) -> bool:\n",
    "    \"\"\"\n",
    "    Check if feature is valid (finite, non-constant, non-duplicate).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    values : np.ndarray\n",
    "        Feature values\n",
    "    existing_columns : List[np.ndarray], optional\n",
    "        Existing features for duplicate checking\n",
    "    corr_threshold : float\n",
    "        Correlation threshold for duplicate detection\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if feature is valid\n",
    "    \"\"\"\n",
    "    # Check finite\n",
    "    if not np.all(np.isfinite(values)):\n",
    "        return False\n",
    "    \n",
    "    # Check non-constant\n",
    "    if np.std(values) < 1e-10:\n",
    "        return False\n",
    "    \n",
    "    # Check non-duplicate\n",
    "    if existing_columns is not None:\n",
    "        for existing in existing_columns[-20:]:  # Only check recent columns\n",
    "            if len(existing) == len(values):\n",
    "                corr = np.corrcoef(values, existing)[0, 1]\n",
    "                if np.abs(corr) > corr_threshold:\n",
    "                    return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"v4.1 memory and optimization utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Test Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# WARM RAIN DATA GENERATOR\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_warm_rain_data(\n",
    "    n_samples: int = 500,\n",
    "    noise_level: float = 0.01,\n",
    "    seed: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str], UserInputs]:\n",
    "    \"\"\"\n",
    "    Generate synthetic warm rain microphysics data.\n",
    "    \n",
    "    True equation: dq_r/dt = 0.89 * q_c^2.47 * N_d^(-1.79)\n",
    "    (KK2000 autoconversion rate)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    noise_level : float\n",
    "        Relative noise standard deviation\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, 4)\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "    feature_names : List[str]\n",
    "        List of feature names\n",
    "    user_inputs : UserInputs\n",
    "        Complete UserInputs with dimensional information\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate realistic ranges\n",
    "    q_c = np.random.uniform(1e-4, 5e-3, n_samples)      # kg/kg\n",
    "    N_d = np.random.uniform(1e7, 5e8, n_samples)        # m^-3\n",
    "    r_eff = np.random.uniform(5e-6, 25e-6, n_samples)   # m\n",
    "    LWC = np.random.uniform(0.1, 2.0, n_samples)        # kg/m^3\n",
    "    \n",
    "    # True autoconversion rate\n",
    "    y_true = 0.89 * (q_c ** 2.47) * (N_d ** (-1.79))\n",
    "    \n",
    "    # Add multiplicative noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.exp(np.random.normal(0, noise_level, n_samples))\n",
    "        y = y_true * noise\n",
    "    else:\n",
    "        y = y_true.copy()\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    X = np.column_stack([q_c, N_d, r_eff, LWC])\n",
    "    feature_names = ['q_c', 'N_d', 'r_eff', 'LWC']\n",
    "    \n",
    "    # Create UserInputs with dimensional information\n",
    "    user_inputs = UserInputs(\n",
    "        variable_dimensions={\n",
    "            'q_c':   [0, 0, 0, 0],     # kg/kg (dimensionless)\n",
    "            'N_d':   [0, -3, 0, 0],    # m^-3\n",
    "            'r_eff': [0, 1, 0, 0],     # m\n",
    "            'LWC':   [1, -3, 0, 0],    # kg/m^3\n",
    "        },\n",
    "        target_dimensions=[0, 0, -1, 0],  # s^-1 (rate)\n",
    "        physical_bounds={\n",
    "            'target': {'min': 0, 'max': None},\n",
    "            'q_c': {'min': 0, 'max': 0.01},\n",
    "            'N_d': {'min': 0, 'max': None},\n",
    "            'r_eff': {'min': 0, 'max': None},\n",
    "            'LWC': {'min': 0, 'max': None},\n",
    "        },\n",
    "        variable_mapping=None,\n",
    "        unit_conversions=None\n",
    "    )\n",
    "    \n",
    "    return X, y, feature_names, user_inputs\n",
    "\n",
    "print(\"Warm rain data generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# POLYNOMIAL DATA GENERATOR\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_polynomial_data(\n",
    "    n_samples: int = 500,\n",
    "    noise_level: float = 0.01,\n",
    "    seed: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Generate synthetic polynomial data for testing.\n",
    "    \n",
    "    True equation: y = 0.5*x1^2 + 0.3*x2*x3 - 0.1*x1*x2^2 + 0.8\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    noise_level : float\n",
    "        Relative noise standard deviation\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, 5)\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "    feature_names : List[str]\n",
    "        List of feature names\n",
    "    true_coefficients : Dict[str, float]\n",
    "        True coefficient values\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate base features\n",
    "    x1 = np.random.uniform(-1, 1, n_samples)\n",
    "    x2 = np.random.uniform(-1, 1, n_samples)\n",
    "    x3 = np.random.uniform(-1, 1, n_samples)\n",
    "    \n",
    "    # Noise features (should not be selected)\n",
    "    noise1 = np.random.uniform(-1, 1, n_samples)\n",
    "    noise2 = np.random.uniform(-1, 1, n_samples)\n",
    "    \n",
    "    # True equation: y = 0.5*x1^2 + 0.3*x2*x3 - 0.1*x1*x2^2 + 0.8\n",
    "    y_true = 0.5 * x1**2 + 0.3 * x2 * x3 - 0.1 * x1 * x2**2 + 0.8\n",
    "    \n",
    "    # Add noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.random.normal(0, noise_level * np.std(y_true), n_samples)\n",
    "        y = y_true + noise\n",
    "    else:\n",
    "        y = y_true.copy()\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    X = np.column_stack([x1, x2, x3, noise1, noise2])\n",
    "    feature_names = ['x1', 'x2', 'x3', 'noise1', 'noise2']\n",
    "    \n",
    "    true_coefficients = {\n",
    "        '1': 0.8,\n",
    "        'x1^2': 0.5,\n",
    "        'x2*x3': 0.3,\n",
    "        'x1*x2^2': -0.1\n",
    "    }\n",
    "    \n",
    "    return X, y, feature_names, true_coefficients\n",
    "\n",
    "print(\"Polynomial data generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRIGONOMETRIC DATA GENERATOR\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_trigonometric_data(\n",
    "    n_samples: int = 500,\n",
    "    noise_level: float = 0.01,\n",
    "    seed: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Generate synthetic trigonometric data for testing.\n",
    "    \n",
    "    True equation: y = sin(x1) + 0.5*cos(x2) + 0.3*x1*x2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    noise_level : float\n",
    "        Relative noise standard deviation\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, 4)\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "    feature_names : List[str]\n",
    "        List of feature names\n",
    "    true_coefficients : Dict[str, float]\n",
    "        True coefficient values\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate base features (in range suitable for trig functions)\n",
    "    x1 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    x2 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    \n",
    "    # Noise features (should not be selected)\n",
    "    noise1 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    noise2 = np.random.uniform(-np.pi, np.pi, n_samples)\n",
    "    \n",
    "    # True equation: y = sin(x1) + 0.5*cos(x2) + 0.3*x1*x2\n",
    "    y_true = np.sin(x1) + 0.5 * np.cos(x2) + 0.3 * x1 * x2\n",
    "    \n",
    "    # Add noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.random.normal(0, noise_level * np.std(y_true), n_samples)\n",
    "        y = y_true + noise\n",
    "    else:\n",
    "        y = y_true.copy()\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    X = np.column_stack([x1, x2, noise1, noise2])\n",
    "    feature_names = ['x1', 'x2', 'noise1', 'noise2']\n",
    "    \n",
    "    true_coefficients = {\n",
    "        'sin(x1)': 1.0,\n",
    "        'cos(x2)': 0.5,\n",
    "        'x1*x2': 0.3\n",
    "    }\n",
    "    \n",
    "    return X, y, feature_names, true_coefficients\n",
    "\n",
    "print(\"Trigonometric data generator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PENDULUM DATA GENERATOR (FOR BUCKINGHAM PI TESTING)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_pendulum_data(\n",
    "    n_samples: int = 500,\n",
    "    noise_level: float = 0.01,\n",
    "    seed: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str], UserInputs]:\n",
    "    \"\"\"\n",
    "    Generate synthetic simple pendulum period data.\n",
    "    \n",
    "    True equation: T = 2*pi * sqrt(L/g)\n",
    "    \n",
    "    Variables:\n",
    "    - L: pendulum length (m)\n",
    "    - m: mass (kg) - does not appear in true equation\n",
    "    - g: gravitational acceleration (m/s^2)\n",
    "    - T: period (s)\n",
    "    \n",
    "    This is a classic dimensional analysis example where:\n",
    "    T * sqrt(g/L) is the dimensionless group.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data points to generate\n",
    "    noise_level : float\n",
    "        Relative noise standard deviation\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, 3) with L, m, g\n",
    "    y : np.ndarray\n",
    "        Target vector - period T\n",
    "    feature_names : List[str]\n",
    "        List of feature names\n",
    "    user_inputs : UserInputs\n",
    "        Complete UserInputs with dimensional information\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate physically realistic ranges\n",
    "    L = np.random.uniform(0.1, 2.0, n_samples)      # Length (m)\n",
    "    m = np.random.uniform(0.01, 1.0, n_samples)     # Mass (kg) - irrelevant\n",
    "    g = np.random.uniform(9.7, 10.0, n_samples)     # Gravity (m/s^2)\n",
    "    \n",
    "    # True period: T = 2*pi * sqrt(L/g)\n",
    "    T_true = 2 * np.pi * np.sqrt(L / g)\n",
    "    \n",
    "    # Add multiplicative noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.exp(np.random.normal(0, noise_level, n_samples))\n",
    "        T = T_true * noise\n",
    "    else:\n",
    "        T = T_true.copy()\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    X = np.column_stack([L, m, g])\n",
    "    feature_names = ['L', 'm', 'g']\n",
    "    \n",
    "    # Create UserInputs with dimensional information\n",
    "    user_inputs = UserInputs(\n",
    "        variable_dimensions={\n",
    "            'L': [0, 1, 0, 0],      # Length: m\n",
    "            'm': [1, 0, 0, 0],      # Mass: kg\n",
    "            'g': [0, 1, -2, 0],     # Acceleration: m/s^2\n",
    "        },\n",
    "        target_dimensions=[0, 0, 1, 0],  # Time: s\n",
    "        physical_bounds={\n",
    "            'target': {'min': 0, 'max': None},\n",
    "            'L': {'min': 0, 'max': None},\n",
    "            'm': {'min': 0, 'max': None},\n",
    "            'g': {'min': 0, 'max': None},\n",
    "        },\n",
    "        variable_mapping=None,\n",
    "        unit_conversions=None\n",
    "    )\n",
    "    \n",
    "    return X, T, feature_names, user_inputs\n",
    "\n",
    "print(\"Pendulum data generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Module Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODULE SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" 00_Core.ipynb v4.1 - Module Summary\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"DATACLASSES:\")\n",
    "print(\"  - UserInputs: User-defined inputs (dimensions, bounds, mappings)\")\n",
    "print(\"  - Stage1Results: Variable selection & preprocessing results\")\n",
    "print(\"  - Stage2Results: Structure-guided discovery results (v4.1 enhanced)\")\n",
    "print(\"  - Stage3Results: Validation & UQ results\")\n",
    "print()\n",
    "print(\"TimeBudgetManager (NEW v4.1):\")\n",
    "print(\"  - Adaptive time allocation for computational optimization\")\n",
    "print(\"  - Methods: allocate_pysr_time(), allocate_bootstrap_count()\")\n",
    "print(\"  - Methods: should_skip_optional(), record_stage(), report()\")\n",
    "print()\n",
    "print(\"UTILITY FUNCTIONS:\")\n",
    "print(\"  - safe_log, safe_exp, safe_divide, safe_sqrt, safe_power: Numerical safety\")\n",
    "print(\"  - compute_r2, compute_mse, compute_rmse, compute_mae: Metrics\")\n",
    "print(\"  - format_equation: Equation string formatting\")\n",
    "print(\"  - print_section_header, print_subsection_header: Output formatting\")\n",
    "print(\"  - normalize_features, check_data_validity: Data preprocessing\")\n",
    "print(\"  - cleanup_memory, convert_to_float32, is_valid_feature: v4.1 optimization\")\n",
    "print()\n",
    "print(\"DATA GENERATORS:\")\n",
    "print(\"  - generate_warm_rain_data: dq_r/dt = 0.89 * q_c^2.47 * N_d^(-1.79)\")\n",
    "print(\"  - generate_polynomial_data: y = 0.5*x1^2 + 0.3*x2*x3 - 0.1*x1*x2^2 + 0.8\")\n",
    "print(\"  - generate_trigonometric_data: y = sin(x1) + 0.5*cos(x2) + 0.3*x1*x2\")\n",
    "print(\"  - generate_pendulum_data: T = 2*pi * sqrt(L/g)\")\n",
    "print()\n",
    "print(\"CONFIGURATION CONSTANTS:\")\n",
    "print(f\"  - RANDOM_SEED: {RANDOM_SEED}\")\n",
    "print(f\"  - DEFAULT_N_BOOTSTRAP: {DEFAULT_N_BOOTSTRAP}\")\n",
    "print(f\"  - DEFAULT_RUNTIME_BUDGET: {DEFAULT_RUNTIME_BUDGET}s\")\n",
    "print(f\"  - PYSR_MODES: {list(PYSR_MODES.keys())}\")\n",
    "print(f\"  - PYSR_AVAILABLE: {PYSR_AVAILABLE}\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"Module loaded successfully. Import via: %run 00_Core.ipynb\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}