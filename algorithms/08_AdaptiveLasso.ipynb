{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08_AdaptiveLasso - Physics-SR Framework v3.0\n",
    "\n",
    "## Stage 2.2c: Adaptive Lasso with Oracle Property\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Achieve the oracle property for variable selection in symbolic regression. The oracle property guarantees:\n",
    "1. **Selection consistency:** $P(\\text{support} = \\text{true support}) \\to 1$ as $n \\to \\infty$\n",
    "2. **Asymptotic normality:** $\\sqrt{n}(\\hat{\\xi} - \\xi^*) \\xrightarrow{d} N(0, V)$\n",
    "\n",
    "### Key Innovation\n",
    "\n",
    "Standard LASSO has a limitation: it applies equal penalty to all coefficients, leading to:\n",
    "- Shrinkage bias on large coefficients\n",
    "- Inconsistent variable selection\n",
    "\n",
    "Adaptive LASSO solves this by using **data-driven weights**:\n",
    "$$w_j = \\frac{1}{(|\\hat{\\beta}_j^{init}| + \\varepsilon)^\\gamma}$$\n",
    "\n",
    "where $\\gamma > 0$ and $\\varepsilon$ is a stabilization constant.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The adaptive LASSO objective:\n",
    "$$\\hat{\\xi} = \\arg\\min_\\xi \\|y - \\Phi\\xi\\|^2 + \\lambda \\sum_{j=1}^{p} w_j |\\xi_j|$$\n",
    "\n",
    "### Reference\n",
    "\n",
    "- Zou, H. (2006). The adaptive lasso and its oracle properties. *JASA*, 101(476), 1418-1429."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "08_AdaptiveLasso.ipynb - Adaptive Lasso with Oracle Property\n",
    "=============================================================\n",
    "\n",
    "Three-Stage Physics-Informed Symbolic Regression Framework v3.0\n",
    "\n",
    "This module provides:\n",
    "- AdaptiveLassoSelector: Adaptive Lasso with data-driven weights\n",
    "- Oracle property for variable selection consistency\n",
    "- Epsilon-stabilization to prevent weight explosion\n",
    "- Cross-validation for lambda selection\n",
    "\n",
    "Algorithm:\n",
    "    1. Compute initial estimate via Ridge regression\n",
    "    2. Compute adaptive weights: w_j = 1 / (|beta_init[j]| + eps)^gamma\n",
    "    3. Fit weighted Lasso with CV for lambda selection\n",
    "    4. Transform coefficients back to original scale\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "\"\"\"\n",
    "\n",
    "# Import core module\n",
    "%run 00_Core.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for Adaptive Lasso\n",
    "from sklearn.linear_model import Ridge, LassoCV, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "print(\"08_AdaptiveLasso: Additional imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ADAPTIVE LASSO SELECTOR CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class AdaptiveLassoSelector:\n",
    "    \"\"\"\n",
    "    Adaptive Lasso with Oracle Property for Symbolic Regression.\n",
    "    \n",
    "    Implements adaptive LASSO which achieves the oracle property:\n",
    "    - Selection consistency (recovers true support)\n",
    "    - Asymptotic normality (valid inference)\n",
    "    \n",
    "    The key innovation is using data-driven weights that penalize\n",
    "    small coefficients more heavily, allowing large coefficients\n",
    "    to remain unbiased.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    gamma : float\n",
    "        Weight exponent (default: 1.0). Higher gamma = stronger penalty\n",
    "        on small initial coefficients.\n",
    "    eps : float\n",
    "        Stabilization constant to prevent weight explosion (default: 1e-6)\n",
    "    cv_folds : int\n",
    "        Number of folds for cross-validation (default: 5)\n",
    "    initial_method : str\n",
    "        Method for initial estimate: 'ridge' or 'ols' (default: 'ridge')\n",
    "    ridge_alpha : float\n",
    "        Regularization for initial Ridge estimate (default: 0.1)\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> selector = AdaptiveLassoSelector(gamma=1.0)\n",
    "    >>> result = selector.fit(Phi, y, feature_names=names)\n",
    "    >>> print(result['equation'])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        gamma: float = 1.0,\n",
    "        eps: float = 1e-6,\n",
    "        cv_folds: int = DEFAULT_CV_FOLDS,\n",
    "        initial_method: str = 'ridge',\n",
    "        ridge_alpha: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize AdaptiveLassoSelector.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "            Exponent for adaptive weights. gamma=1 is standard,\n",
    "            gamma=2 provides stronger adaptation.\n",
    "            Default: 1.0\n",
    "        eps : float\n",
    "            Stabilization constant to prevent division by zero.\n",
    "            Default: 1e-6\n",
    "        cv_folds : int\n",
    "            Number of cross-validation folds for lambda selection.\n",
    "            Default: 5\n",
    "        initial_method : str\n",
    "            'ridge' or 'ols' for initial estimate.\n",
    "            Default: 'ridge' (more stable)\n",
    "        ridge_alpha : float\n",
    "            Ridge regularization parameter for initial estimate.\n",
    "            Default: 0.1\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.cv_folds = cv_folds\n",
    "        self.initial_method = initial_method\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        \n",
    "        # Internal state\n",
    "        self._coefficients = None\n",
    "        self._support = None\n",
    "        self._feature_names = None\n",
    "        self._n_features = None\n",
    "        self._initial_estimate = None\n",
    "        self._adaptive_weights = None\n",
    "        self._optimal_lambda = None\n",
    "        self._fit_complete = False\n",
    "        self._r2_score = None\n",
    "        self._mse = None\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        feature_library: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fit Adaptive Lasso model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_library : np.ndarray\n",
    "            Feature matrix of shape (n_samples, n_features)\n",
    "        y : np.ndarray\n",
    "            Target vector of shape (n_samples,)\n",
    "        feature_names : List[str], optional\n",
    "            Names of features\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Dictionary containing:\n",
    "            - coefficients: Coefficient vector\n",
    "            - support: Boolean mask of active terms\n",
    "            - equation: String representation\n",
    "            - n_active_terms: Number of non-zero coefficients\n",
    "            - optimal_lambda: Selected lambda via CV\n",
    "            - r2_score: R-squared on training data\n",
    "            - mse: Mean squared error\n",
    "        \"\"\"\n",
    "        n_samples, n_features = feature_library.shape\n",
    "        self._n_features = n_features\n",
    "        \n",
    "        # Set feature names\n",
    "        if feature_names is None:\n",
    "            self._feature_names = [f'f{i}' for i in range(n_features)]\n",
    "        else:\n",
    "            self._feature_names = list(feature_names)\n",
    "        \n",
    "        # Step 1: Compute initial estimate\n",
    "        self._initial_estimate = self._compute_initial_estimate(\n",
    "            feature_library, y\n",
    "        )\n",
    "        \n",
    "        # Step 2: Compute adaptive weights\n",
    "        self._adaptive_weights = self._compute_adaptive_weights(\n",
    "            self._initial_estimate\n",
    "        )\n",
    "        \n",
    "        # Step 3: Fit weighted Lasso\n",
    "        self._coefficients, self._optimal_lambda = self._fit_weighted_lasso(\n",
    "            feature_library, y, self._adaptive_weights\n",
    "        )\n",
    "        \n",
    "        # Determine support\n",
    "        self._support = np.abs(self._coefficients) > 1e-10\n",
    "        \n",
    "        # Compute metrics\n",
    "        y_pred = feature_library @ self._coefficients\n",
    "        self._mse = np.mean((y - y_pred)**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        ss_res = np.sum((y - y_pred)**2)\n",
    "        self._r2_score = 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "        \n",
    "        self._fit_complete = True\n",
    "        \n",
    "        return {\n",
    "            'coefficients': self._coefficients,\n",
    "            'support': self._support,\n",
    "            'equation': self.get_equation(),\n",
    "            'n_active_terms': int(np.sum(self._support)),\n",
    "            'optimal_lambda': self._optimal_lambda,\n",
    "            'r2_score': self._r2_score,\n",
    "            'mse': self._mse,\n",
    "            'initial_estimate': self._initial_estimate,\n",
    "            'adaptive_weights': self._adaptive_weights,\n",
    "            'gamma': self.gamma,\n",
    "            'eps': self.eps\n",
    "        }\n",
    "    \n",
    "    def _compute_initial_estimate(\n",
    "        self,\n",
    "        Phi: np.ndarray,\n",
    "        y: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute initial coefficient estimate.\n",
    "        \n",
    "        Uses Ridge regression for stability (always has a solution\n",
    "        even when p > n or features are collinear).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Phi : np.ndarray\n",
    "            Feature matrix\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Initial coefficient estimate\n",
    "        \"\"\"\n",
    "        if self.initial_method == 'ridge':\n",
    "            ridge = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "            ridge.fit(Phi, y)\n",
    "            return ridge.coef_\n",
    "        else:  # OLS\n",
    "            try:\n",
    "                beta, _, _, _ = np.linalg.lstsq(Phi, y, rcond=None)\n",
    "                return beta\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Fallback to Ridge if OLS fails\n",
    "                ridge = Ridge(alpha=0.1, fit_intercept=False)\n",
    "                ridge.fit(Phi, y)\n",
    "                return ridge.coef_\n",
    "    \n",
    "    def _compute_adaptive_weights(\n",
    "        self,\n",
    "        beta_init: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute adaptive weights with epsilon stabilization.\n",
    "        \n",
    "        w_j = 1 / (|beta_init[j]| + eps)^gamma\n",
    "        \n",
    "        The epsilon prevents weight explosion when beta_init â‰ˆ 0.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        beta_init : np.ndarray\n",
    "            Initial coefficient estimate\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Adaptive weights\n",
    "        \"\"\"\n",
    "        return 1.0 / (np.abs(beta_init) + self.eps) ** self.gamma\n",
    "    \n",
    "    def _fit_weighted_lasso(\n",
    "        self,\n",
    "        Phi: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        weights: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Fit weighted Lasso via variable transformation.\n",
    "        \n",
    "        Transform: Phi_weighted = Phi / sqrt(weights)\n",
    "        This makes the penalty: lambda * sum(w_j * |beta_j|)\n",
    "        equivalent to: lambda * sum(|beta_weighted_j|)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Phi : np.ndarray\n",
    "            Feature matrix\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        weights : np.ndarray\n",
    "            Adaptive weights\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float]\n",
    "            (coefficients, optimal_lambda)\n",
    "        \"\"\"\n",
    "        # Transform design matrix\n",
    "        # Note: We divide by sqrt(weights) so that after Lasso\n",
    "        # we can recover original coefficients\n",
    "        sqrt_weights = np.sqrt(weights)\n",
    "        Phi_weighted = Phi / sqrt_weights\n",
    "        \n",
    "        # Fit Lasso with CV\n",
    "        lasso_cv = LassoCV(\n",
    "            cv=self.cv_folds,\n",
    "            fit_intercept=False,\n",
    "            max_iter=10000,\n",
    "            tol=1e-6\n",
    "        )\n",
    "        lasso_cv.fit(Phi_weighted, y)\n",
    "        \n",
    "        # Transform coefficients back\n",
    "        beta_weighted = lasso_cv.coef_\n",
    "        beta = self._transform_back(beta_weighted, weights)\n",
    "        \n",
    "        return beta, lasso_cv.alpha_\n",
    "    \n",
    "    def _transform_back(\n",
    "        self,\n",
    "        beta_weighted: np.ndarray,\n",
    "        weights: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform weighted coefficients back to original scale.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        beta_weighted : np.ndarray\n",
    "            Coefficients from weighted Lasso\n",
    "        weights : np.ndarray\n",
    "            Adaptive weights\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Original-scale coefficients\n",
    "        \"\"\"\n",
    "        sqrt_weights = np.sqrt(weights)\n",
    "        return beta_weighted / sqrt_weights\n",
    "    \n",
    "    def get_equation(\n",
    "        self,\n",
    "        feature_names: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Get string representation of discovered equation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_names : List[str], optional\n",
    "            Feature names to use\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Equation string\n",
    "        \"\"\"\n",
    "        if self._coefficients is None:\n",
    "            return \"\"\n",
    "        \n",
    "        names = feature_names or self._feature_names\n",
    "        \n",
    "        terms = []\n",
    "        for i, (coef, name) in enumerate(zip(self._coefficients, names)):\n",
    "            if abs(coef) > 1e-10:\n",
    "                if coef >= 0 and len(terms) > 0:\n",
    "                    terms.append(f\"+ {coef:.6f}*{name}\")\n",
    "                else:\n",
    "                    terms.append(f\"{coef:.6f}*{name}\")\n",
    "        \n",
    "        if len(terms) == 0:\n",
    "            return \"0\"\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        Phi_new: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict using fitted model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Phi_new : np.ndarray\n",
    "            Feature matrix for new data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        if self._coefficients is None:\n",
    "            raise ValueError(\"Must call fit() before predict()\")\n",
    "        return Phi_new @ self._coefficients\n",
    "    \n",
    "    def get_active_terms(\n",
    "        self\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get list of active terms with coefficients.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[str, float]]\n",
    "            List of (name, coefficient) tuples\n",
    "        \"\"\"\n",
    "        if not self._fit_complete:\n",
    "            raise ValueError(\"Must call fit() first\")\n",
    "        \n",
    "        active = []\n",
    "        for i, (coef, name) in enumerate(zip(self._coefficients, self._feature_names)):\n",
    "            if self._support[i]:\n",
    "                active.append((name, float(coef)))\n",
    "        \n",
    "        return active\n",
    "    \n",
    "    def print_alasso_report(self) -> None:\n",
    "        \"\"\"\n",
    "        Print detailed Adaptive Lasso report.\n",
    "        \"\"\"\n",
    "        if not self._fit_complete:\n",
    "            print(\"Fit not yet performed. Call fit() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\" Adaptive Lasso Results\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"  Gamma: {self.gamma}\")\n",
    "        print(f\"  Epsilon: {self.eps}\")\n",
    "        print(f\"  CV folds: {self.cv_folds}\")\n",
    "        print(f\"  Initial method: {self.initial_method}\")\n",
    "        print()\n",
    "        print(f\"Results:\")\n",
    "        print(f\"  Optimal lambda: {self._optimal_lambda:.6e}\")\n",
    "        print(f\"  Active terms: {int(np.sum(self._support))} / {self._n_features}\")\n",
    "        print(f\"  R-squared: {self._r2_score:.6f}\")\n",
    "        print(f\"  MSE: {self._mse:.6e}\")\n",
    "        print()\n",
    "        print(\"-\" * 70)\n",
    "        print(\" Discovered Equation:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"  {self.get_equation()}\")\n",
    "        print()\n",
    "        print(\"-\" * 70)\n",
    "        print(\" Coefficient Details:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"  {'Term':<25} {'Init Est':<12} {'Weight':<12} {'Final':<12}\")\n",
    "        print(\"  \" + \"-\" * 60)\n",
    "        \n",
    "        for i in range(self._n_features):\n",
    "            if self._support[i]:\n",
    "                print(f\"  {self._feature_names[i]:<25} \"\n",
    "                      f\"{self._initial_estimate[i]:<12.4f} \"\n",
    "                      f\"{self._adaptive_weights[i]:<12.4f} \"\n",
    "                      f\"{self._coefficients[i]:<12.6f}\")\n",
    "        \n",
    "        print()\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Internal Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST CONTROL FLAG\n",
    "# ==============================================================================\n",
    "\n",
    "_RUN_TESTS = False  # Set to True to run internal tests\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" RUNNING INTERNAL TESTS FOR 08_AdaptiveLasso\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 1: Oracle Property - Selection Consistency\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 1: Oracle Property - Selection Consistency\")\n",
    "    \n",
    "    # Generate data with known sparse solution\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    n_features = 20\n",
    "    n_active = 3\n",
    "    \n",
    "    # True coefficients: only first 3 are non-zero\n",
    "    true_coef = np.zeros(n_features)\n",
    "    true_coef[0] = 2.0\n",
    "    true_coef[1] = -1.5\n",
    "    true_coef[2] = 0.8\n",
    "    \n",
    "    # Generate features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = X @ true_coef + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    feature_names = [f'x{i}' for i in range(n_features)]\n",
    "    \n",
    "    print(f\"True active features: x0, x1, x2\")\n",
    "    print(f\"True coefficients: {true_coef[:3]}\")\n",
    "    print()\n",
    "    \n",
    "    # Fit Adaptive Lasso\n",
    "    selector = AdaptiveLassoSelector(gamma=1.0)\n",
    "    result = selector.fit(X, y, feature_names=feature_names)\n",
    "    \n",
    "    print(f\"Discovered active: {result['n_active_terms']} features\")\n",
    "    print(f\"Active terms: {[t[0] for t in selector.get_active_terms()]}\")\n",
    "    \n",
    "    # Check if true support is recovered\n",
    "    recovered_support = set(t[0] for t in selector.get_active_terms())\n",
    "    true_support = {'x0', 'x1', 'x2'}\n",
    "    \n",
    "    if recovered_support == true_support:\n",
    "        print(\"[PASS] Oracle property: True support recovered exactly\")\n",
    "    elif true_support.issubset(recovered_support):\n",
    "        print(\"[PARTIAL] True support included, but extra features selected\")\n",
    "    else:\n",
    "        print(f\"[INFO] Support: {recovered_support}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 2: Epsilon Stabilization\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 2: Epsilon Stabilization\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    # Feature with very small initial coefficient\n",
    "    x1 = np.random.randn(n_samples)\n",
    "    x2 = np.random.randn(n_samples)\n",
    "    x3 = np.random.randn(n_samples)  # Irrelevant feature\n",
    "    \n",
    "    y = 2*x1 + 0.5*x2 + 0.01*np.random.randn(n_samples)\n",
    "    \n",
    "    X = np.column_stack([x1, x2, x3])\n",
    "    feature_names = ['x1', 'x2', 'x3']\n",
    "    \n",
    "    # Test different epsilon values\n",
    "    eps_values = [1e-10, 1e-6, 1e-3]\n",
    "    \n",
    "    print(f\"Testing different epsilon values:\")\n",
    "    print(f\"{'Epsilon':<12} {'Active':<10} {'R2':<10} {'Max Weight':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        selector = AdaptiveLassoSelector(gamma=1.0, eps=eps)\n",
    "        result = selector.fit(X, y, feature_names=feature_names)\n",
    "        max_weight = np.max(selector._adaptive_weights)\n",
    "        print(f\"{eps:<12.0e} {result['n_active_terms']:<10} \"\n",
    "              f\"{result['r2_score']:<10.4f} {max_weight:<15.2e}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Note: Smaller eps = larger max weight (potential instability)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 3: Gamma Sensitivity\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 3: Gamma Sensitivity\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    x1 = np.random.randn(n_samples)\n",
    "    x2 = np.random.randn(n_samples)\n",
    "    x3 = np.random.randn(n_samples)\n",
    "    \n",
    "    # Sparse signal\n",
    "    y = 3*x1 + 0.01*np.random.randn(n_samples)\n",
    "    \n",
    "    X = np.column_stack([x1, x2, x3])\n",
    "    feature_names = ['x1', 'x2', 'x3']\n",
    "    \n",
    "    gamma_values = [0.5, 1.0, 2.0]\n",
    "    \n",
    "    print(f\"True: y = 3*x1 (only x1 is active)\")\n",
    "    print()\n",
    "    print(f\"{'Gamma':<10} {'Active':<10} {'R2':<10} {'x1 coef':<12}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for gamma in gamma_values:\n",
    "        selector = AdaptiveLassoSelector(gamma=gamma)\n",
    "        result = selector.fit(X, y, feature_names=feature_names)\n",
    "        x1_coef = result['coefficients'][0]\n",
    "        print(f\"{gamma:<10.1f} {result['n_active_terms']:<10} \"\n",
    "              f\"{result['r2_score']:<10.4f} {x1_coef:<12.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Note: Higher gamma = stronger penalty on small initial coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 4: Comparison with Standard Lasso\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 4: Adaptive Lasso vs Standard Lasso\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    x1 = np.random.randn(n_samples)\n",
    "    x2 = np.random.randn(n_samples)\n",
    "    x3 = np.random.randn(n_samples)\n",
    "    \n",
    "    # True: y = 5*x1 + 2*x2 (large difference in coefficients)\n",
    "    y = 5*x1 + 2*x2 + 0.1*np.random.randn(n_samples)\n",
    "    \n",
    "    X = np.column_stack([x1, x2, x3])\n",
    "    feature_names = ['x1', 'x2', 'x3']\n",
    "    \n",
    "    print(f\"True: y = 5*x1 + 2*x2\")\n",
    "    print()\n",
    "    \n",
    "    # Standard Lasso\n",
    "    lasso = LassoCV(cv=5, fit_intercept=False)\n",
    "    lasso.fit(X, y)\n",
    "    \n",
    "    # Adaptive Lasso\n",
    "    alasso = AdaptiveLassoSelector(gamma=1.0)\n",
    "    alasso_result = alasso.fit(X, y, feature_names=feature_names)\n",
    "    \n",
    "    print(f\"{'Method':<20} {'x1':<12} {'x2':<12} {'x3':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'True':<20} {5.0:<12.4f} {2.0:<12.4f} {0.0:<12.4f}\")\n",
    "    print(f\"{'Standard Lasso':<20} {lasso.coef_[0]:<12.4f} \"\n",
    "          f\"{lasso.coef_[1]:<12.4f} {lasso.coef_[2]:<12.4f}\")\n",
    "    print(f\"{'Adaptive Lasso':<20} {alasso._coefficients[0]:<12.4f} \"\n",
    "          f\"{alasso._coefficients[1]:<12.4f} {alasso._coefficients[2]:<12.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Note: Adaptive Lasso should have less bias on large coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Module Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODULE SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" 08_AdaptiveLasso.ipynb - Module Summary\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"CLASS: AdaptiveLassoSelector\")\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(\"Purpose:\")\n",
    "print(\"  Adaptive Lasso with oracle property for variable selection.\")\n",
    "print(\"  Achieves selection consistency and asymptotic normality.\")\n",
    "print()\n",
    "print(\"Main Methods:\")\n",
    "print(\"  fit(feature_library, y, feature_names=None)\")\n",
    "print(\"      Fit Adaptive Lasso model\")\n",
    "print(\"      Returns: dict with coefficients, support, equation, metrics\")\n",
    "print()\n",
    "print(\"  get_equation()\")\n",
    "print(\"      Get string representation of equation\")\n",
    "print()\n",
    "print(\"  predict(Phi_new)\")\n",
    "print(\"      Make predictions\")\n",
    "print()\n",
    "print(\"  get_active_terms()\")\n",
    "print(\"      Get list of active terms with coefficients\")\n",
    "print()\n",
    "print(\"  print_alasso_report()\")\n",
    "print(\"      Print detailed results report\")\n",
    "print()\n",
    "print(\"Key Parameters:\")\n",
    "print(\"  gamma: Weight exponent (1.0 = standard, 2.0 = stronger)\")\n",
    "print(\"  eps: Stabilization constant (default: 1e-6)\")\n",
    "print()\n",
    "print(\"Usage Example:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "# Build feature library\n",
    "builder = FeatureLibraryBuilder(max_poly_degree=3)\n",
    "Phi, names = builder.build(X, feature_names)\n",
    "\n",
    "# Fit Adaptive Lasso\n",
    "selector = AdaptiveLassoSelector(gamma=1.0)\n",
    "result = selector.fit(Phi, y, feature_names=names)\n",
    "\n",
    "print(f\"Equation: {result['equation']}\")\n",
    "print(f\"Active terms: {result['n_active_terms']}\")\n",
    "print(f\"R-squared: {result['r2_score']:.4f}\")\n",
    "\"\"\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"Module loaded successfully. Import via: %run 08_AdaptiveLasso.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
