{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07_EWSINDy_STLSQ - Physics-SR Framework v4.1\n",
    "\n",
    "## Stage 2.4: E-WSINDy Sparse Selection on Augmented Library\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Contact:** zz3239@columbia.edu  \n",
    "**Date:** January 2026  \n",
    "**Version:** 4.1 (Three-Layer Sparse Selection Enhancement)\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Noise-robust equation discovery via weak-form sparse regression on augmented library.\n",
    "This module provides a **three-layer enhancement** for robust sparse selection.\n",
    "\n",
    "### v4.1 Three-Layer Enhancement\n",
    "\n",
    "| Layer | Component | Purpose |\n",
    "|-------|-----------|----------|\n",
    "| **Layer 1** | Relative Thresholding | Scale-invariant STLSQ thresholding |\n",
    "| **Layer 2** | E-SINDy Ensemble | Bootstrap-based robust selection with inclusion probabilities |\n",
    "| **Layer 3** | EBIC/CV Selection | Automatic threshold selection via information criteria |\n",
    "\n",
    "### v4.1 Key Classes and Functions\n",
    "\n",
    "| Component | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `EWSINDySTLSQ` | Class | STLSQ with relative threshold (Layer 1) |\n",
    "| `ESINDyEnsemble` | Class | Bootstrap ensemble selection (Layer 2) |\n",
    "| `select_threshold_by_ebic()` | Function | EBIC-based threshold selection (Layer 3) |\n",
    "| `select_threshold_by_cv()` | Function | CV-based threshold selection (Layer 3) |\n",
    "\n",
    "### Weak Form Theory\n",
    "\n",
    "**Strong form** (noise-sensitive):\n",
    "$$\\frac{\\partial q}{\\partial t} = f(q, \\nabla q, \\nabla^2 q)$$\n",
    "\n",
    "**Weak form** (noise-robust): Multiply by test function $\\psi$ and integrate by parts:\n",
    "$$\\int \\psi \\cdot \\nabla^2 q \\, dx = -\\int \\nabla\\psi \\cdot \\nabla q \\, dx + \\text{boundary terms}$$\n",
    "\n",
    "**Result:** Derivatives transferred from noisy data $q$ to smooth test function $\\psi$.\n",
    "\n",
    "### Key Properties of v4.1 Design\n",
    "\n",
    "1. **Relative Threshold:** `threshold = base_threshold * max(|coef|)` makes STLSQ robust to coefficient scale\n",
    "2. **Ensemble Selection:** Bootstrap aggregation provides inclusion probabilities for natural UQ\n",
    "3. **EBIC Selection:** Automatic threshold tuning balances fit vs. complexity\n",
    "4. **Can KEEP correct PySR terms:** If PySR found sin(x^2) and it's correct, E-WSINDy will select it\n",
    "5. **Can DISCOVER missed terms:** If PySR missed x*z, E-WSINDy can find it in polynomial layer\n",
    "6. **Can REJECT errors:** If PySR included spurious term, E-WSINDy's sparsity can exclude it\n",
    "\n",
    "### References\n",
    "\n",
    "- Messenger, D. A., & Bortz, D. M. (2021). Weak SINDy for PDEs. *JCP*, 443, 110525.\n",
    "- Fasel et al. (2022). Ensemble-SINDy. *Proc. R. Soc. A.*\n",
    "- Chen, J., & Chen, Z. (2008). Extended BIC for large model spaces. *Biometrika*, 95(3).\n",
    "- Framework v4.0/v4.1 Section 4.4: E-WSINDy Sparse Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "07_EWSINDy_STLSQ.ipynb - E-WSINDy with STLSQ on Augmented Library\n",
    "===================================================================\n",
    "\n",
    "Three-Stage Physics-Informed Symbolic Regression Framework v4.1\n",
    "\n",
    "This module provides:\n",
    "- EWSINDySTLSQ: Weak-form SINDy with STLSQ sparse regression\n",
    "- Source attribution via analyze_selection_sources()\n",
    "- 50-1000x noise robustness improvement over finite differences\n",
    "- Exact sparsity (true zeros) via iterative thresholding\n",
    "\n",
    "v4.1 Key Changes from v3.0:\n",
    "- Now accepts library_names with source tags [PySR], [Var], [Poly], [Op]\n",
    "- New method: analyze_selection_sources()\n",
    "- Returns selection_analysis in output dictionary\n",
    "- New parameter: normalize_columns (default: True)\n",
    "\n",
    "Output Format:\n",
    "- coefficients, support, equation, r_squared (same as v3.0)\n",
    "- selection_analysis: Dict with from_pysr, from_variant, from_poly, from_op counts\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "Contact: zz3239@columbia.edu\n",
    "\"\"\"\n",
    "\n",
    "# Import core module\n",
    "%run 00_Core.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for E-WSINDy\n",
    "from scipy import integrate\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "print(\"07_EWSINDy_STLSQ v4.1: Additional imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# E-WSINDY STLSQ CLASS (v4.2 HYBRID)\n",
    "# ==============================================================================\n",
    "# \n",
    "# v4.2 HYBRID: Adaptive term limiting using percentile threshold\n",
    "#              with MIN_TERMS and MAX_TERMS bounds\n",
    "#\n",
    "# Strategy:\n",
    "#   1. Compute 80th percentile of |coefficients|\n",
    "#   2. Keep terms above this percentile\n",
    "#   3. Ensure result is within [MIN_TERMS, MAX_TERMS] bounds\n",
    "#\n",
    "# Author: Zhengze Zhang\n",
    "# Contact: zz3239@columbia.edu\n",
    "# ==============================================================================\n",
    "\n",
    "class EWSINDySTLSQ:\n",
    "    \"\"\"\n",
    "    E-WSINDy with STLSQ: Weak-form Sparse Regression (v4.2 Hybrid).\n",
    "    \n",
    "    Now operates on augmented library from v4.0 and includes\n",
    "    source attribution for selected terms.\n",
    "    \n",
    "    v4.2 HYBRID: Adaptive term limiting using percentile threshold\n",
    "    to prevent overfitting while maintaining flexibility.\n",
    "    \n",
    "    Provides 50-1000x noise improvement over strong-form methods.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    threshold : float\n",
    "        STLSQ sparsity threshold (default: 0.1)\n",
    "    max_iter : int\n",
    "        Maximum STLSQ iterations (default: 20)\n",
    "    n_test_functions : int\n",
    "        Number of test functions for weak form (default: 50)\n",
    "    test_function_type : str\n",
    "        Type of test function: 'gaussian' or 'polynomial'\n",
    "    test_function_width : float\n",
    "        Width parameter for test functions (default: 0.1)\n",
    "    use_weak_form : bool\n",
    "        Whether to use weak form transformation (default: True)\n",
    "    normalize_columns : bool\n",
    "        Whether to normalize feature columns (default: True, v4.1)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    fit(feature_library, y, library_names, t) -> Dict\n",
    "        Fit E-WSINDy model using STLSQ\n",
    "    analyze_selection_sources(support, library_names) -> Dict\n",
    "        Analyze where selected terms originated (v4.1)\n",
    "    get_equation() -> str\n",
    "        Get string representation of equation\n",
    "    predict(Phi_new) -> np.ndarray\n",
    "        Make predictions\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Messenger & Bortz (2021). Multiscale Modeling & Simulation.\n",
    "    Framework v4.0/v4.1 Section 4.4: E-WSINDy Sparse Selection\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> model = EWSINDySTLSQ(threshold=0.1)\n",
    "    >>> result = model.fit(Phi_aug, y, library_names=names)\n",
    "    >>> print(f\"Selection analysis: {result['selection_analysis']}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # v4.2 HYBRID: Adaptive term limiting parameters\n",
    "    MIN_TERMS = 5\n",
    "    MAX_TERMS = 20\n",
    "    COEF_PERCENTILE = 80\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        threshold: float = DEFAULT_STLSQ_THRESHOLD,\n",
    "        max_iter: int = DEFAULT_STLSQ_MAX_ITER,\n",
    "        n_test_functions: int = 50,\n",
    "        test_function_type: str = 'gaussian',\n",
    "        test_function_width: float = 0.1,\n",
    "        use_weak_form: bool = True,\n",
    "        normalize_columns: bool = True,\n",
    "        relative_threshold: bool = True,\n",
    "        ridge_alpha: float = 1e-6\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize EWSINDySTLSQ.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        threshold : float\n",
    "            Base sparsity threshold. If relative_threshold=True, this is\n",
    "            multiplied by max(|coefficients|). Default: 0.1\n",
    "        max_iter : int\n",
    "            Maximum number of STLSQ iterations.\n",
    "            Default: 20\n",
    "        n_test_functions : int\n",
    "            Number of test functions for weak form.\n",
    "            Default: 50\n",
    "        test_function_type : str\n",
    "            'gaussian' for Gaussian bumps, 'polynomial' for polynomial.\n",
    "            Default: 'gaussian'\n",
    "        test_function_width : float\n",
    "            Width of Gaussian bumps (as fraction of domain).\n",
    "            Default: 0.1\n",
    "        use_weak_form : bool\n",
    "            Whether to use weak form (True) or standard form (False).\n",
    "            Default: True\n",
    "        normalize_columns : bool\n",
    "            Whether to normalize feature library columns (v4.1).\n",
    "            Default: True\n",
    "        relative_threshold : bool\n",
    "            If True, threshold = threshold * max(|coef|). This makes\n",
    "            STLSQ robust to coefficient scale variations (v4.1 FIX).\n",
    "            Default: True\n",
    "        ridge_alpha : float\n",
    "            Ridge regularization for initial OLS (stability).\n",
    "            Default: 1e-6\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.max_iter = max_iter\n",
    "        self.n_test_functions = n_test_functions\n",
    "        self.test_function_type = test_function_type\n",
    "        self.test_function_width = test_function_width\n",
    "        self.use_weak_form = use_weak_form\n",
    "        self.normalize_columns = normalize_columns\n",
    "        self.relative_threshold = relative_threshold\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        \n",
    "        # Internal state\n",
    "        self._coefficients = None\n",
    "        self._support = None\n",
    "        self._library_names = None\n",
    "        self._n_features = None\n",
    "        self._n_iterations = 0\n",
    "        self._convergence_history = []\n",
    "        self._fit_complete = False\n",
    "        self._r2_score = None\n",
    "        self._mse = None\n",
    "        self._selection_analysis = None\n",
    "        self._column_scales = None\n",
    "        self._effective_threshold = None\n",
    "        self._terms_limited = False  # v4.2: Track if limiting was applied\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        feature_library: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        t: np.ndarray = None,\n",
    "        library_names: List[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fit E-WSINDy model using STLSQ.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_library : np.ndarray\n",
    "            Feature library (augmented or standard)\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        t : np.ndarray, optional\n",
    "            Time vector for weak form (if None, uses indices)\n",
    "        library_names : List[str], optional\n",
    "            Feature names with source tags for attribution\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            - coefficients: Sparse coefficient vector\n",
    "            - support: Boolean mask of active terms\n",
    "            - equation: Formatted equation string\n",
    "            - selection_analysis: Source attribution (v4.1)\n",
    "            - r_squared: Coefficient of determination\n",
    "            - n_iterations: Convergence iterations\n",
    "            - weak_form_Q: Weak-form feature matrix (if used)\n",
    "            - weak_form_b: Weak-form target vector (if used)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = feature_library.shape\n",
    "        self._n_features = n_features\n",
    "        \n",
    "        # Set feature names\n",
    "        if library_names is None:\n",
    "            self._library_names = [f'f{i}' for i in range(n_features)]\n",
    "        else:\n",
    "            self._library_names = list(library_names)\n",
    "        \n",
    "        # Generate time vector if not provided\n",
    "        if t is None:\n",
    "            t = self._generate_time_vector(n_samples)\n",
    "        \n",
    "        # Normalize columns if requested (v4.1)\n",
    "        Phi_normalized = feature_library.copy()\n",
    "        if self.normalize_columns:\n",
    "            Phi_normalized, self._column_scales = self._normalize_library(feature_library)\n",
    "        else:\n",
    "            self._column_scales = np.ones(n_features)\n",
    "        \n",
    "        # Apply weak form transformation if enabled\n",
    "        if self.use_weak_form:\n",
    "            Q, b = self._weak_form_transform(Phi_normalized, y, t)\n",
    "        else:\n",
    "            # Standard form: direct regression\n",
    "            Q = Phi_normalized\n",
    "            b = y\n",
    "        \n",
    "        # Run STLSQ\n",
    "        self._coefficients = self._stlsq_iteration(Q, b)\n",
    "        \n",
    "        # Rescale coefficients if normalized\n",
    "        if self.normalize_columns:\n",
    "            self._coefficients = self._coefficients / self._column_scales\n",
    "        \n",
    "        self._support = np.abs(self._coefficients) > 0\n",
    "        \n",
    "        # ======================================================================\n",
    "        # v4.2 HYBRID: Adaptive term limiting\n",
    "        # ======================================================================\n",
    "        n_selected_original = np.sum(self._support)\n",
    "        self._terms_limited = False\n",
    "        \n",
    "        if n_selected_original > self.MAX_TERMS:\n",
    "            self._terms_limited = True\n",
    "            coef_abs = np.abs(self._coefficients)\n",
    "            nonzero_coefs = coef_abs[self._support]\n",
    "            \n",
    "            # Compute percentile threshold\n",
    "            percentile_threshold = np.percentile(nonzero_coefs, self.COEF_PERCENTILE)\n",
    "            \n",
    "            # Apply percentile threshold\n",
    "            candidate_support = (coef_abs >= percentile_threshold) & (coef_abs > 0)\n",
    "            n_candidates = np.sum(candidate_support)\n",
    "            \n",
    "            # Ensure within [MIN_TERMS, MAX_TERMS] bounds\n",
    "            if n_candidates < self.MIN_TERMS:\n",
    "                # Keep top MIN_TERMS by coefficient magnitude\n",
    "                nonzero_indices = np.where(self._support)[0]\n",
    "                top_k_local = np.argsort(coef_abs[nonzero_indices])[-self.MIN_TERMS:]\n",
    "                top_k_global = nonzero_indices[top_k_local]\n",
    "                new_support = np.zeros(n_features, dtype=bool)\n",
    "                new_support[top_k_global] = True\n",
    "            elif n_candidates > self.MAX_TERMS:\n",
    "                # Keep top MAX_TERMS by coefficient magnitude\n",
    "                nonzero_indices = np.where(self._support)[0]\n",
    "                top_k_local = np.argsort(coef_abs[nonzero_indices])[-self.MAX_TERMS:]\n",
    "                top_k_global = nonzero_indices[top_k_local]\n",
    "                new_support = np.zeros(n_features, dtype=bool)\n",
    "                new_support[top_k_global] = True\n",
    "            else:\n",
    "                new_support = candidate_support\n",
    "            \n",
    "            # Update support\n",
    "            self._support = new_support\n",
    "            \n",
    "            # Re-estimate coefficients with limited support\n",
    "            if np.sum(self._support) > 0:\n",
    "                Phi_selected = feature_library[:, self._support]\n",
    "                new_coefficients = np.zeros(n_features)\n",
    "                try:\n",
    "                    new_coefficients[self._support] = np.linalg.lstsq(\n",
    "                        Phi_selected, y, rcond=None\n",
    "                    )[0]\n",
    "                    self._coefficients = new_coefficients\n",
    "                except:\n",
    "                    pass  # Keep original if lstsq fails\n",
    "        # ======================================================================\n",
    "        \n",
    "        # Compute metrics on original scale\n",
    "        y_pred = feature_library @ self._coefficients\n",
    "        self._mse = np.mean((y - y_pred)**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        ss_res = np.sum((y - y_pred)**2)\n",
    "        self._r2_score = 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "        \n",
    "        # Analyze selection sources (v4.1)\n",
    "        self._selection_analysis = self.analyze_selection_sources(\n",
    "            self._support, self._library_names\n",
    "        )\n",
    "        \n",
    "        self._fit_complete = True\n",
    "        \n",
    "        result = {\n",
    "            'coefficients': self._coefficients,\n",
    "            'support': self._support,\n",
    "            'equation': self.get_equation(),\n",
    "            'selection_analysis': self._selection_analysis,\n",
    "            'n_active_terms': int(np.sum(self._support)),\n",
    "            'n_iterations': self._n_iterations,\n",
    "            'r_squared': self._r2_score,\n",
    "            'r2_score': self._r2_score,  # Alias for compatibility\n",
    "            'mse': self._mse,\n",
    "            'convergence_history': self._convergence_history,\n",
    "            'threshold': self.threshold,\n",
    "            'terms_limited': self._terms_limited,  # v4.2\n",
    "            'n_selected_before_limit': n_selected_original  # v4.2\n",
    "        }\n",
    "        \n",
    "        # Add weak form matrices if used\n",
    "        if self.use_weak_form:\n",
    "            result['weak_form_Q'] = Q\n",
    "            result['weak_form_b'] = b\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_selection_sources(\n",
    "        self,\n",
    "        support: np.ndarray,\n",
    "        library_names: List[str]\n",
    "    ) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Analyze where selected terms originated (v4.1).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        support : np.ndarray\n",
    "            Boolean mask of selected terms\n",
    "        library_names : List[str]\n",
    "            Feature names with source tags\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, int]\n",
    "            - from_powerlaw: Count of [PowLaw] terms selected (v4.1 NEW)\n",
    "            - from_pysr: Count of [PySR] terms selected\n",
    "            - from_variant: Count of [Var] terms selected\n",
    "            - from_poly: Count of [Poly] terms selected\n",
    "            - from_op: Count of [Op] terms selected\n",
    "            - from_unknown: Count of untagged terms selected\n",
    "            - total_selected: Total selected terms\n",
    "        \"\"\"\n",
    "        sources = {\n",
    "            'from_powerlaw': 0,\n",
    "            'from_pysr': 0,\n",
    "            'from_variant': 0,\n",
    "            'from_poly': 0,\n",
    "            'from_op': 0,\n",
    "            'from_unknown': 0,\n",
    "            'total_selected': 0\n",
    "        }\n",
    "        \n",
    "        selected_indices = np.where(support)[0]\n",
    "        sources['total_selected'] = len(selected_indices)\n",
    "        \n",
    "        for idx in selected_indices:\n",
    "            if idx >= len(library_names):\n",
    "                sources['from_unknown'] += 1\n",
    "                continue\n",
    "                \n",
    "            name = library_names[idx]\n",
    "            if name.startswith('[PowLaw]'):\n",
    "                sources['from_powerlaw'] += 1\n",
    "            elif name.startswith('[PySR]'):\n",
    "                sources['from_pysr'] += 1\n",
    "            elif name.startswith('[Var]'):\n",
    "                sources['from_variant'] += 1\n",
    "            elif name.startswith('[Poly]'):\n",
    "                sources['from_poly'] += 1\n",
    "            elif name.startswith('[Op]'):\n",
    "                sources['from_op'] += 1\n",
    "            else:\n",
    "                sources['from_unknown'] += 1\n",
    "        \n",
    "        return sources\n",
    "    \n",
    "    def _normalize_library(\n",
    "        self,\n",
    "        Phi: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Normalize feature library columns to unit variance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Phi : np.ndarray\n",
    "            Feature library matrix\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "            (normalized_Phi, column_scales)\n",
    "        \"\"\"\n",
    "        scales = np.std(Phi, axis=0)\n",
    "        scales[scales < 1e-10] = 1.0  # Avoid division by zero\n",
    "        \n",
    "        normalized = Phi / scales\n",
    "        return normalized, scales\n",
    "    \n",
    "    def _generate_time_vector(\n",
    "        self,\n",
    "        n_samples: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate uniform time vector.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of samples\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Time vector from 0 to 1\n",
    "        \"\"\"\n",
    "        return np.linspace(0, 1, n_samples)\n",
    "    \n",
    "    def _generate_test_functions(\n",
    "        self,\n",
    "        t: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate test functions and their derivatives.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        t : np.ndarray\n",
    "            Time vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "            - psi: Test functions of shape (n_test, n_samples)\n",
    "            - dpsi: Derivatives of shape (n_test, n_samples)\n",
    "        \"\"\"\n",
    "        n_samples = len(t)\n",
    "        t_min, t_max = t.min(), t.max()\n",
    "        t_range = t_max - t_min\n",
    "        \n",
    "        # Centers for test functions (avoid boundaries)\n",
    "        centers = np.linspace(\n",
    "            t_min + 0.1 * t_range,\n",
    "            t_max - 0.1 * t_range,\n",
    "            self.n_test_functions\n",
    "        )\n",
    "        \n",
    "        width = self.test_function_width * t_range\n",
    "        \n",
    "        psi = np.zeros((self.n_test_functions, n_samples))\n",
    "        dpsi = np.zeros((self.n_test_functions, n_samples))\n",
    "        \n",
    "        for m, center in enumerate(centers):\n",
    "            if self.test_function_type == 'gaussian':\n",
    "                psi[m], dpsi[m] = self._gaussian_bump(t, center, width)\n",
    "            else:\n",
    "                psi[m], dpsi[m] = self._polynomial_bump(t, center, width)\n",
    "        \n",
    "        return psi, dpsi\n",
    "    \n",
    "    def _gaussian_bump(\n",
    "        self,\n",
    "        t: np.ndarray,\n",
    "        center: float,\n",
    "        width: float\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate Gaussian bump test function.\n",
    "        \n",
    "        psi(t) = exp(-(t - center)^2 / (2 * width^2))\n",
    "        dpsi(t) = -(t - center) / width^2 * psi(t)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        t : np.ndarray\n",
    "            Time vector\n",
    "        center : float\n",
    "            Center of Gaussian\n",
    "        width : float\n",
    "            Width (standard deviation)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "            (psi, dpsi)\n",
    "        \"\"\"\n",
    "        z = (t - center) / width\n",
    "        psi = np.exp(-0.5 * z**2)\n",
    "        dpsi = -z / width * psi\n",
    "        return psi, dpsi\n",
    "    \n",
    "    def _polynomial_bump(\n",
    "        self,\n",
    "        t: np.ndarray,\n",
    "        center: float,\n",
    "        width: float\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate polynomial bump test function.\n",
    "        \n",
    "        Uses (1 - ((t-center)/width)^2)^4 for compact support.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        t : np.ndarray\n",
    "            Time vector\n",
    "        center : float\n",
    "            Center of bump\n",
    "        width : float\n",
    "            Half-width of support\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "            (psi, dpsi)\n",
    "        \"\"\"\n",
    "        z = (t - center) / width\n",
    "        mask = np.abs(z) < 1\n",
    "        \n",
    "        psi = np.zeros_like(t)\n",
    "        dpsi = np.zeros_like(t)\n",
    "        \n",
    "        psi[mask] = (1 - z[mask]**2)**4\n",
    "        dpsi[mask] = -8 * z[mask] / width * (1 - z[mask]**2)**3\n",
    "        \n",
    "        return psi, dpsi\n",
    "    \n",
    "    def _weak_form_transform(\n",
    "        self,\n",
    "        Phi: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        t: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Apply weak form transformation.\n",
    "        \n",
    "        Q[m,k] = integral(psi_m * Phi_k) dt\n",
    "        b[m] = -integral(dpsi_m * y) dt  (integration by parts)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Phi : np.ndarray\n",
    "            Feature library (n_samples, n_features)\n",
    "        y : np.ndarray\n",
    "            Target vector (n_samples,)\n",
    "        t : np.ndarray\n",
    "            Time vector (n_samples,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "            (Q, b) - Weak form matrices\n",
    "        \"\"\"\n",
    "        n_samples, n_features = Phi.shape\n",
    "        \n",
    "        # Generate test functions\n",
    "        psi, dpsi = self._generate_test_functions(t)\n",
    "        \n",
    "        # Compute weak form matrices via numerical integration\n",
    "        Q = np.zeros((self.n_test_functions, n_features))\n",
    "        b = np.zeros(self.n_test_functions)\n",
    "        \n",
    "        for m in range(self.n_test_functions):\n",
    "            # Q[m, k] = integral(psi_m * Phi_k)\n",
    "            for k in range(n_features):\n",
    "                Q[m, k] = np.trapz(psi[m] * Phi[:, k], t)\n",
    "            \n",
    "            # b[m] = -integral(dpsi_m * y) (integration by parts)\n",
    "            b[m] = -np.trapz(dpsi[m] * y, t)\n",
    "        \n",
    "        return Q, b\n",
    "    \n",
    "    def _stlsq_iteration(\n",
    "        self,\n",
    "        Q: np.ndarray,\n",
    "        b: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sequentially Thresholded Least Squares (STLSQ) with relative thresholding.\n",
    "        \n",
    "        Algorithm (v4.1 Enhanced):\n",
    "            1. Initialize with Ridge OLS solution\n",
    "            2. Compute effective threshold (relative or absolute)\n",
    "            3. Threshold small coefficients to zero\n",
    "            4. Refit OLS on remaining support\n",
    "            5. Repeat until convergence\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Q : np.ndarray\n",
    "            Design matrix (n_equations, n_features)\n",
    "        b : np.ndarray\n",
    "            Target vector (n_equations,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Sparse coefficient vector\n",
    "        \"\"\"\n",
    "        n_features = Q.shape[1]\n",
    "        self._convergence_history = []\n",
    "        \n",
    "        # Step 1: Initialize with regularized OLS (for stability)\n",
    "        try:\n",
    "            ridge = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "            ridge.fit(Q, b)\n",
    "            xi = ridge.coef_\n",
    "        except Exception:\n",
    "            xi = np.linalg.lstsq(Q, b, rcond=None)[0]\n",
    "        \n",
    "        self._convergence_history.append(xi.copy())\n",
    "        \n",
    "        # Step 2-5: Iterative thresholding with relative threshold\n",
    "        for iteration in range(self.max_iter):\n",
    "            self._n_iterations = iteration + 1\n",
    "            \n",
    "            # v4.1 FIX: Compute effective threshold\n",
    "            if self.relative_threshold:\n",
    "                max_coef = np.max(np.abs(xi))\n",
    "                if max_coef > 1e-10:\n",
    "                    effective_threshold = self.threshold * max_coef\n",
    "                else:\n",
    "                    effective_threshold = self.threshold\n",
    "            else:\n",
    "                effective_threshold = self.threshold\n",
    "            \n",
    "            self._effective_threshold = effective_threshold\n",
    "            \n",
    "            # Threshold small coefficients\n",
    "            small_mask = np.abs(xi) < effective_threshold\n",
    "            xi[small_mask] = 0\n",
    "            \n",
    "            # Get active indices\n",
    "            active = ~small_mask\n",
    "            \n",
    "            # Check if any coefficients remain\n",
    "            if not np.any(active):\n",
    "                break\n",
    "            \n",
    "            # Refit on active support\n",
    "            Q_active = Q[:, active]\n",
    "            try:\n",
    "                xi_active = np.linalg.lstsq(Q_active, b, rcond=None)[0]\n",
    "            except Exception:\n",
    "                break\n",
    "            \n",
    "            # Update full coefficient vector\n",
    "            xi_new = np.zeros(n_features)\n",
    "            xi_new[active] = xi_active\n",
    "            \n",
    "            self._convergence_history.append(xi_new.copy())\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.allclose(xi, xi_new, rtol=1e-6):\n",
    "                xi = xi_new\n",
    "                break\n",
    "            \n",
    "            xi = xi_new\n",
    "        \n",
    "        return xi\n",
    "    \n",
    "    def get_equation(self) -> str:\n",
    "        \"\"\"\n",
    "        Get string representation of discovered equation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Equation string with source tags\n",
    "        \"\"\"\n",
    "        if not self._fit_complete:\n",
    "            return \"\"\n",
    "        \n",
    "        terms = []\n",
    "        for i, (coef, active) in enumerate(zip(self._coefficients, self._support)):\n",
    "            if active:\n",
    "                name = self._library_names[i]\n",
    "                if abs(coef) > 0.001:\n",
    "                    terms.append(f\"{coef:.3f} * {name}\")\n",
    "        \n",
    "        if len(terms) == 0:\n",
    "            return \"0\"\n",
    "        \n",
    "        return \" + \".join(terms)\n",
    "    \n",
    "    def get_active_terms(self) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get list of active terms with coefficients.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[str, float]]\n",
    "            List of (term_name, coefficient) pairs\n",
    "        \"\"\"\n",
    "        if not self._fit_complete:\n",
    "            return []\n",
    "        \n",
    "        active_terms = []\n",
    "        for i, (coef, active) in enumerate(zip(self._coefficients, self._support)):\n",
    "            if active:\n",
    "                active_terms.append((self._library_names[i], coef))\n",
    "        \n",
    "        return active_terms\n",
    "    \n",
    "    def predict(self, Phi_new: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions using discovered equation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Phi_new : np.ndarray\n",
    "            New feature library matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        if not self._fit_complete:\n",
    "            raise RuntimeError(\"Must call fit() before predict()\")\n",
    "        \n",
    "        return Phi_new @ self._coefficients\n",
    "    \n",
    "    def print_stlsq_report(self) -> None:\n",
    "        \"\"\"\n",
    "        Print detailed STLSQ results report in v4.2 format.\n",
    "        \"\"\"\n",
    "        if not self._fit_complete:\n",
    "            print(\"Fit not yet performed. Run fit() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"=== E-WSINDy Results (v4.2 Hybrid) ===\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(f\"Selected terms: {np.sum(self._support)}\")\n",
    "        if self._terms_limited:\n",
    "            print(f\"  [v4.2] Terms were limited (original selection was larger)\")\n",
    "        print()\n",
    "        \n",
    "        # Print active terms with source tags\n",
    "        for name, coef in self.get_active_terms():\n",
    "            print(f\"  {coef:8.3f} * {name}\")\n",
    "        print()\n",
    "        \n",
    "        # Print selection analysis\n",
    "        print(\"Selection Analysis:\")\n",
    "        print(f\"  from_powerlaw: {self._selection_analysis.get('from_powerlaw', 0)}\")\n",
    "        print(f\"  from_pysr: {self._selection_analysis['from_pysr']}\")\n",
    "        print(f\"  from_variant: {self._selection_analysis['from_variant']}\")\n",
    "        print(f\"  from_poly: {self._selection_analysis['from_poly']}\")\n",
    "        print(f\"  from_op: {self._selection_analysis['from_op']}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"R-squared: {self._r2_score:.4f}\")\n",
    "        print(f\"MSE: {self._mse:.6f}\")\n",
    "        print(f\"Iterations: {self._n_iterations}\")\n",
    "        print(f\"Threshold: {self.threshold}\")\n",
    "        if self._effective_threshold is not None:\n",
    "            print(f\"Effective Threshold: {self._effective_threshold:.6f}\")\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "print(\"EWSINDySTLSQ class v4.2 (Hybrid term limiting) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2b: E-SINDy Ensemble Class (v4.1 NEW)\n",
    "\n",
    "**Layer 2 Enhancement:** Bootstrap ensemble for robust sparse selection.\n",
    "\n",
    "Reference: Fasel et al. (2022) Ensemble-SINDy: Robust sparse model discovery\n",
    "in the low-data, high-noise limit. Proc. R. Soc. A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# E-SINDY ENSEMBLE CLASS (v4.1 NEW - Layer 2)\n",
    "# ==============================================================================\n",
    "\n",
    "class ESINDyEnsemble:\n",
    "    \"\"\"\n",
    "    E-SINDy: Ensemble Sparse Identification of Nonlinear Dynamics.\n",
    "    \n",
    "    Provides robust sparse selection via bootstrap aggregation with\n",
    "    inclusion probability estimation. More robust than single-run STLSQ\n",
    "    for threshold selection and provides natural uncertainty quantification.\n",
    "    \n",
    "    Key Features (v4.1):\n",
    "    - Bootstrap sampling for robust term selection\n",
    "    - Inclusion probability estimation (natural UQ)\n",
    "    - Library bagging option for ill-conditioned libraries\n",
    "    - Coefficient distribution estimation\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Fasel et al. (2022). Ensemble-SINDy: Robust sparse model discovery\n",
    "    in the low-data, high-noise limit. Proc. R. Soc. A.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> ensemble = ESINDyEnsemble(n_models=100, inclusion_threshold=0.9)\n",
    "    >>> result = ensemble.fit(Phi, y, library_names=names)\n",
    "    >>> print(f\\\"Inclusion probs: {result['inclusion_probabilities']}\\\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_models: int = 100,\n",
    "        inclusion_threshold: float = 0.9,\n",
    "        threshold_range: Tuple[float, float] = (0.05, 0.3),\n",
    "        library_bagging: bool = False,\n",
    "        bagging_fraction: float = 0.8,\n",
    "        use_weak_form: bool = False,\n",
    "        normalize_columns: bool = True,\n",
    "        random_state: int = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize E-SINDy Ensemble.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_models : int\n",
    "            Number of bootstrap models to fit. Default: 100\n",
    "        inclusion_threshold : float\n",
    "            Minimum inclusion probability for final selection.\n",
    "            Terms with P(inclusion) >= threshold are selected. Default: 0.9\n",
    "        threshold_range : Tuple[float, float]\n",
    "            Range for random STLSQ threshold sampling. Default: (0.05, 0.3)\n",
    "        library_bagging : bool\n",
    "            If True, also subsample library columns. Default: False\n",
    "        bagging_fraction : float\n",
    "            Fraction of library to use if library_bagging=True. Default: 0.8\n",
    "        use_weak_form : bool\n",
    "            Whether to use weak form in base STLSQ. Default: False\n",
    "        normalize_columns : bool\n",
    "            Whether to normalize library columns. Default: True\n",
    "        random_state : int\n",
    "            Random seed for reproducibility. Default: None\n",
    "        \"\"\"\n",
    "        self.n_models = n_models\n",
    "        self.inclusion_threshold = inclusion_threshold\n",
    "        self.threshold_range = threshold_range\n",
    "        self.library_bagging = library_bagging\n",
    "        self.bagging_fraction = bagging_fraction\n",
    "        self.use_weak_form = use_weak_form\n",
    "        self.normalize_columns = normalize_columns\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Results storage\n",
    "        self._inclusion_counts = None\n",
    "        self._inclusion_probabilities = None\n",
    "        self._coef_samples = None\n",
    "        self._selected_support = None\n",
    "        self._final_coefficients = None\n",
    "        self._library_names = None\n",
    "        self._r2_score = None\n",
    "        self._fit_complete = False\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        feature_library: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        library_names: List[str] = None,\n",
    "        t: np.ndarray = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fit E-SINDy ensemble model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_library : np.ndarray\n",
    "            Feature library matrix (n_samples, n_features)\n",
    "        y : np.ndarray\n",
    "            Target vector (n_samples,)\n",
    "        library_names : List[str], optional\n",
    "            Feature names with source tags\n",
    "        t : np.ndarray, optional\n",
    "            Time vector for weak form\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            - coefficients: Final coefficient vector\n",
    "            - support: Boolean mask of selected terms\n",
    "            - inclusion_probabilities: P(term selected) for each term\n",
    "            - coef_mean: Mean coefficients across ensemble\n",
    "            - coef_std: Std of coefficients across ensemble\n",
    "            - selection_analysis: Source attribution\n",
    "            - r_squared: Final model R-squared\n",
    "        \"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples, n_features = feature_library.shape\n",
    "        \n",
    "        # Set library names\n",
    "        if library_names is None:\n",
    "            self._library_names = [f'f{i}' for i in range(n_features)]\n",
    "        else:\n",
    "            self._library_names = list(library_names)\n",
    "        \n",
    "        # Initialize counters\n",
    "        self._inclusion_counts = np.zeros(n_features)\n",
    "        self._coef_samples = []\n",
    "        \n",
    "        # Bootstrap ensemble\n",
    "        for model_idx in range(self.n_models):\n",
    "            # Bootstrap sample rows\n",
    "            row_idx = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            Phi_boot = feature_library[row_idx]\n",
    "            y_boot = y[row_idx]\n",
    "            names_boot = self._library_names\n",
    "            \n",
    "            # Optional library bagging (column subsampling)\n",
    "            col_idx = np.arange(n_features)\n",
    "            if self.library_bagging:\n",
    "                n_cols = int(n_features * self.bagging_fraction)\n",
    "                col_idx = np.random.choice(n_features, n_cols, replace=False)\n",
    "                col_idx = np.sort(col_idx)\n",
    "                Phi_boot = Phi_boot[:, col_idx]\n",
    "                names_boot = [self._library_names[i] for i in col_idx]\n",
    "            \n",
    "            # Random threshold from range\n",
    "            threshold = np.random.uniform(\n",
    "                self.threshold_range[0],\n",
    "                self.threshold_range[1]\n",
    "            )\n",
    "            \n",
    "            # Fit single STLSQ model\n",
    "            stlsq = EWSINDySTLSQ(\n",
    "                threshold=threshold,\n",
    "                use_weak_form=self.use_weak_form,\n",
    "                normalize_columns=self.normalize_columns,\n",
    "                relative_threshold=True\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                result = stlsq.fit(Phi_boot, y_boot, library_names=names_boot, t=t)\n",
    "                \n",
    "                # Map back to full feature space if library bagging\n",
    "                full_support = np.zeros(n_features, dtype=bool)\n",
    "                full_coef = np.zeros(n_features)\n",
    "                \n",
    "                if self.library_bagging:\n",
    "                    full_support[col_idx] = result['support']\n",
    "                    full_coef[col_idx] = result['coefficients']\n",
    "                else:\n",
    "                    full_support = result['support']\n",
    "                    full_coef = result['coefficients']\n",
    "                \n",
    "                # Record inclusion\n",
    "                self._inclusion_counts += full_support.astype(float)\n",
    "                self._coef_samples.append(full_coef)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Skip failed fits\n",
    "                continue\n",
    "        \n",
    "        # Compute inclusion probabilities\n",
    "        n_successful = len(self._coef_samples)\n",
    "        if n_successful == 0:\n",
    "            raise ValueError(\"All ensemble models failed to fit\")\n",
    "        \n",
    "        self._inclusion_probabilities = self._inclusion_counts / n_successful\n",
    "        \n",
    "        # Select terms with high inclusion probability\n",
    "        self._selected_support = self._inclusion_probabilities >= self.inclusion_threshold\n",
    "        \n",
    "        # Compute coefficient statistics\n",
    "        coef_array = np.array(self._coef_samples)\n",
    "        coef_mean = np.mean(coef_array, axis=0)\n",
    "        coef_std = np.std(coef_array, axis=0)\n",
    "        \n",
    "        # Final refit on selected support (unbiased)\n",
    "        if np.any(self._selected_support):\n",
    "            Phi_selected = feature_library[:, self._selected_support]\n",
    "            try:\n",
    "                final_coef_active = np.linalg.lstsq(\n",
    "                    Phi_selected, y, rcond=None\n",
    "                )[0]\n",
    "                self._final_coefficients = np.zeros(n_features)\n",
    "                self._final_coefficients[self._selected_support] = final_coef_active\n",
    "            except Exception:\n",
    "                # Use mean coefficients as fallback\n",
    "                self._final_coefficients = coef_mean * self._selected_support\n",
    "        else:\n",
    "            self._final_coefficients = np.zeros(n_features)\n",
    "        \n",
    "        # Compute R-squared\n",
    "        y_pred = feature_library @ self._final_coefficients\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        ss_res = np.sum((y - y_pred)**2)\n",
    "        self._r2_score = 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "        \n",
    "        # Selection analysis\n",
    "        selection_analysis = self._analyze_sources(\n",
    "            self._selected_support, self._library_names\n",
    "        )\n",
    "        \n",
    "        self._fit_complete = True\n",
    "        \n",
    "        return {\n",
    "            'coefficients': self._final_coefficients,\n",
    "            'support': self._selected_support,\n",
    "            'inclusion_probabilities': self._inclusion_probabilities,\n",
    "            'coef_mean': coef_mean,\n",
    "            'coef_std': coef_std,\n",
    "            'n_active_terms': int(np.sum(self._selected_support)),\n",
    "            'selection_analysis': selection_analysis,\n",
    "            'r_squared': self._r2_score,\n",
    "            'r2_score': self._r2_score,\n",
    "            'n_successful_models': n_successful,\n",
    "            'equation': self.get_equation()\n",
    "        }\n",
    "    \n",
    "    def _analyze_sources(\n",
    "        self,\n",
    "        support: np.ndarray,\n",
    "        library_names: List[str]\n",
    "    ) -> Dict[str, int]:\n",
    "        \"\"\"Analyze selection sources (same as EWSINDySTLSQ).\"\"\"\n",
    "        sources = {\n",
    "            'from_powerlaw': 0,\n",
    "            'from_pysr': 0,\n",
    "            'from_variant': 0,\n",
    "            'from_poly': 0,\n",
    "            'from_op': 0,\n",
    "            'from_unknown': 0,\n",
    "            'total_selected': 0\n",
    "        }\n",
    "        \n",
    "        selected_indices = np.where(support)[0]\n",
    "        sources['total_selected'] = len(selected_indices)\n",
    "        \n",
    "        for idx in selected_indices:\n",
    "            if idx >= len(library_names):\n",
    "                sources['from_unknown'] += 1\n",
    "                continue\n",
    "            \n",
    "            name = library_names[idx]\n",
    "            if name.startswith('[PowLaw]'):\n",
    "                sources['from_powerlaw'] += 1\n",
    "            elif name.startswith('[PySR]'):\n",
    "                sources['from_pysr'] += 1\n",
    "            elif name.startswith('[Var]'):\n",
    "                sources['from_variant'] += 1\n",
    "            elif name.startswith('[Poly]'):\n",
    "                sources['from_poly'] += 1\n",
    "            elif name.startswith('[Op]'):\n",
    "                sources['from_op'] += 1\n",
    "            else:\n",
    "                sources['from_unknown'] += 1\n",
    "        \n",
    "        return sources\n",
    "    \n",
    "    def get_equation(self) -> str:\n",
    "        \"\"\"Get string representation of discovered equation.\"\"\"\n",
    "        if not self._fit_complete:\n",
    "            return \"\"\n",
    "        \n",
    "        terms = []\n",
    "        for i, (coef, active) in enumerate(zip(self._final_coefficients, self._selected_support)):\n",
    "            if active and abs(coef) > 1e-10:\n",
    "                name = self._library_names[i]\n",
    "                prob = self._inclusion_probabilities[i]\n",
    "                terms.append(f\"{coef:.4f} * {name} (P={prob:.2f})\")\n",
    "        \n",
    "        return \" + \".join(terms) if terms else \"0\"\n",
    "    \n",
    "    def get_high_confidence_terms(\n",
    "        self,\n",
    "        min_probability: float = 0.9\n",
    "    ) -> List[Tuple[str, float, float]]:\n",
    "        \"\"\"\n",
    "        Get terms with high inclusion probability.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[str, float, float]]\n",
    "            List of (name, coefficient, inclusion_probability)\n",
    "        \"\"\"\n",
    "        if not self._fit_complete:\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for i, prob in enumerate(self._inclusion_probabilities):\n",
    "            if prob >= min_probability:\n",
    "                results.append((\n",
    "                    self._library_names[i],\n",
    "                    self._final_coefficients[i],\n",
    "                    prob\n",
    "                ))\n",
    "        \n",
    "        return sorted(results, key=lambda x: -x[2])\n",
    "    \n",
    "    def print_ensemble_report(self) -> None:\n",
    "        \"\"\"Print detailed ensemble results.\"\"\"\n",
    "        if not self._fit_complete:\n",
    "            print(\"Model not yet fitted.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\" E-SINDy Ensemble Report\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(f\"Models fitted: {len(self._coef_samples)}\")\n",
    "        print(f\"Inclusion threshold: {self.inclusion_threshold}\")\n",
    "        print(f\"Selected terms: {np.sum(self._selected_support)}\")\n",
    "        print(f\"R-squared: {self._r2_score:.4f}\")\n",
    "        print()\n",
    "        print(\"High-confidence terms (P >= 0.9):\")\n",
    "        for name, coef, prob in self.get_high_confidence_terms(0.9):\n",
    "            print(f\"  {coef:10.4f} * {name:30s} (P={prob:.3f})\")\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "print(\"ESINDyEnsemble class v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2c: EBIC Threshold Selection (v4.1 NEW)\n",
    "\n",
    "**Layer 3 Enhancement:** Automatic threshold selection via Extended BIC.\n",
    "\n",
    "The Extended Bayesian Information Criterion (EBIC) balances model fit\n",
    "against complexity, with a penalty term that accounts for the large\n",
    "search space in sparse regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EBIC THRESHOLD SELECTION (v4.1 NEW - Layer 3)\n",
    "# ==============================================================================\n",
    "\n",
    "def select_threshold_by_ebic(\n",
    "    feature_library: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    library_names: List[str] = None,\n",
    "    thresholds: List[float] = None,\n",
    "    gamma: float = 0.5,\n",
    "    use_weak_form: bool = False,\n",
    "    normalize_columns: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Select optimal STLSQ threshold using Extended BIC.\n",
    "    \n",
    "    The Extended BIC (Chen & Chen, 2008) adds a penalty term that scales\n",
    "    with the size of the model space, making it suitable for high-dimensional\n",
    "    sparse regression where the number of potential models is large.\n",
    "    \n",
    "    EBIC = n * log(MSE) + k * log(n) + 2 * gamma * k * log(p)\n",
    "    \n",
    "    where:\n",
    "    - n = number of samples\n",
    "    - k = number of selected terms\n",
    "    - p = total number of candidate terms\n",
    "    - gamma in [0, 1] controls penalty strength (0.5 recommended)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_library : np.ndarray\n",
    "        Feature library matrix (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target vector (n_samples,)\n",
    "    library_names : List[str], optional\n",
    "        Feature names\n",
    "    thresholds : List[float], optional\n",
    "        Candidate thresholds to evaluate.\n",
    "        Default: [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "    gamma : float\n",
    "        EBIC penalty parameter. Higher values favor sparser models.\n",
    "        Default: 0.5\n",
    "    use_weak_form : bool\n",
    "        Whether to use weak form. Default: False\n",
    "    normalize_columns : bool\n",
    "        Whether to normalize columns. Default: True\n",
    "    verbose : bool\n",
    "        Print details for each threshold. Default: False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        - best_threshold: Optimal threshold\n",
    "        - best_ebic: EBIC at optimal threshold\n",
    "        - best_result: Full result dict from best model\n",
    "        - all_results: Results for all thresholds\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Chen, J., & Chen, Z. (2008). Extended Bayesian information criteria\n",
    "    for model selection with large model spaces. Biometrika, 95(3), 759-771.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "    \n",
    "    n_samples, n_features = feature_library.shape\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"EBIC Threshold Selection\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Threshold':<12} {'k (terms)':<12} {'MSE':<15} {'EBIC':<15}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    best_ebic = np.inf\n",
    "    best_threshold = thresholds[0]\n",
    "    best_result = None\n",
    "    all_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Fit STLSQ with this threshold\n",
    "        stlsq = EWSINDySTLSQ(\n",
    "            threshold=threshold,\n",
    "            use_weak_form=use_weak_form,\n",
    "            normalize_columns=normalize_columns,\n",
    "            relative_threshold=True\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = stlsq.fit(feature_library, y, library_names=library_names)\n",
    "        except Exception:\n",
    "            continue\n",
    "        \n",
    "        k = result['n_active_terms']\n",
    "        mse = result['mse']\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if k == 0:\n",
    "            # No terms selected - use total variance as MSE\n",
    "            mse = np.var(y)\n",
    "            ebic = np.inf\n",
    "        elif mse <= 0:\n",
    "            # Perfect fit\n",
    "            ebic = -np.inf\n",
    "        else:\n",
    "            # Compute EBIC\n",
    "            # EBIC = n * log(MSE) + k * log(n) + 2 * gamma * k * log(p)\n",
    "            ebic = (\n",
    "                n_samples * np.log(mse) +\n",
    "                k * np.log(n_samples) +\n",
    "                2 * gamma * k * np.log(n_features)\n",
    "            )\n",
    "        \n",
    "        result['ebic'] = ebic\n",
    "        result['threshold_used'] = threshold\n",
    "        all_results.append(result)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{threshold:<12.3f} {k:<12d} {mse:<15.6f} {ebic:<15.2f}\")\n",
    "        \n",
    "        if ebic < best_ebic:\n",
    "            best_ebic = ebic\n",
    "            best_threshold = threshold\n",
    "            best_result = result\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Best threshold: {best_threshold} (EBIC={best_ebic:.2f})\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_ebic': best_ebic,\n",
    "        'best_result': best_result,\n",
    "        'all_results': all_results\n",
    "    }\n",
    "\n",
    "\n",
    "def select_threshold_by_cv(\n",
    "    feature_library: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    library_names: List[str] = None,\n",
    "    thresholds: List[float] = None,\n",
    "    n_folds: int = 5,\n",
    "    use_weak_form: bool = False,\n",
    "    normalize_columns: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Select optimal STLSQ threshold using cross-validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_library : np.ndarray\n",
    "        Feature library matrix\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "    library_names : List[str], optional\n",
    "        Feature names\n",
    "    thresholds : List[float], optional\n",
    "        Candidate thresholds. Default: [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "    n_folds : int\n",
    "        Number of CV folds. Default: 5\n",
    "    use_weak_form : bool\n",
    "        Whether to use weak form. Default: False\n",
    "    normalize_columns : bool\n",
    "        Whether to normalize columns. Default: True\n",
    "    verbose : bool\n",
    "        Print details. Default: False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        - best_threshold: Optimal threshold\n",
    "        - best_cv_score: Mean CV R-squared at optimal threshold\n",
    "        - cv_results: Results for all thresholds\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    if thresholds is None:\n",
    "        thresholds = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "    \n",
    "    n_samples = feature_library.shape[0]\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Cross-Validation Threshold Selection\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Threshold':<12} {'Mean R2':<12} {'Std R2':<12} {'Mean k':<12}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    best_cv_score = -np.inf\n",
    "    best_threshold = thresholds[0]\n",
    "    cv_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        fold_r2s = []\n",
    "        fold_ks = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(feature_library):\n",
    "            X_train = feature_library[train_idx]\n",
    "            y_train = y[train_idx]\n",
    "            X_val = feature_library[val_idx]\n",
    "            y_val = y[val_idx]\n",
    "            \n",
    "            stlsq = EWSINDySTLSQ(\n",
    "                threshold=threshold,\n",
    "                use_weak_form=use_weak_form,\n",
    "                normalize_columns=normalize_columns,\n",
    "                relative_threshold=True\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                result = stlsq.fit(X_train, y_train, library_names=library_names)\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                y_pred = X_val @ result['coefficients']\n",
    "                ss_tot = np.sum((y_val - np.mean(y_val))**2)\n",
    "                ss_res = np.sum((y_val - y_pred)**2)\n",
    "                val_r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "                \n",
    "                fold_r2s.append(val_r2)\n",
    "                fold_ks.append(result['n_active_terms'])\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if len(fold_r2s) > 0:\n",
    "            mean_r2 = np.mean(fold_r2s)\n",
    "            std_r2 = np.std(fold_r2s)\n",
    "            mean_k = np.mean(fold_ks)\n",
    "            \n",
    "            cv_results.append({\n",
    "                'threshold': threshold,\n",
    "                'mean_r2': mean_r2,\n",
    "                'std_r2': std_r2,\n",
    "                'mean_k': mean_k\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{threshold:<12.3f} {mean_r2:<12.4f} {std_r2:<12.4f} {mean_k:<12.1f}\")\n",
    "            \n",
    "            if mean_r2 > best_cv_score:\n",
    "                best_cv_score = mean_r2\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Best threshold: {best_threshold} (CV R2={best_cv_score:.4f})\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_cv_score': best_cv_score,\n",
    "        'cv_results': cv_results\n",
    "    }\n",
    "\n",
    "print(\"EBIC and CV threshold selection functions v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Internal Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST CONTROL FLAG\n",
    "# ==============================================================================\n",
    "\n",
    "_RUN_TESTS = False  # Set to True to run internal tests\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" RUNNING INTERNAL TESTS FOR 07_EWSINDy_STLSQ v4.1\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 1: Basic STLSQ with Standard Library\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 1: Basic STLSQ with Standard Library\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    x = np.random.uniform(0.1, 2, n_samples)\n",
    "    y = 3*x + 2*x**2 + 0.01*np.random.randn(n_samples)\n",
    "    \n",
    "    # Standard library (no source tags)\n",
    "    Phi = np.column_stack([np.ones(n_samples), x, x**2, x**3, x**4])\n",
    "    library_names = ['1', 'x', 'x^2', 'x^3', 'x^4']\n",
    "    \n",
    "    model = EWSINDySTLSQ(threshold=0.1, use_weak_form=False)\n",
    "    result = model.fit(Phi, y, library_names=library_names)\n",
    "    \n",
    "    print(f\"True: y = 3*x + 2*x^2\")\n",
    "    print(f\"Discovered: {result['equation']}\")\n",
    "    print(f\"R-squared: {result['r_squared']:.4f}\")\n",
    "    print(f\"Active terms: {result['n_active_terms']}\")\n",
    "    print()\n",
    "    \n",
    "    if result['r_squared'] > 0.99:\n",
    "        print(\"[PASS] High accuracy achieved\")\n",
    "    else:\n",
    "        print(\"[WARNING] Accuracy lower than expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 2: Source Attribution with Augmented Library\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 2: Source Attribution with Augmented Library\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    x = np.random.uniform(0.1, 2, n_samples)\n",
    "    z = np.random.uniform(0.1, 2, n_samples)\n",
    "    \n",
    "    # True equation: y = 0.5*x^2 + sin(z)\n",
    "    y = 0.5*x**2 + np.sin(z) + 0.01*np.random.randn(n_samples)\n",
    "    \n",
    "    # Simulated augmented library with source tags\n",
    "    Phi = np.column_stack([\n",
    "        x**2,           # [PySR] x**2\n",
    "        np.sin(z),      # [PySR] sin(z)\n",
    "        np.sin(x),      # [Var] sin(x)\n",
    "        np.ones(n_samples),  # [Poly] 1\n",
    "        x,              # [Poly] x\n",
    "        z,              # [Poly] z\n",
    "        x*z,            # [Poly] x*z\n",
    "        np.cos(x),      # [Op] cos(x)\n",
    "        np.cos(z)       # [Op] cos(z)\n",
    "    ])\n",
    "    \n",
    "    library_names = [\n",
    "        '[PySR] x**2',\n",
    "        '[PySR] sin(z)',\n",
    "        '[Var] sin(x)',\n",
    "        '[Poly] 1',\n",
    "        '[Poly] x',\n",
    "        '[Poly] z',\n",
    "        '[Poly] x*z',\n",
    "        '[Op] cos(x)',\n",
    "        '[Op] cos(z)'\n",
    "    ]\n",
    "    \n",
    "    model = EWSINDySTLSQ(threshold=0.1, use_weak_form=False)\n",
    "    result = model.fit(Phi, y, library_names=library_names)\n",
    "    \n",
    "    print(f\"True: y = 0.5*x^2 + sin(z)\")\n",
    "    print()\n",
    "    print(\"Selected terms:\")\n",
    "    for name, coef in model.get_active_terms():\n",
    "        print(f\"  {coef:8.3f} * {name}\")\n",
    "    print()\n",
    "    print(f\"Selection Analysis: {result['selection_analysis']}\")\n",
    "    print(f\"R-squared: {result['r_squared']:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Check that PySR terms were selected\n",
    "    analysis = result['selection_analysis']\n",
    "    if analysis['from_pysr'] >= 2:\n",
    "        print(\"[PASS] PySR terms correctly selected\")\n",
    "    else:\n",
    "        print(f\"[INFO] Selected {analysis['from_pysr']} PySR terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 3: Noise Robustness with Weak Form\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 3: Noise Robustness with Weak Form\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    t = np.linspace(0, 1, n_samples)\n",
    "    x = np.sin(2 * np.pi * t)\n",
    "    \n",
    "    # True dynamics: dy/dt = x (approximately)\n",
    "    y_clean = x.copy()\n",
    "    \n",
    "    noise_levels = [0.01, 0.05, 0.10]\n",
    "    \n",
    "    Phi = np.column_stack([np.ones(n_samples), x, x**2])\n",
    "    library_names = ['[Poly] 1', '[Poly] x', '[Poly] x^2']\n",
    "    \n",
    "    print(f\"{'Noise':<12} {'R2 (strong)':<15} {'R2 (weak)':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for noise_level in noise_levels:\n",
    "        y_noisy = y_clean + noise_level * np.random.randn(n_samples)\n",
    "        \n",
    "        # Strong form\n",
    "        model_strong = EWSINDySTLSQ(threshold=0.1, use_weak_form=False)\n",
    "        result_strong = model_strong.fit(Phi, y_noisy, library_names=library_names)\n",
    "        \n",
    "        # Weak form\n",
    "        model_weak = EWSINDySTLSQ(threshold=0.1, use_weak_form=True)\n",
    "        result_weak = model_weak.fit(Phi, y_noisy, t=t, library_names=library_names)\n",
    "        \n",
    "        print(f\"{noise_level:<12.2f} {result_strong['r_squared']:<15.4f} {result_weak['r_squared']:<15.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"[INFO] Weak form should be more robust at higher noise levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 4: analyze_selection_sources Method\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 4: analyze_selection_sources Method\")\n",
    "    \n",
    "    # Create mock support and library names\n",
    "    support = np.array([True, True, False, True, False, False, True, False])\n",
    "    library_names = [\n",
    "        '[PySR] sin(x)',\n",
    "        '[PySR] x**2',\n",
    "        '[Var] sin(y)',\n",
    "        '[Poly] 1',\n",
    "        '[Poly] x',\n",
    "        '[Poly] y',\n",
    "        '[Op] exp(x)',\n",
    "        '[Op] cos(x)'\n",
    "    ]\n",
    "    \n",
    "    model = EWSINDySTLSQ()\n",
    "    analysis = model.analyze_selection_sources(support, library_names)\n",
    "    \n",
    "    print(\"Support mask:\")\n",
    "    for i, (s, n) in enumerate(zip(support, library_names)):\n",
    "        status = \"SELECTED\" if s else \"        \"\n",
    "        print(f\"  {i}: {status} {n}\")\n",
    "    print()\n",
    "    print(f\"Analysis result: {analysis}\")\n",
    "    print()\n",
    "    \n",
    "    # Expected: 2 PySR, 0 Var, 1 Poly, 1 Op\n",
    "    expected = {'from_pysr': 2, 'from_variant': 0, 'from_poly': 1, 'from_op': 1}\n",
    "    \n",
    "    all_correct = True\n",
    "    for key, expected_val in expected.items():\n",
    "        actual_val = analysis[key]\n",
    "        status = \"PASS\" if actual_val == expected_val else \"FAIL\"\n",
    "        if status == \"FAIL\":\n",
    "            all_correct = False\n",
    "        print(f\"  {key}: expected={expected_val}, actual={actual_val} [{status}]\")\n",
    "    \n",
    "    print()\n",
    "    if all_correct:\n",
    "        print(\"[PASS] All source attribution correct\")\n",
    "    else:\n",
    "        print(\"[FAIL] Some source attributions incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 5: Full Report Output\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 5: Full Report Output\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    x = np.random.uniform(0.1, 2, n_samples)\n",
    "    z = np.random.uniform(0.1, 2, n_samples)\n",
    "    y = 0.5*x**2 + np.sin(z) + 0.01*np.random.randn(n_samples)\n",
    "    \n",
    "    # Augmented library\n",
    "    Phi = np.column_stack([\n",
    "        x**2,\n",
    "        np.sin(z),\n",
    "        np.ones(n_samples),\n",
    "        x,\n",
    "        z\n",
    "    ])\n",
    "    \n",
    "    library_names = [\n",
    "        '[PySR] x**2',\n",
    "        '[PySR] sin(z)',\n",
    "        '[Poly] 1',\n",
    "        '[Poly] x',\n",
    "        '[Poly] z'\n",
    "    ]\n",
    "    \n",
    "    model = EWSINDySTLSQ(threshold=0.1, use_weak_form=False)\n",
    "    result = model.fit(Phi, y, library_names=library_names)\n",
    "    \n",
    "    # Print full report\n",
    "    model.print_stlsq_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Module Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODULE SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" 07_EWSINDy_STLSQ.ipynb v4.1 - Module Summary\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"THREE-LAYER SPARSE SELECTION ENHANCEMENT\")\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(\"Layer 1: EWSINDySTLSQ (Relative Threshold)\")\n",
    "print(\"  - Scale-invariant STLSQ: threshold = base * max(|coef|)\")\n",
    "print(\"  - Robust to coefficient magnitude variations\")\n",
    "print(\"  - Parameters: threshold, relative_threshold, ridge_alpha\")\n",
    "print()\n",
    "print(\"Layer 2: ESINDyEnsemble (Bootstrap Selection)\")\n",
    "print(\"  - Bootstrap aggregation for robust term selection\")\n",
    "print(\"  - Inclusion probability estimation (natural UQ)\")\n",
    "print(\"  - Optional library bagging for ill-conditioned libraries\")\n",
    "print(\"  - Parameters: n_models, inclusion_threshold, threshold_range\")\n",
    "print()\n",
    "print(\"Layer 3: EBIC/CV Threshold Selection\")\n",
    "print(\"  - select_threshold_by_ebic(): Extended BIC criterion\")\n",
    "print(\"  - select_threshold_by_cv(): Cross-validation criterion\")\n",
    "print(\"  - Automatic threshold tuning\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASS: EWSINDySTLSQ\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Key Parameters:\")\n",
    "print(\"  threshold: Base STLSQ sparsity threshold (default: 0.1)\")\n",
    "print(\"  relative_threshold: Use relative thresholding (default: True)\")\n",
    "print(\"  use_weak_form: Enable weak form (default: True)\")\n",
    "print(\"  normalize_columns: Normalize library columns (default: True)\")\n",
    "print(\"  ridge_alpha: Ridge regularization strength (default: 1e-6)\")\n",
    "print()\n",
    "print(\"Main Methods:\")\n",
    "print(\"  fit(feature_library, y, library_names=None, t=None) -> Dict\")\n",
    "print(\"  analyze_selection_sources(support, library_names) -> Dict\")\n",
    "print(\"  get_equation() -> str\")\n",
    "print(\"  print_stlsq_report()\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASS: ESINDyEnsemble\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Key Parameters:\")\n",
    "print(\"  n_models: Number of bootstrap models (default: 100)\")\n",
    "print(\"  inclusion_threshold: Min probability for selection (default: 0.9)\")\n",
    "print(\"  threshold_range: STLSQ threshold sampling range (default: (0.05, 0.3))\")\n",
    "print(\"  library_bagging: Subsample columns too (default: False)\")\n",
    "print()\n",
    "print(\"Main Methods:\")\n",
    "print(\"  fit(feature_library, y, library_names=None) -> Dict\")\n",
    "print(\"  get_high_confidence_terms(min_prob=0.9) -> List[Tuple]\")\n",
    "print(\"  print_ensemble_report()\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"FUNCTIONS: Threshold Selection\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"select_threshold_by_ebic(Phi, y, thresholds=None, gamma=0.5) -> Dict\")\n",
    "print(\"  EBIC = n*log(MSE) + k*log(n) + 2*gamma*k*log(p)\")\n",
    "print()\n",
    "print(\"select_threshold_by_cv(Phi, y, thresholds=None, n_folds=5) -> Dict\")\n",
    "print(\"  Cross-validation for threshold selection\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"USAGE EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "# Layer 1: Basic STLSQ with relative threshold\n",
    "stlsq = EWSINDySTLSQ(threshold=0.1, relative_threshold=True)\n",
    "result = stlsq.fit(Phi, y, library_names=names)\n",
    "\n",
    "# Layer 2: Ensemble selection for robust UQ\n",
    "ensemble = ESINDyEnsemble(n_models=100, inclusion_threshold=0.9)\n",
    "result = ensemble.fit(Phi, y, library_names=names)\n",
    "print(f\"High-confidence terms: {ensemble.get_high_confidence_terms()}\")\n",
    "\n",
    "# Layer 3: Automatic threshold selection\n",
    "ebic_result = select_threshold_by_ebic(Phi, y, verbose=True)\n",
    "best_threshold = ebic_result['best_threshold']\n",
    "\"\"\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"References:\")\n",
    "print(\"  - Messenger & Bortz (2021). Weak SINDy. JCP 443.\")\n",
    "print(\"  - Fasel et al. (2022). Ensemble-SINDy. Proc. R. Soc. A.\")\n",
    "print(\"  - Chen & Chen (2008). Extended BIC. Biometrika 95(3).\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Module loaded successfully. Import via: %run 07_EWSINDy_STLSQ.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
