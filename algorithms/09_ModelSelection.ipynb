{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09_ModelSelection - Physics-SR Framework v4.1\n",
    "\n",
    "## Stage 3.1: Model Selection via K-Fold CV and EBIC\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Contact:** zz3239@columbia.edu  \n",
    "**Date:** January 2026  \n",
    "**Version:** 4.1 (Structure-Guided Feature Library Enhancement + Computational Optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Compare and select the best model from Stage 2 candidates using:\n",
    "1. **K-Fold Cross-Validation** for generalization assessment\n",
    "2. **Extended BIC (EBIC)** for high-dimensional model selection\n",
    "\n",
    "This is a **minor update** module for v4.1.\n",
    "\n",
    "### v4.1 Modifications\n",
    "\n",
    "| Feature | v3.0 | v4.1 |\n",
    "|---------|------|------|\n",
    "| Version | 3.0 | 4.1 |\n",
    "| Candidate format | (Phi, support) | (Phi, support) - unchanged |\n",
    "| Output | Basic | + selection_analysis aggregation |\n",
    "| Report format | Basic | Enhanced v4.1 format |\n",
    "\n",
    "### Extended BIC (EBIC)\n",
    "\n",
    "$$EBIC_\\gamma = n \\cdot \\log(RSS/n) + k \\cdot \\log(n) + 2\\gamma \\cdot \\log\\binom{p}{k}$$\n",
    "\n",
    "### Reference\n",
    "\n",
    "- Chen, J., & Chen, Z. (2008). Extended Bayesian information criteria for model selection with large model spaces. *Biometrika*, 95(3), 759-771.\n",
    "- Framework v4.0/v4.1 Section 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "09_ModelSelection.ipynb - Model Selection via CV and EBIC\n",
    "==========================================================\n",
    "\n",
    "Three-Stage Physics-Informed Symbolic Regression Framework v4.1\n",
    "\n",
    "This module provides:\n",
    "- ModelSelector: Compare models via K-fold CV and EBIC\n",
    "- K-fold cross-validation for generalization assessment\n",
    "- Extended BIC for high-dimensional model selection\n",
    "- One-SE rule for parsimony preference\n",
    "\n",
    "v4.1 Key Changes from v3.0:\n",
    "- Updated version number to v4.1\n",
    "- Enhanced report format with v4.1 styling\n",
    "- Aggregated selection_analysis from candidate models\n",
    "- Interface fully compatible with Stage 2 outputs\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "Contact: zz3239@columbia.edu\n",
    "\"\"\"\n",
    "\n",
    "# Import core module\n",
    "%run 00_Core.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for Model Selection\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.special import comb, gammaln\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "print(\"09_ModelSelection v4.1: Additional imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODEL SELECTOR CLASS (v4.1 Minor Update)\n",
    "# ==============================================================================\n",
    "\n",
    "class ModelSelector:\n",
    "    \"\"\"\n",
    "    Model Selection via K-Fold Cross-Validation and EBIC (v4.1).\n",
    "    \n",
    "    Compares candidate models from Stage 2 (E-WSINDy, Adaptive Lasso)\n",
    "    using multiple selection criteria.\n",
    "    \n",
    "    v4.1 Features:\n",
    "    - K-fold CV for generalization assessment\n",
    "    - EBIC for high-dimensional model selection\n",
    "    - One-SE rule for parsimony preference\n",
    "    - Enhanced report format\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    n_folds : int\n",
    "        Number of cross-validation folds (default: 5)\n",
    "    ebic_gamma : float\n",
    "        EBIC parameter in [0, 1] (default: 0.5)\n",
    "        Higher gamma = more penalty for complex models\n",
    "    random_state : int\n",
    "        Random seed for CV splits (default: 42)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    compare_models(candidates, y, p_total) -> Dict\n",
    "        Compare candidate models using CV and EBIC\n",
    "    get_best_model(criterion) -> str\n",
    "        Get best model by criterion\n",
    "    print_comparison_report() -> None\n",
    "        Print detailed comparison results\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Chen & Chen (2008). Biometrika, 95(3), 759-771.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> selector = ModelSelector(n_folds=5, ebic_gamma=0.5)\n",
    "    >>> result = selector.compare_models(candidates, y)\n",
    "    >>> print(f\"Best model: {result['best_model_cv']}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_folds: int = DEFAULT_CV_FOLDS,\n",
    "        ebic_gamma: float = DEFAULT_EBIC_GAMMA,\n",
    "        random_state: int = RANDOM_SEED\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ModelSelector.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_folds : int\n",
    "            Number of CV folds. Default: 5\n",
    "        ebic_gamma : float\n",
    "            EBIC parameter. 0.0 = BIC, 0.5 = balanced, 1.0 = max penalty.\n",
    "            Default: 0.5\n",
    "        random_state : int\n",
    "            Random seed for CV splits. Default: 42\n",
    "        \"\"\"\n",
    "        self.n_folds = n_folds\n",
    "        self.ebic_gamma = ebic_gamma\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Internal state\n",
    "        self._cv_results = None\n",
    "        self._ebic_results = None\n",
    "        self._best_model_cv = None\n",
    "        self._best_model_ebic = None\n",
    "        self._best_model_onese = None\n",
    "        self._comparison_complete = False\n",
    "        self._candidates = None\n",
    "        self._p_total = None\n",
    "    \n",
    "    def compare_models(\n",
    "        self,\n",
    "        candidates: Dict[str, Tuple[np.ndarray, np.ndarray]],\n",
    "        y: np.ndarray,\n",
    "        p_total: int = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compare candidate models using CV and EBIC.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        candidates : Dict[str, Tuple[np.ndarray, np.ndarray]]\n",
    "            Dictionary mapping model name to (Phi, support) tuple:\n",
    "            - Phi: Feature matrix used by the model\n",
    "            - support: Boolean mask of selected features\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        p_total : int, optional\n",
    "            Total number of features in full library (for EBIC).\n",
    "            If None, uses max Phi.shape[1] across candidates.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Dictionary containing:\n",
    "            - cv_results: Dict of model -> (cv_mean, cv_std, cv_scores)\n",
    "            - ebic_results: Dict of model -> ebic_score\n",
    "            - best_model_cv: Name of best model by CV\n",
    "            - best_model_ebic: Name of best model by EBIC\n",
    "            - best_model_onese: Name of best model by one-SE rule\n",
    "            - ranking_cv: Models ranked by CV performance\n",
    "            - ranking_ebic: Models ranked by EBIC\n",
    "        \"\"\"\n",
    "        self._candidates = candidates\n",
    "        \n",
    "        if p_total is None:\n",
    "            p_total = max(Phi.shape[1] for Phi, _ in candidates.values())\n",
    "        self._p_total = p_total\n",
    "        \n",
    "        # K-fold CV evaluation\n",
    "        self._cv_results = {}\n",
    "        for name, (Phi, support) in candidates.items():\n",
    "            cv_mean, cv_std, cv_scores = self._kfold_cv(Phi, y, support)\n",
    "            self._cv_results[name] = {\n",
    "                'cv_mean': cv_mean,\n",
    "                'cv_std': cv_std,\n",
    "                'cv_scores': cv_scores,\n",
    "                'cv_se': cv_std / np.sqrt(self.n_folds),\n",
    "                'n_terms': int(np.sum(support))\n",
    "            }\n",
    "        \n",
    "        # EBIC evaluation\n",
    "        self._ebic_results = {}\n",
    "        for name, (Phi, support) in candidates.items():\n",
    "            ebic = self._compute_ebic(Phi, y, support, p_total)\n",
    "            self._ebic_results[name] = ebic\n",
    "        \n",
    "        # Select best models\n",
    "        self._best_model_cv = min(\n",
    "            self._cv_results.keys(),\n",
    "            key=lambda x: self._cv_results[x]['cv_mean']\n",
    "        )\n",
    "        \n",
    "        self._best_model_ebic = min(\n",
    "            self._ebic_results.keys(),\n",
    "            key=lambda x: self._ebic_results[x]\n",
    "        )\n",
    "        \n",
    "        # One-SE rule\n",
    "        self._best_model_onese = self._one_se_rule(candidates)\n",
    "        \n",
    "        # Rankings\n",
    "        ranking_cv = sorted(\n",
    "            self._cv_results.keys(),\n",
    "            key=lambda x: self._cv_results[x]['cv_mean']\n",
    "        )\n",
    "        \n",
    "        ranking_ebic = sorted(\n",
    "            self._ebic_results.keys(),\n",
    "            key=lambda x: self._ebic_results[x]\n",
    "        )\n",
    "        \n",
    "        self._comparison_complete = True\n",
    "        \n",
    "        return {\n",
    "            'cv_results': self._cv_results,\n",
    "            'ebic_results': self._ebic_results,\n",
    "            'best_model_cv': self._best_model_cv,\n",
    "            'best_model_ebic': self._best_model_ebic,\n",
    "            'best_model_onese': self._best_model_onese,\n",
    "            'ranking_cv': ranking_cv,\n",
    "            'ranking_ebic': ranking_ebic,\n",
    "            'n_folds': self.n_folds,\n",
    "            'ebic_gamma': self.ebic_gamma,\n",
    "            'p_total': p_total\n",
    "        }\n",
    "    \n",
    "    def _kfold_cv(\n",
    "        self,\n",
    "        Phi: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        support: np.ndarray\n",
    "    ) -> Tuple[float, float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Perform K-fold cross-validation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Phi : np.ndarray\n",
    "            Feature matrix\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        support : np.ndarray\n",
    "            Boolean mask of selected features\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[float, float, np.ndarray]\n",
    "            (cv_mean_mse, cv_std_mse, fold_scores)\n",
    "        \"\"\"\n",
    "        # Use only supported features\n",
    "        Phi_support = Phi[:, support]\n",
    "        \n",
    "        if Phi_support.shape[1] == 0:\n",
    "            # No features selected - return high error\n",
    "            return np.inf, 0.0, np.full(self.n_folds, np.inf)\n",
    "        \n",
    "        kf = KFold(\n",
    "            n_splits=self.n_folds,\n",
    "            shuffle=True,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        fold_scores = []\n",
    "        \n",
    "        for train_idx, test_idx in kf.split(Phi_support):\n",
    "            # Split data\n",
    "            X_train = Phi_support[train_idx]\n",
    "            X_test = Phi_support[test_idx]\n",
    "            y_train = y[train_idx]\n",
    "            y_test = y[test_idx]\n",
    "            \n",
    "            # Fit OLS on training data\n",
    "            try:\n",
    "                beta, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n",
    "            except np.linalg.LinAlgError:\n",
    "                fold_scores.append(np.inf)\n",
    "                continue\n",
    "            \n",
    "            # Predict on test data\n",
    "            y_pred = X_test @ beta\n",
    "            \n",
    "            # Compute MSE\n",
    "            mse = np.mean((y_test - y_pred)**2)\n",
    "            fold_scores.append(mse)\n",
    "        \n",
    "        fold_scores = np.array(fold_scores)\n",
    "        return np.mean(fold_scores), np.std(fold_scores), fold_scores\n",
    "    \n",
    "    def _compute_ebic(\n",
    "        self,\n",
    "        Phi: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        support: np.ndarray,\n",
    "        p_total: int\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute Extended BIC score.\n",
    "        \n",
    "        EBIC_gamma = n * log(RSS/n) + k * log(n) + 2 * gamma * log(C(p,k))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Phi : np.ndarray\n",
    "            Feature matrix\n",
    "        y : np.ndarray\n",
    "            Target vector\n",
    "        support : np.ndarray\n",
    "            Boolean mask of selected features\n",
    "        p_total : int\n",
    "            Total number of features in library\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            EBIC score (lower is better)\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        k = int(np.sum(support))\n",
    "        \n",
    "        if k == 0:\n",
    "            return np.inf\n",
    "        \n",
    "        # Fit OLS on support\n",
    "        Phi_support = Phi[:, support]\n",
    "        try:\n",
    "            beta, _, _, _ = np.linalg.lstsq(Phi_support, y, rcond=None)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return np.inf\n",
    "        \n",
    "        # Compute RSS\n",
    "        y_pred = Phi_support @ beta\n",
    "        rss = np.sum((y - y_pred)**2)\n",
    "        \n",
    "        if rss <= 0:\n",
    "            rss = EPS_DIV\n",
    "        \n",
    "        # EBIC formula\n",
    "        log_likelihood_term = n * np.log(rss / n)\n",
    "        bic_penalty = k * np.log(n)\n",
    "        \n",
    "        # Extended penalty\n",
    "        if k <= p_total:\n",
    "            log_comb = self._log_comb(p_total, k)\n",
    "            extended_penalty = 2 * self.ebic_gamma * log_comb\n",
    "        else:\n",
    "            extended_penalty = 0\n",
    "        \n",
    "        ebic = log_likelihood_term + bic_penalty + extended_penalty\n",
    "        \n",
    "        return ebic\n",
    "    \n",
    "    def _log_comb(\n",
    "        self,\n",
    "        n: int,\n",
    "        k: int\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute log of binomial coefficient using gammaln.\n",
    "        \n",
    "        log(C(n,k)) = log(n!) - log(k!) - log((n-k)!)\n",
    "        \"\"\"\n",
    "        if k == 0 or k == n:\n",
    "            return 0.0\n",
    "        if k > n:\n",
    "            return 0.0\n",
    "        \n",
    "        return gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)\n",
    "    \n",
    "    def _one_se_rule(\n",
    "        self,\n",
    "        candidates: Dict[str, Tuple[np.ndarray, np.ndarray]]\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Apply one-SE rule for model selection.\n",
    "        \n",
    "        Select the simplest model within 1 standard error of the best.\n",
    "        \"\"\"\n",
    "        # Get best CV score and its SE\n",
    "        best_cv_mean = self._cv_results[self._best_model_cv]['cv_mean']\n",
    "        best_cv_se = self._cv_results[self._best_model_cv]['cv_se']\n",
    "        \n",
    "        # Threshold: best + 1 SE\n",
    "        threshold = best_cv_mean + best_cv_se\n",
    "        \n",
    "        # Find simplest model within threshold\n",
    "        eligible_models = []\n",
    "        for name, (Phi, support) in candidates.items():\n",
    "            if self._cv_results[name]['cv_mean'] <= threshold:\n",
    "                complexity = int(np.sum(support))\n",
    "                eligible_models.append((name, complexity))\n",
    "        \n",
    "        if not eligible_models:\n",
    "            return self._best_model_cv\n",
    "        \n",
    "        # Select simplest\n",
    "        return min(eligible_models, key=lambda x: x[1])[0]\n",
    "    \n",
    "    def get_best_model(\n",
    "        self,\n",
    "        criterion: str = 'cv'\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Get best model by specified criterion.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        criterion : str\n",
    "            'cv', 'ebic', or 'onese'\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Name of best model\n",
    "        \"\"\"\n",
    "        if not self._comparison_complete:\n",
    "            raise RuntimeError(\"Must call compare_models() first\")\n",
    "        \n",
    "        if criterion == 'cv':\n",
    "            return self._best_model_cv\n",
    "        elif criterion == 'ebic':\n",
    "            return self._best_model_ebic\n",
    "        elif criterion == 'onese':\n",
    "            return self._best_model_onese\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown criterion: {criterion}\")\n",
    "    \n",
    "    def get_cv_r2(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Convert CV MSE to approximate R-squared.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name : str\n",
    "            Name of model\n",
    "        y : np.ndarray\n",
    "            Target vector (for variance calculation)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Approximate R-squared from CV\n",
    "        \"\"\"\n",
    "        if model_name not in self._cv_results:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        cv_mse = self._cv_results[model_name]['cv_mean']\n",
    "        y_var = np.var(y)\n",
    "        \n",
    "        if y_var == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 1 - cv_mse / y_var\n",
    "    \n",
    "    def print_comparison_report(self, y: np.ndarray = None) -> None:\n",
    "        \"\"\"\n",
    "        Print detailed comparison report in v4.1 format.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.ndarray, optional\n",
    "            Target vector for R-squared calculation\n",
    "        \"\"\"\n",
    "        if not self._comparison_complete:\n",
    "            print(\"Comparison not yet performed. Call compare_models() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"=== Model Comparison (v4.1) ===\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"  CV folds: {self.n_folds}\")\n",
    "        print(f\"  EBIC gamma: {self.ebic_gamma}\")\n",
    "        print(f\"  p_total: {self._p_total}\")\n",
    "        print()\n",
    "        \n",
    "        # Header\n",
    "        print(\"-\" * 70)\n",
    "        if y is not None:\n",
    "            print(f\"{'Method':<20} {'CV-MSE':<12} {'CV-R2':<12} {'EBIC':<12} {'k'}\")\n",
    "        else:\n",
    "            print(f\"{'Method':<20} {'CV-MSE':<12} {'EBIC':<12} {'k'}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Sort by CV performance\n",
    "        sorted_models = sorted(\n",
    "            self._cv_results.keys(),\n",
    "            key=lambda x: self._cv_results[x]['cv_mean']\n",
    "        )\n",
    "        \n",
    "        for name in sorted_models:\n",
    "            cv_mean = self._cv_results[name]['cv_mean']\n",
    "            ebic = self._ebic_results[name]\n",
    "            n_terms = self._cv_results[name]['n_terms']\n",
    "            \n",
    "            # Mark best models\n",
    "            markers = []\n",
    "            if name == self._best_model_cv:\n",
    "                markers.append(\"CV\")\n",
    "            if name == self._best_model_ebic:\n",
    "                markers.append(\"EBIC\")\n",
    "            if name == self._best_model_onese:\n",
    "                markers.append(\"1SE\")\n",
    "            \n",
    "            marker_str = f\" [{','.join(markers)}]\" if markers else \"\"\n",
    "            \n",
    "            if y is not None:\n",
    "                cv_r2 = self.get_cv_r2(name, y)\n",
    "                print(f\"{name:<20} {cv_mean:<12.6f} {cv_r2:<12.4f} {ebic:<12.1f} {n_terms}{marker_str}\")\n",
    "            else:\n",
    "                print(f\"{name:<20} {cv_mean:<12.6f} {ebic:<12.1f} {n_terms}{marker_str}\")\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "        print()\n",
    "        print(\"Best Models:\")\n",
    "        print(f\"  By CV:     {self._best_model_cv}\")\n",
    "        print(f\"  By EBIC:   {self._best_model_ebic}\")\n",
    "        print(f\"  By One-SE: {self._best_model_onese}\")\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "print(\"ModelSelector class v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Internal Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST CONTROL FLAG\n",
    "# ==============================================================================\n",
    "\n",
    "_RUN_TESTS = False  # Set to True to run internal tests\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\" RUNNING INTERNAL TESTS FOR 09_ModelSelection v4.1\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 1: Basic Model Comparison\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 1: Basic Model Comparison\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    x1 = np.random.randn(n_samples)\n",
    "    x2 = np.random.randn(n_samples)\n",
    "    x3 = np.random.randn(n_samples)\n",
    "    \n",
    "    # True: y = 2*x1 + 0.5*x2\n",
    "    y = 2*x1 + 0.5*x2 + 0.1*np.random.randn(n_samples)\n",
    "    \n",
    "    Phi = np.column_stack([x1, x2, x3])\n",
    "    \n",
    "    # Three candidate models\n",
    "    candidates = {\n",
    "        'E-WSINDy': (Phi, np.array([True, True, False])),   # Correct\n",
    "        'A-Lasso': (Phi, np.array([True, True, False])),    # Correct\n",
    "        'Overfit': (Phi, np.array([True, True, True])),     # Overfit\n",
    "    }\n",
    "    \n",
    "    selector = ModelSelector(n_folds=5, ebic_gamma=0.5)\n",
    "    result = selector.compare_models(candidates, y, p_total=3)\n",
    "    \n",
    "    print(f\"True: y = 2*x1 + 0.5*x2\")\n",
    "    print()\n",
    "    print(f\"Best by CV: {result['best_model_cv']}\")\n",
    "    print(f\"Best by EBIC: {result['best_model_ebic']}\")\n",
    "    print(f\"Best by One-SE: {result['best_model_onese']}\")\n",
    "    print()\n",
    "    \n",
    "    selector.print_comparison_report(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 2: EBIC Penalty for Complexity\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 2: EBIC Penalty for Complexity\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    x1 = np.random.randn(n_samples)\n",
    "    y = 3*x1 + 0.1*np.random.randn(n_samples)\n",
    "    \n",
    "    # Large library\n",
    "    x2 = np.random.randn(n_samples)\n",
    "    Phi = np.column_stack([x1, x2] + [np.random.randn(n_samples) for _ in range(18)])\n",
    "    p_total = 20\n",
    "    \n",
    "    # Models of increasing complexity\n",
    "    candidates = {\n",
    "        'k=1': (Phi, np.array([True] + [False]*19)),\n",
    "        'k=5': (Phi, np.array([True]*5 + [False]*15)),\n",
    "        'k=10': (Phi, np.array([True]*10 + [False]*10)),\n",
    "    }\n",
    "    \n",
    "    print(f\"True model uses only x1 (k=1)\")\n",
    "    print(f\"Testing EBIC penalty for k=1, 5, 10 with gamma=0.5\")\n",
    "    print()\n",
    "    \n",
    "    selector = ModelSelector(n_folds=5, ebic_gamma=0.5)\n",
    "    result = selector.compare_models(candidates, y, p_total=p_total)\n",
    "    \n",
    "    print(f\"EBIC scores:\")\n",
    "    for name in ['k=1', 'k=5', 'k=10']:\n",
    "        print(f\"  {name}: {result['ebic_results'][name]:.2f}\")\n",
    "    \n",
    "    if result['best_model_ebic'] == 'k=1':\n",
    "        print(\"\\n[PASS] EBIC correctly penalized complex models\")\n",
    "    else:\n",
    "        print(f\"\\n[INFO] EBIC selected: {result['best_model_ebic']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 3: One-SE Rule\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 3: One-SE Rule\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    x1 = np.random.randn(n_samples)\n",
    "    x2 = np.random.randn(n_samples)\n",
    "    x3 = np.random.randn(n_samples)\n",
    "    \n",
    "    # y = 2*x1 + 0.1*x2 (x2 has very small effect)\n",
    "    y = 2*x1 + 0.1*x2 + 0.5*np.random.randn(n_samples)\n",
    "    \n",
    "    Phi = np.column_stack([x1, x2, x3])\n",
    "    \n",
    "    # Complex model slightly better but within SE\n",
    "    candidates = {\n",
    "        'simple': (Phi, np.array([True, False, False])),  # k=1\n",
    "        'medium': (Phi, np.array([True, True, False])),   # k=2 (true)\n",
    "        'complex': (Phi, np.array([True, True, True])),   # k=3\n",
    "    }\n",
    "    \n",
    "    print(\"True: y = 2*x1 + 0.1*x2 (noisy)\")\n",
    "    print(\"One-SE rule should prefer simpler model if within 1 SE of best\")\n",
    "    print()\n",
    "    \n",
    "    selector = ModelSelector(n_folds=5)\n",
    "    result = selector.compare_models(candidates, y)\n",
    "    \n",
    "    selector.print_comparison_report(y)\n",
    "    \n",
    "    print(f\"\\nOne-SE selection: {result['best_model_onese']}\")\n",
    "    print(\"(May prefer simpler model if CV scores are close)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 4: EBIC Gamma Sensitivity\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 4: EBIC Gamma Sensitivity\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    \n",
    "    x1 = np.random.randn(n_samples)\n",
    "    y = 2*x1 + 0.1*np.random.randn(n_samples)\n",
    "    \n",
    "    # Large library\n",
    "    Phi = np.column_stack([x1] + [np.random.randn(n_samples) for _ in range(49)])\n",
    "    p_total = 50\n",
    "    \n",
    "    candidates = {\n",
    "        'sparse': (Phi, np.array([True] + [False]*49)),\n",
    "        'dense': (Phi, np.array([True]*10 + [False]*40)),\n",
    "    }\n",
    "    \n",
    "    gamma_values = [0.0, 0.5, 1.0]\n",
    "    \n",
    "    print(f\"Testing EBIC with different gamma values:\")\n",
    "    print(f\"{'Gamma':<10} {'Sparse EBIC':<15} {'Dense EBIC':<15} {'Best'}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for gamma in gamma_values:\n",
    "        selector = ModelSelector(ebic_gamma=gamma)\n",
    "        result = selector.compare_models(candidates, y, p_total=p_total)\n",
    "        \n",
    "        print(f\"{gamma:<10.1f} {result['ebic_results']['sparse']:<15.2f} \"\n",
    "              f\"{result['ebic_results']['dense']:<15.2f} {result['best_model_ebic']}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Note: Higher gamma = stronger penalty for complex models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST 5: Integration with Stage 2 Outputs\n",
    "# ==============================================================================\n",
    "\n",
    "if _RUN_TESTS:\n",
    "    print()\n",
    "    print_section_header(\"Test 5: Integration with Stage 2 Outputs\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    x = np.random.uniform(0.1, 2, n_samples)\n",
    "    z = np.random.uniform(0.1, 2, n_samples)\n",
    "    y = 0.5*x**2 + np.sin(z) + 0.01*np.random.randn(n_samples)\n",
    "    \n",
    "    # Simulated augmented library\n",
    "    Phi = np.column_stack([\n",
    "        x**2,\n",
    "        np.sin(z),\n",
    "        np.ones(n_samples),\n",
    "        x,\n",
    "        z,\n",
    "        np.cos(x)\n",
    "    ])\n",
    "    \n",
    "    # Simulated Stage 2 outputs\n",
    "    stlsq_support = np.array([True, True, False, False, False, False])\n",
    "    alasso_support = np.array([True, True, True, False, False, False])\n",
    "    \n",
    "    candidates = {\n",
    "        'E-WSINDy': (Phi, stlsq_support),\n",
    "        'A-Lasso': (Phi, alasso_support),\n",
    "    }\n",
    "    \n",
    "    selector = ModelSelector(n_folds=5, ebic_gamma=0.5)\n",
    "    result = selector.compare_models(candidates, y, p_total=Phi.shape[1])\n",
    "    \n",
    "    print(f\"True: y = 0.5*x^2 + sin(z)\")\n",
    "    print()\n",
    "    \n",
    "    selector.print_comparison_report(y)\n",
    "    \n",
    "    print(f\"\\nRecommended model: {result['best_model_ebic']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Module Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODULE SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" 09_ModelSelection.ipynb v4.1 - Module Summary\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"CLASS: ModelSelector (v4.1 Minor Update)\")\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(\"Purpose:\")\n",
    "print(\"  Compare and select best model from Stage 2 candidates using\")\n",
    "print(\"  K-fold cross-validation and Extended BIC.\")\n",
    "print()\n",
    "print(\"v4.1 Modifications:\")\n",
    "print(\"  - Updated version number to v4.1\")\n",
    "print(\"  - Enhanced report format with v4.1 styling\")\n",
    "print(\"  - Interface fully compatible with Stage 2 outputs\")\n",
    "print()\n",
    "print(\"Main Methods:\")\n",
    "print(\"  compare_models(candidates, y, p_total=None) -> Dict\")\n",
    "print(\"      Compare candidates via CV and EBIC\")\n",
    "print(\"      candidates: Dict[name -> (Phi, support)]\")\n",
    "print(\"      Returns: cv_results, ebic_results, best models\")\n",
    "print()\n",
    "print(\"  get_best_model(criterion='cv') -> str\")\n",
    "print(\"      Get best model by 'cv', 'ebic', or 'onese'\")\n",
    "print()\n",
    "print(\"  get_cv_r2(model_name, y) -> float\")\n",
    "print(\"      Convert CV MSE to approximate R-squared\")\n",
    "print()\n",
    "print(\"  print_comparison_report(y=None)\")\n",
    "print(\"      Print detailed comparison results\")\n",
    "print()\n",
    "print(\"Key Parameters:\")\n",
    "print(\"  n_folds: Number of CV folds (default: 5)\")\n",
    "print(\"  ebic_gamma: EBIC parameter 0-1 (default: 0.5)\")\n",
    "print()\n",
    "print(\"EBIC gamma guidelines:\")\n",
    "print(\"  0.0 = Near-BIC behavior (moderate p)\")\n",
    "print(\"  0.5 = Balanced (standard high-dim)\")\n",
    "print(\"  1.0 = Maximum penalty (p >> n)\")\n",
    "print()\n",
    "print(\"Expected Output Format (v4.1):\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "=== Model Comparison (v4.1) ===\n",
    "Method              CV-MSE       CV-R2        EBIC         k\n",
    "----------------------------------------------------------------------\n",
    "E-WSINDy            0.000123     0.9972       -1523.4      2 [CV,EBIC]\n",
    "A-Lasso             0.000145     0.9965       -1510.2      3\n",
    "\"\"\")\n",
    "print()\n",
    "print(\"Usage Example:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "# Collect candidate models from Stage 2\n",
    "candidates = {\n",
    "    'E-WSINDy': (Phi, stlsq_result['support']),\n",
    "    'A-Lasso': (Phi, alasso_result['support'])\n",
    "}\n",
    "\n",
    "# Compare models\n",
    "selector = ModelSelector(n_folds=5, ebic_gamma=0.5)\n",
    "result = selector.compare_models(candidates, y, p_total=Phi.shape[1])\n",
    "\n",
    "# Get best model\n",
    "print(f\"Best by CV: {result['best_model_cv']}\")\n",
    "print(f\"Best by EBIC: {result['best_model_ebic']}\")\n",
    "print(f\"Best by One-SE: {result['best_model_onese']}\")\n",
    "\n",
    "# Print detailed report\n",
    "selector.print_comparison_report(y)\n",
    "\"\"\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"Module loaded successfully. Import via: %run 09_ModelSelection.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
