{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments - Physics-SR Framework v4.1 Benchmark\n",
    "\n",
    "## Benchmark Experiment Execution Module\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Contact:** zz3239@columbia.edu  \n",
    "**Date:** January 2026  \n",
    "**Version:** 4.1 (Structure-Guided Feature Library + Computational Optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook executes all benchmark experiments for the Physics-SR Framework v4.1.\n",
    "\n",
    "### Experimental Design\n",
    "\n",
    "**Core Experiments (96 runs):**\n",
    "- 4 equations x 2 noise (0%, 5%) x 2 dummy (0, 5) x 2 dims (T/F) x 3 methods = 96\n",
    "\n",
    "**Supplementary Experiments (8 runs):**\n",
    "- 4 equations x 2 sample sizes (250, 750) x Physics-SR only = 8\n",
    "\n",
    "**Total: 104 experiments**\n",
    "\n",
    "### Test Equations (AI Feynman Benchmark)\n",
    "\n",
    "| # | Name | AI Feynman ID | Type |\n",
    "|---|------|---------------|------|\n",
    "| 1 | Coulomb | I.12.2 | Rational |\n",
    "| 2 | Cosines | I.29.16 | Nested trigonometric |\n",
    "| 3 | Barometric | I.40.1 | Exponential |\n",
    "| 4 | DotProduct | I.11.19 | Polynomial interaction |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENVIRONMENT RESET AND FRESH CLONE\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    import shutil\n",
    "    import gc\n",
    "\n",
    "    try:\n",
    "        os.chdir('/content')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    repo_path = '/content/Physics-Informed-Symbolic-Regression'\n",
    "    if os.path.exists(repo_path):\n",
    "        shutil.rmtree(repo_path)\n",
    "        print(\"[OK] Removed existing repository.\")\n",
    "\n",
    "    !git clone https://github.com/Garthzzz/Physics-Informed-Symbolic-Regression.git\n",
    "\n",
    "    if os.path.exists(repo_path):\n",
    "        print(\"[OK] Fresh repository cloned.\")\n",
    "        os.chdir(repo_path + '/benchmark')\n",
    "        print(f\"[OK] Working directory: {os.getcwd()}\")\n",
    "        !git log --oneline -3\n",
    "    else:\n",
    "        print(\"[FAIL] Clone failed!\")\n",
    "\n",
    "    print(\"\\n[OK] Environment reset complete.\")\n",
    "else:\n",
    "    print(\"[INFO] Not in Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# COLAB SETUP - PySR Installation\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    if not os.path.exists('/content/Physics-Informed-Symbolic-Regression'):\n",
    "        !git clone https://github.com/Garthzzz/Physics-Informed-Symbolic-Regression.git\n",
    "        print(\"Repository cloned!\")\n",
    "\n",
    "    %cd /content/Physics-Informed-Symbolic-Regression\n",
    "\n",
    "    !pip install -q pysr\n",
    "    import pysr\n",
    "    pysr.install()\n",
    "\n",
    "    from pathlib import Path\n",
    "    data_files = list(Path('benchmark/data').glob('*.npz'))\n",
    "    print(f\"\\nFound {len(data_files)} data files\")\n",
    "\n",
    "    if len(data_files) == 24:\n",
    "        print(\"[OK] All data files present!\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Expected 24 files, found {len(data_files)}\")\n",
    "\n",
    "    print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiments.ipynb - Benchmark Experiment Execution Module\n",
    "Physics-SR Framework v4.1 Benchmark Suite\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Contact: zz3239@columbia.edu\n",
    "Version: 4.1\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV, Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Experiments v4.1: All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    BASE_DIR = Path('/content/Physics-Informed-Symbolic-Regression')\n",
    "    ALGORITHMS_DIR = BASE_DIR / 'algorithms'\n",
    "    BENCHMARK_DIR = BASE_DIR / 'benchmark'\n",
    "else:\n",
    "    BENCHMARK_DIR = Path('.').resolve()\n",
    "    ALGORITHMS_DIR = BENCHMARK_DIR.parent / 'algorithms'\n",
    "    BASE_DIR = BENCHMARK_DIR.parent\n",
    "\n",
    "DATA_DIR = BENCHMARK_DIR / 'data'\n",
    "RESULTS_DIR = BENCHMARK_DIR / 'results'\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Algorithms directory: {ALGORITHMS_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "\n",
    "data_files = list(DATA_DIR.glob('*.npz'))\n",
    "print(f\"Found {len(data_files)} data files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPERIMENT CONFIGURATION CONSTANTS (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "EXPERIMENT_SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "METHODS = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "\n",
    "PYSR_CONFIG = {\n",
    "    'niterations': 30,\n",
    "    'maxsize': 18,\n",
    "    'timeout_in_seconds': 60,\n",
    "    'populations': 12,\n",
    "}\n",
    "\n",
    "PHYSICS_SR_CONFIG = {\n",
    "    'screening_threshold': 0.8,\n",
    "    'power_law_r2_threshold': 0.9,\n",
    "    'interaction_stability': 0.5,\n",
    "    'max_poly_degree': 3,\n",
    "    'stlsq_threshold': 0.1,\n",
    "    'pysr_maxsize': 18,\n",
    "    'pysr_niterations': 30,\n",
    "    'pysr_timeout': 60,\n",
    "    'pysr_mode': 'fast',\n",
    "    'generate_variants': True,\n",
    "    'use_weak_form': True,\n",
    "    'cv_folds': 5,\n",
    "    'ebic_gamma': 0.5,\n",
    "    'n_bootstrap': 50,\n",
    "    'confidence_level': 0.95\n",
    "}\n",
    "\n",
    "LASSO_CONFIG = {\n",
    "    'cv': 5,\n",
    "    'max_iter': 10000,\n",
    "}\n",
    "\n",
    "EQUATION_INDEX_MAP = {\n",
    "    'coulomb': 1,\n",
    "    'cosines': 2,\n",
    "    'barometric': 3,\n",
    "    'dotproduct': 4\n",
    "}\n",
    "\n",
    "GROUND_TRUTH_REGISTRY = {\n",
    "    'coulomb': {\n",
    "        'ai_feynman_id': 'I.12.2',\n",
    "        'equation': 'F = k * q1 * q2 / r**2',\n",
    "        'active_features': ['q1', 'q2', 'r'],\n",
    "        'type': 'rational'\n",
    "    },\n",
    "    'cosines': {\n",
    "        'ai_feynman_id': 'I.29.16',\n",
    "        'equation': 'x = sqrt(x1**2 + x2**2 - 2*x1*x2*cos(theta1-theta2))',\n",
    "        'active_features': ['x1', 'x2', 'theta1', 'theta2'],\n",
    "        'type': 'nested_trigonometric'\n",
    "    },\n",
    "    'barometric': {\n",
    "        'ai_feynman_id': 'I.40.1',\n",
    "        'equation': 'n = n0 * exp(-m*g*x/(k_b*T))',\n",
    "        'active_features': ['n0', 'm', 'g', 'x', 'k_b', 'T'],\n",
    "        'type': 'exponential'\n",
    "    },\n",
    "    'dotproduct': {\n",
    "        'ai_feynman_id': 'I.11.19',\n",
    "        'equation': 'A = x1*y1 + x2*y2 + x3*y3',\n",
    "        'active_features': ['x1', 'x2', 'x3', 'y1', 'y2', 'y3'],\n",
    "        'type': 'polynomial_interaction'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Experiment configuration v4.1 loaded.\")\n",
    "print(f\"Methods: {METHODS}\")\n",
    "print(f\"Equations: {list(EQUATION_INDEX_MAP.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPERIMENT CONFIGURATION GENERATORS (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "def get_core_experiment_configs() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate configurations for core experiments.\n",
    "    Core: 4 equations x 2 noise x 2 dummy x 2 dims = 32 configurations\n",
    "    With 3 methods each = 96 total experiments\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    equations = ['coulomb', 'cosines', 'barometric', 'dotproduct']\n",
    "    noise_levels = [0.0, 0.05]\n",
    "    dummy_counts = [0, 5]\n",
    "    with_dims_options = [True, False]\n",
    "    n_samples = 500\n",
    "    \n",
    "    for eq_name in equations:\n",
    "        for noise in noise_levels:\n",
    "            for n_dummy in dummy_counts:\n",
    "                for with_dims in with_dims_options:\n",
    "                    configs.append({\n",
    "                        'equation_name': eq_name,\n",
    "                        'n_samples': n_samples,\n",
    "                        'noise_level': noise,\n",
    "                        'n_dummy': n_dummy,\n",
    "                        'with_dims': with_dims\n",
    "                    })\n",
    "    return configs\n",
    "\n",
    "\n",
    "def get_supplementary_experiment_configs() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate configurations for supplementary experiments.\n",
    "    Supplementary: 4 equations x 2 sample sizes (250, 750) = 8 configurations\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    equations = ['coulomb', 'cosines', 'barometric', 'dotproduct']\n",
    "    sample_sizes = [250, 750]\n",
    "    \n",
    "    for eq_name in equations:\n",
    "        for n_samples in sample_sizes:\n",
    "            configs.append({\n",
    "                'equation_name': eq_name,\n",
    "                'n_samples': n_samples,\n",
    "                'noise_level': 0.05,\n",
    "                'n_dummy': 5,\n",
    "                'with_dims': True\n",
    "            })\n",
    "    return configs\n",
    "\n",
    "\n",
    "print(f\"Core experiment configs: {len(get_core_experiment_configs())}\")\n",
    "print(f\"Supplementary experiment configs: {len(get_supplementary_experiment_configs())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Algorithm Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD ALGORITHM MODULES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading algorithm modules...\")\n",
    "\n",
    "%cd {ALGORITHMS_DIR}\n",
    "\n",
    "%run 00_Core.ipynb\n",
    "%run 01_BuckinghamPi.ipynb\n",
    "%run 02_VariableScreening.ipynb\n",
    "%run 03_SymmetryAnalysis.ipynb\n",
    "%run 04_InteractionDiscovery.ipynb\n",
    "%run 05_FeatureLibrary.ipynb\n",
    "%run 06_PySR.ipynb\n",
    "%run 07_EWSINDy_STLSQ.ipynb\n",
    "%run 08_AdaptiveLasso.ipynb\n",
    "%run 09_ModelSelection.ipynb\n",
    "%run 10_PhysicsVerification.ipynb\n",
    "%run 11_UQ_Inference.ipynb\n",
    "%run 12_Full_Pipeline.ipynb\n",
    "\n",
    "%cd {BENCHMARK_DIR}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" All algorithm modules loaded successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD DATAGEN UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading DataGen utilities...\")\n",
    "%run DataGen.ipynb\n",
    "print(\"\\nDataGen utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Method Runners (v4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD RESULT DATACLASS (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class MethodResult:\n",
    "    \"\"\"\n",
    "    Standardized result container for all method runners (v4.1).\n",
    "    \n",
    "    v4.1 additions:\n",
    "    - stage2_results: Full Stage 2 results for library analysis\n",
    "    - pysr_model: Stored PySR model for test predictions (FIX)\n",
    "    \"\"\"\n",
    "    method_name: str\n",
    "    discovered_features: List[str]\n",
    "    equation: str\n",
    "    coefficients: Optional[np.ndarray]\n",
    "    support: Optional[np.ndarray]\n",
    "    predictions: np.ndarray\n",
    "    runtime_seconds: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "    stage2_results: Optional[Any] = None\n",
    "    pysr_model: Optional[Any] = None\n",
    "    method_specific: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "print(\"MethodResult dataclass v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHYSICS-SR RUNNER (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "class PhysicsSRRunner:\n",
    "    \"\"\"\n",
    "    Runner for the complete Physics-SR pipeline (v4.1).\n",
    "    v4.1: Stores pysr_model for accurate test predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    method_name = \"physics_sr\"\n",
    "\n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        self.config = config or PHYSICS_SR_CONFIG.copy()\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str],\n",
    "        user_inputs: Optional[UserInputs] = None,\n",
    "        with_dims: bool = True\n",
    "    ) -> MethodResult:\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Modify user_inputs if with_dims=False\n",
    "            if not with_dims and user_inputs is not None:\n",
    "                user_inputs_modified = UserInputs(\n",
    "                    variable_dimensions={name: [0, 0, 0, 0] for name in feature_names},\n",
    "                    target_dimensions=[0, 0, 0, 0],\n",
    "                    physical_bounds=user_inputs.physical_bounds\n",
    "                )\n",
    "            else:\n",
    "                user_inputs_modified = user_inputs\n",
    "\n",
    "            # Create and run pipeline\n",
    "            pipeline = PhysicsSRPipeline(config=self.config)\n",
    "            result = pipeline.run(X, y, feature_names, user_inputs_modified)\n",
    "\n",
    "            runtime = time.time() - start_time\n",
    "\n",
    "            # Extract results\n",
    "            discovered_features = self._extract_features(result)\n",
    "            equation = result.get('final_equation', 'Not discovered')\n",
    "            stage2_results = result.get('stage2', None)\n",
    "            \n",
    "            # v4.1 FIX: Get PySR model\n",
    "            pysr_model = None\n",
    "            if stage2_results is not None:\n",
    "                pysr_model = getattr(stage2_results, 'pysr_model', None)\n",
    "            \n",
    "            predictions = self._get_predictions(result, X, y, feature_names)\n",
    "            coefficients = self._extract_coefficients(result)\n",
    "            support = self._extract_support(result)\n",
    "\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=discovered_features,\n",
    "                equation=str(equation),\n",
    "                coefficients=coefficients,\n",
    "                support=support,\n",
    "                predictions=predictions,\n",
    "                runtime_seconds=runtime,\n",
    "                success=True,\n",
    "                error_message=None,\n",
    "                stage2_results=stage2_results,\n",
    "                pysr_model=pysr_model,\n",
    "                method_specific={\n",
    "                    'stage1': result.get('stage1', {}),\n",
    "                    'stage2': result.get('stage2', {}),\n",
    "                    'stage3': result.get('stage3', {}),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            runtime = time.time() - start_time\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=[],\n",
    "                equation='ERROR',\n",
    "                coefficients=None,\n",
    "                support=None,\n",
    "                predictions=np.zeros(len(y)),\n",
    "                runtime_seconds=runtime,\n",
    "                success=False,\n",
    "                error_message=str(e),\n",
    "                stage2_results=None,\n",
    "                pysr_model=None,\n",
    "                method_specific={}\n",
    "            )\n",
    "\n",
    "    def _extract_features(self, result: Dict) -> List[str]:\n",
    "        try:\n",
    "            stage1 = result.get('stage1', {})\n",
    "            if isinstance(stage1, dict):\n",
    "                screening = stage1.get('screening', {})\n",
    "                if isinstance(screening, dict):\n",
    "                    return screening.get('selected_features', [])\n",
    "            if hasattr(stage1, 'selected_names') and stage1.selected_names is not None:\n",
    "                return stage1.selected_names\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "    def _extract_coefficients(self, result: Dict) -> Optional[np.ndarray]:\n",
    "        try:\n",
    "            stage2 = result.get('stage2', {})\n",
    "            if hasattr(stage2, 'stlsq_coefficients'):\n",
    "                return stage2.stlsq_coefficients\n",
    "            if isinstance(stage2, dict):\n",
    "                return stage2.get('stlsq_coefficients', None)\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    def _extract_support(self, result: Dict) -> Optional[np.ndarray]:\n",
    "        try:\n",
    "            stage2 = result.get('stage2', {})\n",
    "            if hasattr(stage2, 'stlsq_support'):\n",
    "                return stage2.stlsq_support\n",
    "            if isinstance(stage2, dict):\n",
    "                return stage2.get('stlsq_support', None)\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    def _get_predictions(self, result: Dict, X: np.ndarray, y: np.ndarray, feature_names: List[str]) -> np.ndarray:\n",
    "        try:\n",
    "            stage3 = result.get('stage3', {})\n",
    "            if isinstance(stage3, dict):\n",
    "                best_model = stage3.get('best_model', {})\n",
    "                if isinstance(best_model, dict):\n",
    "                    preds = best_model.get('predictions', None)\n",
    "                    if preds is not None:\n",
    "                        return preds\n",
    "            \n",
    "            stage2 = result.get('stage2', None)\n",
    "            if stage2 is not None:\n",
    "                if hasattr(stage2, 'augmented_library') and hasattr(stage2, 'stlsq_coefficients'):\n",
    "                    Phi = stage2.augmented_library\n",
    "                    support = stage2.stlsq_support\n",
    "                    coef = stage2.stlsq_coefficients\n",
    "                    if Phi is not None and support is not None and coef is not None:\n",
    "                        Phi_selected = Phi[:, support]\n",
    "                        return Phi_selected @ coef[support]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        discovered = self._extract_features(result)\n",
    "        if len(discovered) > 0:\n",
    "            try:\n",
    "                indices = [feature_names.index(f) for f in discovered if f in feature_names]\n",
    "                if len(indices) > 0:\n",
    "                    model = Ridge(alpha=0.1)\n",
    "                    model.fit(X[:, indices], y)\n",
    "                    return model.predict(X[:, indices])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return np.zeros(X.shape[0])\n",
    "\n",
    "\n",
    "print(\"PhysicsSRRunner v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PYSR-ONLY RUNNER (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "class PySROnlyRunner:\n",
    "    \"\"\"\n",
    "    Runner for PySR-only baseline (v4.1).\n",
    "    v4.1 FIX: Store pysr_model for accurate test predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    method_name = \"pysr_only\"\n",
    "\n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        self.config = config or PYSR_CONFIG.copy()\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str],\n",
    "        user_inputs: Optional[UserInputs] = None,\n",
    "        with_dims: bool = True\n",
    "    ) -> MethodResult:\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            discoverer = PySRDiscoverer(\n",
    "                maxsize=self.config.get('maxsize', 18),\n",
    "                niterations=self.config.get('niterations', 30)\n",
    "            )\n",
    "\n",
    "            result = discoverer.discover(X, y, feature_names)\n",
    "\n",
    "            runtime = time.time() - start_time\n",
    "\n",
    "            equation = result.get('best_equation', 'Not discovered')\n",
    "            discovered_features = self._extract_features_from_equation(equation, feature_names)\n",
    "            predictions = result.get('predictions', np.zeros(len(y)))\n",
    "            pysr_model = discoverer._model\n",
    "\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=discovered_features,\n",
    "                equation=str(equation),\n",
    "                coefficients=None,\n",
    "                support=None,\n",
    "                predictions=predictions,\n",
    "                runtime_seconds=runtime,\n",
    "                success=True,\n",
    "                error_message=None,\n",
    "                stage2_results=None,\n",
    "                pysr_model=pysr_model,\n",
    "                method_specific={\n",
    "                    'all_equations': result.get('equations', []),\n",
    "                    'complexity': result.get('complexity', 0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            runtime = time.time() - start_time\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=[],\n",
    "                equation='ERROR',\n",
    "                coefficients=None,\n",
    "                support=None,\n",
    "                predictions=np.zeros(len(y)),\n",
    "                runtime_seconds=runtime,\n",
    "                success=False,\n",
    "                error_message=str(e),\n",
    "                stage2_results=None,\n",
    "                pysr_model=None,\n",
    "                method_specific={}\n",
    "            )\n",
    "\n",
    "    def _extract_features_from_equation(self, equation: str, feature_names: List[str]) -> List[str]:\n",
    "        discovered = []\n",
    "        equation_str = str(equation)\n",
    "        for name in feature_names:\n",
    "            if re.search(r'\\b' + re.escape(name) + r'\\b', equation_str):\n",
    "                discovered.append(name)\n",
    "        return list(set(discovered))\n",
    "\n",
    "\n",
    "print(\"PySROnlyRunner v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LASSO + PYSR RUNNER (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "class LASSOPySRRunner:\n",
    "    \"\"\"\n",
    "    Runner for LASSO + PySR baseline (v4.1).\n",
    "    v4.1 FIX: Store pysr_model for accurate test predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    method_name = \"lasso_pysr\"\n",
    "\n",
    "    def __init__(self, lasso_config: Optional[Dict] = None, pysr_config: Optional[Dict] = None):\n",
    "        self.lasso_config = lasso_config or LASSO_CONFIG.copy()\n",
    "        self.pysr_config = pysr_config or PYSR_CONFIG.copy()\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str],\n",
    "        user_inputs: Optional[UserInputs] = None,\n",
    "        with_dims: bool = True\n",
    "    ) -> MethodResult:\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Step 1: LASSO feature selection\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "            lasso = LassoCV(\n",
    "                cv=self.lasso_config.get('cv', 5),\n",
    "                max_iter=self.lasso_config.get('max_iter', 10000),\n",
    "                random_state=EXPERIMENT_SEED\n",
    "            )\n",
    "            lasso.fit(X_scaled, y)\n",
    "\n",
    "            selected_mask = np.abs(lasso.coef_) > 1e-10\n",
    "            selected_indices = np.where(selected_mask)[0]\n",
    "            selected_features = [feature_names[i] for i in selected_indices]\n",
    "\n",
    "            if len(selected_features) == 0:\n",
    "                selected_features = feature_names\n",
    "                X_selected = X\n",
    "            else:\n",
    "                X_selected = X[:, selected_indices]\n",
    "\n",
    "            # Step 2: PySR on selected features\n",
    "            discoverer = PySRDiscoverer(\n",
    "                maxsize=self.pysr_config.get('maxsize', 18),\n",
    "                niterations=self.pysr_config.get('niterations', 30)\n",
    "            )\n",
    "\n",
    "            pysr_result = discoverer.discover(X_selected, y, selected_features)\n",
    "\n",
    "            runtime = time.time() - start_time\n",
    "\n",
    "            equation = pysr_result.get('best_equation', 'Not discovered')\n",
    "            predictions = pysr_result.get('predictions', np.zeros(len(y)))\n",
    "            pysr_model = discoverer._model\n",
    "\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=selected_features,\n",
    "                equation=str(equation),\n",
    "                coefficients=None,\n",
    "                support=None,\n",
    "                predictions=predictions,\n",
    "                runtime_seconds=runtime,\n",
    "                success=True,\n",
    "                error_message=None,\n",
    "                stage2_results=None,\n",
    "                pysr_model=pysr_model,\n",
    "                method_specific={\n",
    "                    'lasso_alpha': lasso.alpha_,\n",
    "                    'lasso_coef': lasso.coef_.tolist(),\n",
    "                    'n_lasso_selected': len(selected_features),\n",
    "                    'selected_indices': selected_indices.tolist(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            runtime = time.time() - start_time\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=[],\n",
    "                equation='ERROR',\n",
    "                coefficients=None,\n",
    "                support=None,\n",
    "                predictions=np.zeros(len(y)),\n",
    "                runtime_seconds=runtime,\n",
    "                success=False,\n",
    "                error_message=str(e),\n",
    "                stage2_results=None,\n",
    "                pysr_model=None,\n",
    "                method_specific={}\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"LASSOPySRRunner v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD RUNNER REGISTRY\n",
    "# ==============================================================================\n",
    "\n",
    "METHOD_RUNNERS = {\n",
    "    'physics_sr': PhysicsSRRunner,\n",
    "    'pysr_only': PySROnlyRunner,\n",
    "    'lasso_pysr': LASSOPySRRunner,\n",
    "}\n",
    "\n",
    "def get_method_runner(method_name: str):\n",
    "    if method_name not in METHOD_RUNNERS:\n",
    "        raise ValueError(f\"Unknown method: {method_name}. Available: {list(METHOD_RUNNERS.keys())}\")\n",
    "    return METHOD_RUNNERS[method_name]()\n",
    "\n",
    "\n",
    "print(\"Method Runner Registry:\")\n",
    "for name in METHOD_RUNNERS:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_variable_selection(\n",
    "    discovered_features: List[str],\n",
    "    true_features: List[str],\n",
    "    all_features: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate variable selection performance.\"\"\"\n",
    "    true_set = set(true_features)\n",
    "    discovered_set = set(discovered_features)\n",
    "    all_set = set(all_features)\n",
    "\n",
    "    tp = len(discovered_set & true_set)\n",
    "    fp = len(discovered_set - true_set)\n",
    "    fn = len(true_set - discovered_set)\n",
    "    tn = len(all_set - discovered_set - true_set)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn,\n",
    "        'selected_correct': discovered_set == true_set,\n",
    "        'n_discovered': len(discovered_set), 'n_true': len(true_set),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_prediction(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate prediction performance.\"\"\"\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "    except:\n",
    "        r2 = -np.inf\n",
    "    try:\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    except:\n",
    "        rmse = np.inf\n",
    "    try:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "    except:\n",
    "        mae = np.inf\n",
    "    y_std = np.std(y_true)\n",
    "    nrmse = rmse / y_std if y_std > 0 else np.inf\n",
    "    return {'r2': r2, 'rmse': rmse, 'mae': mae, 'nrmse': nrmse}\n",
    "\n",
    "\n",
    "def evaluate_library_composition(stage2_results: Any) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"Evaluate augmented library composition (v4.1 NEW).\"\"\"\n",
    "    result = {\n",
    "        'library_n_total': None, 'library_n_pysr': None,\n",
    "        'library_n_variant': None, 'library_n_poly': None, 'library_n_op': None,\n",
    "        'selected_from_pysr': None, 'selected_from_variant': None,\n",
    "        'selected_from_poly': None, 'selected_from_op': None,\n",
    "    }\n",
    "    if stage2_results is None:\n",
    "        return result\n",
    "    library_info = getattr(stage2_results, 'library_info', None)\n",
    "    if library_info is None:\n",
    "        return result\n",
    "    if isinstance(library_info, dict):\n",
    "        result['library_n_total'] = library_info.get('n_total', None)\n",
    "        result['library_n_pysr'] = library_info.get('n_pysr', None)\n",
    "        result['library_n_variant'] = library_info.get('n_variant', None)\n",
    "        result['library_n_poly'] = library_info.get('n_poly', None)\n",
    "        result['library_n_op'] = library_info.get('n_op', None)\n",
    "    selection_analysis = getattr(stage2_results, 'selection_analysis', None)\n",
    "    if selection_analysis is not None and isinstance(selection_analysis, dict):\n",
    "        result['selected_from_pysr'] = selection_analysis.get('from_pysr', None)\n",
    "        result['selected_from_variant'] = selection_analysis.get('from_variant', None)\n",
    "        result['selected_from_poly'] = selection_analysis.get('from_poly', None)\n",
    "        result['selected_from_op'] = selection_analysis.get('from_op', None)\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_result(\n",
    "    method_result: MethodResult,\n",
    "    true_features: List[str],\n",
    "    all_features: List[str],\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    y_pred_test: np.ndarray\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Complete evaluation of a method result.\"\"\"\n",
    "    var_metrics = evaluate_variable_selection(method_result.discovered_features, true_features, all_features)\n",
    "    train_metrics = evaluate_prediction(y_train, method_result.predictions)\n",
    "    test_metrics = evaluate_prediction(y_test, y_pred_test)\n",
    "    library_metrics = evaluate_library_composition(method_result.stage2_results)\n",
    "\n",
    "    return {\n",
    "        'var_precision': var_metrics['precision'], 'var_recall': var_metrics['recall'],\n",
    "        'var_f1': var_metrics['f1'], 'var_tp': var_metrics['tp'],\n",
    "        'var_fp': var_metrics['fp'], 'var_fn': var_metrics['fn'],\n",
    "        'selected_correct': var_metrics['selected_correct'],\n",
    "        'train_r2': train_metrics['r2'], 'train_rmse': train_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'], 'test_rmse': test_metrics['rmse'],\n",
    "        'test_mae': test_metrics['mae'], 'test_nrmse': test_metrics['nrmse'],\n",
    "        **library_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Experiment Runner (v4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPERIMENT RESULT DATACLASS (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"\n",
    "    Container for a single experiment result (v4.1).\n",
    "    Includes ai_feynman_id, library composition, and timing profile fields.\n",
    "    \"\"\"\n",
    "    # Experiment identifiers\n",
    "    experiment_id: str\n",
    "    equation_name: str\n",
    "    equation_type: str\n",
    "    ai_feynman_id: str\n",
    "    noise_level: float\n",
    "    n_dummy: int\n",
    "    n_samples: int\n",
    "    with_dims: bool\n",
    "    method: str\n",
    "    \n",
    "    # Variable selection metrics\n",
    "    var_precision: float\n",
    "    var_recall: float\n",
    "    var_f1: float\n",
    "    var_tp: int\n",
    "    var_fp: int\n",
    "    var_fn: int\n",
    "    selected_correct: bool\n",
    "    \n",
    "    # Prediction metrics\n",
    "    train_r2: float\n",
    "    test_r2: float\n",
    "    train_rmse: float\n",
    "    test_rmse: float\n",
    "    \n",
    "    # Efficiency\n",
    "    runtime_seconds: float\n",
    "    \n",
    "    # v4.1: Library composition analysis\n",
    "    library_n_total: Optional[int] = None\n",
    "    library_n_pysr: Optional[int] = None\n",
    "    library_n_variant: Optional[int] = None\n",
    "    library_n_poly: Optional[int] = None\n",
    "    library_n_op: Optional[int] = None\n",
    "    selected_from_pysr: Optional[int] = None\n",
    "    selected_from_variant: Optional[int] = None\n",
    "    selected_from_poly: Optional[int] = None\n",
    "    selected_from_op: Optional[int] = None\n",
    "    \n",
    "    # v4.1: Timing profile\n",
    "    timing_stage1: Optional[float] = None\n",
    "    timing_pysr: Optional[float] = None\n",
    "    timing_library: Optional[float] = None\n",
    "    timing_ewsindy: Optional[float] = None\n",
    "    timing_stage3: Optional[float] = None\n",
    "    \n",
    "    # Additional info\n",
    "    discovered_equation: str = \"\"\n",
    "    true_equation: str = \"\"\n",
    "    success: bool = True\n",
    "    error_message: Optional[str] = None\n",
    "    timestamp: str = \"\"\n",
    "\n",
    "\n",
    "print(\"ExperimentResult dataclass v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPERIMENT RUNNER CLASS (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"Orchestrates benchmark experiments (v4.1).\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path = DATA_DIR, results_dir: Path = RESULTS_DIR, methods: List[str] = METHODS):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.methods = methods\n",
    "        self.results = []\n",
    "        self.checkpoint_file = self.results_dir / 'checkpoint_v4.1.pkl'\n",
    "        self.results_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    def run_single_experiment(self, dataset_filename: str, method_name: str, with_dims: bool = True) -> ExperimentResult:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Load dataset\n",
    "        filepath = self.data_dir / dataset_filename\n",
    "        dataset = np.load(filepath, allow_pickle=True)\n",
    "        \n",
    "        X = dataset['X']\n",
    "        y = dataset['y']\n",
    "        feature_names = list(dataset['feature_names'])\n",
    "        true_features = list(dataset['true_features'])\n",
    "        equation_name = str(dataset['equation_name'])\n",
    "        equation_type = str(dataset['equation_type'])\n",
    "        equation_str = str(dataset['equation_str'])\n",
    "        noise_level = float(dataset['noise_level'])\n",
    "        n_dummy = int(dataset['n_dummy'])\n",
    "        n_samples = int(dataset['n_samples'])\n",
    "        ai_feynman_id = str(dataset.get('ai_feynman_id', 'Unknown'))\n",
    "        \n",
    "        # Create UserInputs\n",
    "        if 'variable_dimensions' in dataset:\n",
    "            variable_dimensions = dataset['variable_dimensions'].item()\n",
    "        elif 'variable_dimensions_pkl' in dataset:\n",
    "            variable_dimensions = pickle.loads(dataset['variable_dimensions_pkl'].tobytes())\n",
    "        else:\n",
    "            variable_dimensions = {}\n",
    "        \n",
    "        if 'physical_bounds' in dataset:\n",
    "            physical_bounds = dataset['physical_bounds'].item()\n",
    "        elif 'physical_bounds_pkl' in dataset:\n",
    "            physical_bounds = pickle.loads(dataset['physical_bounds_pkl'].tobytes())\n",
    "        else:\n",
    "            physical_bounds = {}\n",
    "        \n",
    "        user_inputs = UserInputs(\n",
    "            variable_dimensions=variable_dimensions,\n",
    "            target_dimensions=list(dataset['target_dimensions']),\n",
    "            physical_bounds=physical_bounds\n",
    "        )\n",
    "        \n",
    "        experiment_id = f\"{equation_name}_n{n_samples}_noise{noise_level:.2f}_dummy{n_dummy}_dims{with_dims}_{method_name}\"\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=EXPERIMENT_SEED)\n",
    "        \n",
    "        runner = get_method_runner(method_name)\n",
    "        method_result = runner.run(X_train, y_train, feature_names, user_inputs, with_dims)\n",
    "        \n",
    "        y_pred_test = self._get_test_predictions(method_result, X_test, X_train, y_train, feature_names)\n",
    "        \n",
    "        eval_metrics = evaluate_result(method_result, true_features, feature_names, y_train, y_test, y_pred_test)\n",
    "        \n",
    "        return ExperimentResult(\n",
    "            experiment_id=experiment_id, equation_name=equation_name, equation_type=equation_type,\n",
    "            ai_feynman_id=ai_feynman_id, noise_level=noise_level, n_dummy=n_dummy,\n",
    "            n_samples=n_samples, with_dims=with_dims, method=method_name,\n",
    "            var_precision=eval_metrics['var_precision'], var_recall=eval_metrics['var_recall'],\n",
    "            var_f1=eval_metrics['var_f1'], var_tp=eval_metrics['var_tp'],\n",
    "            var_fp=eval_metrics['var_fp'], var_fn=eval_metrics['var_fn'],\n",
    "            selected_correct=eval_metrics['selected_correct'],\n",
    "            train_r2=eval_metrics['train_r2'], test_r2=eval_metrics['test_r2'],\n",
    "            train_rmse=eval_metrics['train_rmse'], test_rmse=eval_metrics['test_rmse'],\n",
    "            runtime_seconds=method_result.runtime_seconds,\n",
    "            library_n_total=eval_metrics.get('library_n_total'),\n",
    "            library_n_pysr=eval_metrics.get('library_n_pysr'),\n",
    "            library_n_variant=eval_metrics.get('library_n_variant'),\n",
    "            library_n_poly=eval_metrics.get('library_n_poly'),\n",
    "            library_n_op=eval_metrics.get('library_n_op'),\n",
    "            selected_from_pysr=eval_metrics.get('selected_from_pysr'),\n",
    "            selected_from_variant=eval_metrics.get('selected_from_variant'),\n",
    "            selected_from_poly=eval_metrics.get('selected_from_poly'),\n",
    "            selected_from_op=eval_metrics.get('selected_from_op'),\n",
    "            discovered_equation=method_result.equation,\n",
    "            true_equation=equation_str, success=method_result.success,\n",
    "            error_message=method_result.error_message, timestamp=timestamp\n",
    "        )\n",
    "    \n",
    "    def _get_test_predictions(self, method_result: MethodResult, X_test: np.ndarray, X_train: np.ndarray, y_train: np.ndarray, feature_names: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get predictions on test set (v4.1 FIXED).\"\"\"\n",
    "        if not method_result.success or len(method_result.discovered_features) == 0:\n",
    "            return np.zeros(X_test.shape[0])\n",
    "        \n",
    "        # Method 1: Use PySR model if available (v4.1 FIX)\n",
    "        if method_result.pysr_model is not None:\n",
    "            try:\n",
    "                return method_result.pysr_model.predict(X_test)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Method 2: Use Stage2Results with augmented library\n",
    "        if method_result.stage2_results is not None:\n",
    "            stage2 = method_result.stage2_results\n",
    "            if (hasattr(stage2, 'stlsq_coefficients') and hasattr(stage2, 'stlsq_support') and\n",
    "                stage2.stlsq_coefficients is not None and stage2.stlsq_support is not None):\n",
    "                try:\n",
    "                    if hasattr(stage2, 'library_builder') and stage2.library_builder is not None:\n",
    "                        Phi_test = stage2.library_builder.transform(X_test)\n",
    "                        if hasattr(stage2, 'scaler') and stage2.scaler is not None:\n",
    "                            Phi_test = stage2.scaler.transform(Phi_test)\n",
    "                        support = stage2.stlsq_support\n",
    "                        coef = stage2.stlsq_coefficients\n",
    "                        Phi_selected = Phi_test[:, support]\n",
    "                        return Phi_selected @ coef[support]\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Method 3: Fallback - polynomial model\n",
    "        try:\n",
    "            discovered = method_result.discovered_features\n",
    "            if len(discovered) > 0:\n",
    "                feature_indices = [feature_names.index(f) for f in discovered if f in feature_names]\n",
    "                if len(feature_indices) > 0:\n",
    "                    X_train_sel = X_train[:, feature_indices]\n",
    "                    X_test_sel = X_test[:, feature_indices]\n",
    "                    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "                    X_train_poly = poly.fit_transform(X_train_sel)\n",
    "                    X_test_poly = poly.transform(X_test_sel)\n",
    "                    model = Ridge(alpha=0.1)\n",
    "                    model.fit(X_train_poly, y_train)\n",
    "                    return model.predict(X_test_poly)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return np.zeros(X_test.shape[0])\n",
    "    \n",
    "    def run_core_experiments(self, verbose: bool = True) -> List[ExperimentResult]:\n",
    "        results = []\n",
    "        configs = get_core_experiment_configs()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(\" CORE EXPERIMENTS (v4.1)\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"Total configurations: {len(configs)}\")\n",
    "            print(f\"Methods: {self.methods}\")\n",
    "            print(f\"Total experiments: {len(configs) * len(self.methods)}\")\n",
    "            print()\n",
    "        \n",
    "        total = len(configs) * len(self.methods)\n",
    "        \n",
    "        with tqdm(total=total, desc=\"Core Experiments\") as pbar:\n",
    "            for config in configs:\n",
    "                eq_idx = EQUATION_INDEX_MAP[config['equation_name']]\n",
    "                filename = f\"eq{eq_idx}_{config['equation_name']}_n{config['n_samples']}_noise{config['noise_level']:.2f}_dummy{config['n_dummy']}.npz\"\n",
    "                \n",
    "                filepath = self.data_dir / filename\n",
    "                if not filepath.exists():\n",
    "                    if verbose:\n",
    "                        print(f\"  [SKIP] Dataset not found: {filename}\")\n",
    "                    pbar.update(len(self.methods))\n",
    "                    continue\n",
    "                \n",
    "                for method in self.methods:\n",
    "                    try:\n",
    "                        result = self.run_single_experiment(filename, method, with_dims=config['with_dims'])\n",
    "                        results.append(result)\n",
    "                    except Exception as e:\n",
    "                        if verbose:\n",
    "                            print(f\"  [ERROR] {filename}/{method}: {e}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    if len(results) % 10 == 0:\n",
    "                        self._save_checkpoint(results)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nCore experiments complete: {len(results)} results\")\n",
    "        return results\n",
    "    \n",
    "    def run_supplementary_experiments(self, verbose: bool = True) -> List[ExperimentResult]:\n",
    "        results = []\n",
    "        configs = get_supplementary_experiment_configs()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(\" SUPPLEMENTARY EXPERIMENTS (v4.1)\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"Total configurations: {len(configs)}\")\n",
    "            print()\n",
    "        \n",
    "        with tqdm(total=len(configs), desc=\"Supplementary\") as pbar:\n",
    "            for config in configs:\n",
    "                eq_idx = EQUATION_INDEX_MAP[config['equation_name']]\n",
    "                filename = f\"eq{eq_idx}_{config['equation_name']}_n{config['n_samples']}_noise{config['noise_level']:.2f}_dummy{config['n_dummy']}.npz\"\n",
    "                \n",
    "                filepath = self.data_dir / filename\n",
    "                if not filepath.exists():\n",
    "                    if verbose:\n",
    "                        print(f\"  [SKIP] Dataset not found: {filename}\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    result = self.run_single_experiment(filename, 'physics_sr', with_dims=config['with_dims'])\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"  [ERROR] {filename}: {e}\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nSupplementary complete: {len(results)} results\")\n",
    "        return results\n",
    "    \n",
    "    def run_all_experiments(self, include_supplementary: bool = True, verbose: bool = True) -> List[ExperimentResult]:\n",
    "        all_results = []\n",
    "        core_results = self.run_core_experiments(verbose=verbose)\n",
    "        all_results.extend(core_results)\n",
    "        if include_supplementary:\n",
    "            supp_results = self.run_supplementary_experiments(verbose=verbose)\n",
    "            all_results.extend(supp_results)\n",
    "        self.results = all_results\n",
    "        if self.checkpoint_file.exists():\n",
    "            self.checkpoint_file.unlink()\n",
    "        return all_results\n",
    "    \n",
    "    def _save_checkpoint(self, results: List[ExperimentResult]):\n",
    "        with open(self.checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    def load_checkpoint(self) -> Optional[List[ExperimentResult]]:\n",
    "        if self.checkpoint_file.exists():\n",
    "            with open(self.checkpoint_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return None\n",
    "    \n",
    "    def results_to_dataframe(self, results: List[ExperimentResult] = None) -> pd.DataFrame:\n",
    "        if results is None:\n",
    "            results = self.results\n",
    "        return pd.DataFrame([asdict(r) for r in results])\n",
    "    \n",
    "    def save_results(self, results: List[ExperimentResult] = None, filename_base: str = 'experiment_results_v4.1'):\n",
    "        if results is None:\n",
    "            results = self.results\n",
    "        df = self.results_to_dataframe(results)\n",
    "        csv_path = self.results_dir / f'{filename_base}.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved CSV: {csv_path}\")\n",
    "        pkl_path = self.results_dir / f'{filename_base}.pkl'\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"Saved PKL: {pkl_path}\")\n",
    "    \n",
    "    def load_results(self, filename_base: str = 'experiment_results_v4.1') -> List[ExperimentResult]:\n",
    "        pkl_path = self.results_dir / f'{filename_base}.pkl'\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "print(\"ExperimentRunner class v4.1 defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Run All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VERIFY DATA AVAILABILITY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Checking data availability...\")\n",
    "print()\n",
    "\n",
    "data_files = sorted(DATA_DIR.glob('*.npz'))\n",
    "print(f\"Found {len(data_files)} datasets in {DATA_DIR}\")\n",
    "print()\n",
    "\n",
    "if len(data_files) > 0:\n",
    "    print(\"Available datasets:\")\n",
    "    for f in data_files[:10]:\n",
    "        print(f\"  - {f.name}\")\n",
    "    if len(data_files) > 10:\n",
    "        print(f\"  ... and {len(data_files) - 10} more\")\n",
    "else:\n",
    "    print(\"[WARNING] No datasets found! Run DataGen.ipynb first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CHECK FOR EXISTING CHECKPOINT\n",
    "# ==============================================================================\n",
    "\n",
    "runner = ExperimentRunner(\n",
    "    data_dir=DATA_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    methods=METHODS\n",
    ")\n",
    "\n",
    "checkpoint_results = runner.load_checkpoint()\n",
    "if checkpoint_results is not None:\n",
    "    print(f\"Found checkpoint with {len(checkpoint_results)} results.\")\n",
    "    print(\"You can resume from checkpoint or start fresh.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Will start fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" PHYSICS-SR FRAMEWORK v4.1 BENCHMARK\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Methods: {METHODS}\")\n",
    "print(f\"Equations: {list(EQUATION_INDEX_MAP.keys())}\")\n",
    "print(f\"Expected core experiments: 96\")\n",
    "print(f\"Expected supplementary experiments: 8\")\n",
    "print(f\"Expected total: 104\")\n",
    "print()\n",
    "print(\"Starting experiments...\")\n",
    "print()\n",
    "\n",
    "all_results = runner.run_all_experiments(\n",
    "    include_supplementary=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(f\" EXPERIMENTS COMPLETE: {len(all_results)} results\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Saving results...\")\n",
    "print()\n",
    "\n",
    "runner.save_results(filename_base='experiment_results_v4.1')\n",
    "\n",
    "print()\n",
    "print(\"Results saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "df = runner.results_to_dataframe()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" RESULTS SUMMARY (v4.1)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Overall statistics\n",
    "print(\"OVERALL STATISTICS:\")\n",
    "print(f\"  Total experiments: {len(df)}\")\n",
    "print(f\"  Successful: {df['success'].sum()}\")\n",
    "print(f\"  Failed: {(~df['success']).sum()}\")\n",
    "print()\n",
    "\n",
    "# By method\n",
    "print(\"BY METHOD:\")\n",
    "method_summary = df.groupby('method').agg({\n",
    "    'test_r2': ['mean', 'std'],\n",
    "    'var_f1': ['mean', 'std'],\n",
    "    'runtime_seconds': 'mean',\n",
    "    'success': 'sum'\n",
    "}).round(4)\n",
    "print(method_summary)\n",
    "print()\n",
    "\n",
    "# By equation\n",
    "print(\"BY EQUATION:\")\n",
    "equation_summary = df.groupby('equation_name').agg({\n",
    "    'test_r2': 'mean',\n",
    "    'var_f1': 'mean',\n",
    "    'ai_feynman_id': 'first'\n",
    "}).round(4)\n",
    "print(equation_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DETAILED COMPARISON TABLE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"DETAILED METHOD COMPARISON:\")\n",
    "print()\n",
    "\n",
    "core_df = df[df['n_samples'] == 500].copy()\n",
    "\n",
    "comparison = core_df.groupby(['method', 'with_dims']).agg({\n",
    "    'test_r2': ['mean', 'std'],\n",
    "    'var_f1': ['mean', 'std'],\n",
    "    'var_precision': 'mean',\n",
    "    'var_recall': 'mean',\n",
    "    'runtime_seconds': 'mean',\n",
    "}).round(4)\n",
    "\n",
    "print(comparison)\n",
    "print()\n",
    "\n",
    "print(\"PHYSICS-SR ADVANTAGE (with_dims=True):\")\n",
    "physics_sr = core_df[(core_df['method'] == 'physics_sr') & (core_df['with_dims'] == True)]\n",
    "pysr_only = core_df[(core_df['method'] == 'pysr_only') & (core_df['with_dims'] == True)]\n",
    "lasso_pysr = core_df[(core_df['method'] == 'lasso_pysr') & (core_df['with_dims'] == True)]\n",
    "\n",
    "print(f\"  Physics-SR mean Test R2: {physics_sr['test_r2'].mean():.4f}\")\n",
    "print(f\"  PySR-Only mean Test R2: {pysr_only['test_r2'].mean():.4f}\")\n",
    "print(f\"  LASSO+PySR mean Test R2: {lasso_pysr['test_r2'].mean():.4f}\")\n",
    "print()\n",
    "print(f\"  Physics-SR mean Var F1: {physics_sr['var_f1'].mean():.4f}\")\n",
    "print(f\"  PySR-Only mean Var F1: {pysr_only['var_f1'].mean():.4f}\")\n",
    "print(f\"  LASSO+PySR mean Var F1: {lasso_pysr['var_f1'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# NOISE ROBUSTNESS ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"NOISE ROBUSTNESS ANALYSIS:\")\n",
    "print()\n",
    "\n",
    "core_df = df[df['n_samples'] == 500].copy()\n",
    "\n",
    "noise_comparison = core_df.groupby(['method', 'noise_level']).agg({\n",
    "    'test_r2': 'mean',\n",
    "    'var_f1': 'mean',\n",
    "}).round(4)\n",
    "\n",
    "print(noise_comparison)\n",
    "print()\n",
    "\n",
    "print(\"DEGRADATION FROM 0% TO 5% NOISE:\")\n",
    "for method in METHODS:\n",
    "    r2_0 = core_df[(core_df['method'] == method) & (core_df['noise_level'] == 0.0)]['test_r2'].mean()\n",
    "    r2_5 = core_df[(core_df['method'] == method) & (core_df['noise_level'] == 0.05)]['test_r2'].mean()\n",
    "    degradation = r2_0 - r2_5\n",
    "    print(f\"  {method}: {degradation:.4f} (R2 drop)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" EXPERIMENT EXECUTION COMPLETE (v4.1)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"KEY FINDINGS:\")\n",
    "print()\n",
    "\n",
    "best_method_r2 = df.groupby('method')['test_r2'].mean().idxmax()\n",
    "best_r2 = df.groupby('method')['test_r2'].mean().max()\n",
    "print(f\"1. Best method by Test R2: {best_method_r2} ({best_r2:.4f})\")\n",
    "\n",
    "best_method_f1 = df.groupby('method')['var_f1'].mean().idxmax()\n",
    "best_f1 = df.groupby('method')['var_f1'].mean().max()\n",
    "print(f\"2. Best method by Var F1: {best_method_f1} ({best_f1:.4f})\")\n",
    "\n",
    "physics_dims = df[(df['method'] == 'physics_sr') & (df['with_dims'] == True)]['test_r2'].mean()\n",
    "physics_nodims = df[(df['method'] == 'physics_sr') & (df['with_dims'] == False)]['test_r2'].mean()\n",
    "print(f\"3. Dimensional analysis R2 improvement: +{physics_dims - physics_nodims:.4f}\")\n",
    "\n",
    "physics_0 = df[(df['method'] == 'physics_sr') & (df['noise_level'] == 0.0)]['test_r2'].mean()\n",
    "physics_5 = df[(df['method'] == 'physics_sr') & (df['noise_level'] == 0.05)]['test_r2'].mean()\n",
    "pysr_0 = df[(df['method'] == 'pysr_only') & (df['noise_level'] == 0.0)]['test_r2'].mean()\n",
    "pysr_5 = df[(df['method'] == 'pysr_only') & (df['noise_level'] == 0.05)]['test_r2'].mean()\n",
    "print(f\"4. Physics-SR noise degradation: {physics_0 - physics_5:.4f}\")\n",
    "print(f\"   PySR-Only noise degradation: {pysr_0 - pysr_5:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"OUTPUT FILES:\")\n",
    "print(f\"  - {RESULTS_DIR / 'experiment_results_v4.1.csv'}\")\n",
    "print(f\"  - {RESULTS_DIR / 'experiment_results_v4.1.pkl'}\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\" Ready for Analysis.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Module Summary\n",
    "\n",
    "### Experiments.ipynb v4.1 - Complete\n",
    "\n",
    "**Completed Components:**\n",
    "\n",
    "1. **Configuration (Section 1)** - Environment setup, path configuration, v4.1 constants\n",
    "2. **Algorithm Import (Section 2)** - All 13 algorithm modules loaded\n",
    "3. **Method Runners (Section 3)** - PhysicsSRRunner, PySROnlyRunner, LASSOPySRRunner with pysr_model storage (v4.1 FIX)\n",
    "4. **Evaluation Functions (Section 4)** - Variable selection, prediction, library composition metrics\n",
    "5. **Experiment Runner (Section 5)** - ExperimentResult dataclass, ExperimentRunner class with fixed _get_test_predictions\n",
    "6. **Execution (Section 6)** - Data verification, checkpoint support, full experiment execution\n",
    "7. **Summary (Section 7)** - Overall statistics, method comparison, noise robustness analysis\n",
    "\n",
    "**Key v4.1 Fixes:**\n",
    "- Stored pysr_model in MethodResult for accurate test predictions\n",
    "- Added ai_feynman_id field for AI Feynman benchmark tracking\n",
    "- Updated equation mapping for new test equations (Coulomb, Cosines, Barometric, DotProduct)\n",
    "- Added library composition analysis fields"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}