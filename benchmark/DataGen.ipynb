{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataGen - Physics-SR Framework v3.0 Benchmark\n",
    "\n",
    "## Benchmark Data Generation Module\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook generates all test datasets for the Physics-SR Framework v3.0 benchmark experiments.\n",
    "\n",
    "### Test Equations\n",
    "\n",
    "1. **KK2000** - Khairoutdinov-Kogan Warm Rain Autoconversion (power-law)\n",
    "2. **Newton** - Gravitational Force Law (rational function)\n",
    "3. **Ideal Gas** - Ideal Gas Law for Pressure (rational function)\n",
    "4. **Damped** - Damped Harmonic Oscillation (nested transcendental)\n",
    "\n",
    "### Experimental Design\n",
    "\n",
    "**Core Experiments (16 datasets):**\n",
    "- 4 equations x 2 noise (0%, 5%) x 2 dummy (0, 5) = 16\n",
    "- Each configuration has n=500 samples\n",
    "\n",
    "**Supplementary Experiments (8 datasets):**\n",
    "- Sample size sensitivity: n = 250, 750\n",
    "- Fixed: noise=5%, dummy=5\n",
    "\n",
    "### Output Format\n",
    "\n",
    "Each dataset is saved as a `.npz` file containing:\n",
    "- `X`: Feature matrix (n_samples, n_features)\n",
    "- `y`: Target vector with noise\n",
    "- `y_true`: Noise-free target\n",
    "- Metadata: equation info, ground truth, dimensional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# COLAB SETUP - Run this cell first!\n",
    "# ==============================================================================\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    if not os.path.exists('/content/Physics-Informed-Symbolic-Regression'):\n",
    "        !git clone https://github.com/Garthzzz/Physics-Informed-Symbolic-Regression.git\n",
    "    %cd /content/Physics-Informed-Symbolic-Regression\n",
    "    \n",
    "    !pip install -q pysr\n",
    "    import pysr\n",
    "    pysr.install()\n",
    "    \n",
    "    print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DataGen.ipynb - Benchmark Data Generation Module\n",
    "=================================================\n",
    "\n",
    "Physics-SR Framework v3.0 Benchmark Suite\n",
    "\n",
    "This module generates synthetic datasets for testing the Physics-SR framework\n",
    "across four physics problems with varying complexity:\n",
    "- KK2000 Autoconversion (power-law, 2 true variables)\n",
    "- Newton Gravitation (rational, 3 true variables)\n",
    "- Ideal Gas Law (rational, 3 true variables)\n",
    "- Damped Oscillation (nested transcendental, 4 true variables)\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"DataGen: All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION CONSTANTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Default sample size for core experiments\n",
    "DEFAULT_N_SAMPLES = 500\n",
    "\n",
    "# Output directory for generated datasets\n",
    "DATA_DIR = Path('data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Noise levels to test (multiplicative log-normal noise)\n",
    "NOISE_LEVELS = [0.0, 0.05]  # 0% and 5%\n",
    "\n",
    "# Dummy feature counts\n",
    "DUMMY_COUNTS = [0, 5]\n",
    "\n",
    "# Sample sizes for supplementary experiments\n",
    "SAMPLE_SIZES_SUPPLEMENTARY = [250, 750]\n",
    "\n",
    "# Dimensional exponent order: [Mass, Length, Time, Temperature]\n",
    "DIM_NAMES = ['M', 'L', 'T', 'Theta']\n",
    "\n",
    "print(f\"Output directory: {DATA_DIR.resolve()}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "print(f\"Noise levels: {NOISE_LEVELS}\")\n",
    "print(f\"Dummy feature counts: {DUMMY_COUNTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Base Test Equation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ABSTRACT BASE CLASS FOR TEST EQUATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "class BaseTestEquation(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for test equations in the Physics-SR benchmark.\n",
    "    \n",
    "    Each equation defines:\n",
    "    - True mathematical relationship\n",
    "    - Feature generation with physically realistic ranges\n",
    "    - Dummy feature generation\n",
    "    - Dimensional information for physics-informed methods\n",
    "    - Noise addition mechanism\n",
    "    \n",
    "    Subclasses must implement all abstract methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes to be defined by subclasses\n",
    "    name: str                           # Short identifier (e.g., \"kk2000\")\n",
    "    full_name: str                      # Full description\n",
    "    equation_str: str                   # Human-readable equation\n",
    "    equation_type: str                  # power_law, rational, nested_transcendental\n",
    "    equation_index: int                 # 1, 2, 3, 4 for file naming\n",
    "    \n",
    "    true_feature_names: List[str]       # Features in true equation\n",
    "    dummy_feature_pool: List[str]       # Available dummy features\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_true_features(self, n_samples: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate values for features that appear in the true equation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of samples to generate\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, np.ndarray]\n",
    "            Dictionary mapping feature names to value arrays\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_dummy_features(self, n_samples: int, n_dummy: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate values for dummy (irrelevant) features.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of samples to generate\n",
    "        n_dummy : int\n",
    "            Number of dummy features to include (0 to len(dummy_feature_pool))\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, np.ndarray]\n",
    "            Dictionary mapping dummy feature names to value arrays\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute_target(self, features: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the target variable from true features.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features : Dict[str, np.ndarray]\n",
    "            Dictionary mapping feature names to value arrays\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Target variable values (noise-free)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_variable_dimensions(self, feature_names: List[str]) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Return dimensional exponents [M, L, T, Theta] for given features.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_names : List[str]\n",
    "            List of feature names to get dimensions for\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, List[float]]\n",
    "            Dictionary mapping feature names to dimension vectors\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_target_dimensions(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Return dimensional exponents [M, L, T, Theta] for target variable.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            Dimension vector for target\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_physical_bounds(self) -> Dict[str, Dict[str, Optional[float]]]:\n",
    "        \"\"\"\n",
    "        Return physical bounds for all variables.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Dict[str, Optional[float]]]\n",
    "            Dictionary mapping variable names to {min, max} bounds\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def add_noise(self, y: np.ndarray, noise_level: float, seed: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Add multiplicative log-normal noise to target.\n",
    "        \n",
    "        For positive-valued targets (rates, forces), multiplicative noise\n",
    "        is more physically appropriate than additive noise.\n",
    "        \n",
    "        y_noisy = y * exp(N(0, noise_level))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.ndarray\n",
    "            Noise-free target values\n",
    "        noise_level : float\n",
    "            Standard deviation of log-normal noise (0.05 = 5%)\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Target with noise added\n",
    "        \"\"\"\n",
    "        if noise_level <= 0:\n",
    "            return y.copy()\n",
    "        \n",
    "        np.random.seed(seed + 2000)  # Offset seed for noise\n",
    "        \n",
    "        # Multiplicative log-normal noise\n",
    "        noise_factor = np.exp(np.random.normal(0, noise_level, len(y)))\n",
    "        y_noisy = y * noise_factor\n",
    "        \n",
    "        return y_noisy\n",
    "    \n",
    "    def get_ground_truth(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return ground truth information for evaluation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Ground truth including equation string, active features, type\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'equation': self.equation_str,\n",
    "            'active_features': self.true_feature_names,\n",
    "            'equation_type': self.equation_type,\n",
    "            'equation_index': self.equation_index,\n",
    "            'name': self.name,\n",
    "            'full_name': self.full_name\n",
    "        }\n",
    "\n",
    "print(\"BaseTestEquation abstract class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Equation 1 - KK2000 Warm Rain Autoconversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EQUATION 1: KHAIROUTDINOV-KOGAN 2000 AUTOCONVERSION\n",
    "# ==============================================================================\n",
    "\n",
    "class KK2000Equation(BaseTestEquation):\n",
    "    \"\"\"\n",
    "    Khairoutdinov-Kogan 2000 Warm Rain Autoconversion Parameterization.\n",
    "    \n",
    "    Equation: P_auto = 1350 * q_c^2.47 * N_d^(-1.79)\n",
    "    \n",
    "    This is a widely-used empirical parameterization for the autoconversion\n",
    "    process in warm rain formation, derived from LES simulations.\n",
    "    \n",
    "    Reference:\n",
    "        Khairoutdinov, M., and Y. Kogan, 2000: \"A New Cloud Physics\n",
    "        Parameterization in a Large-Eddy Simulation Model of Marine\n",
    "        Stratocumulus.\" Monthly Weather Review, 128, 229-243.\n",
    "    \n",
    "    Characteristics:\n",
    "    - Type: Power-law (multiplicative)\n",
    "    - True variables: 2 (q_c, N_d)\n",
    "    - Non-integer exponents: Yes (2.47, -1.79)\n",
    "    - Difficulty: Medium (core application of framework)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes\n",
    "    name = \"kk2000\"\n",
    "    full_name = \"Khairoutdinov-Kogan 2000 Autoconversion\"\n",
    "    equation_str = \"P_auto = 1350 * q_c^2.47 * N_d^(-1.79)\"\n",
    "    equation_type = \"power_law\"\n",
    "    equation_index = 1\n",
    "    \n",
    "    true_feature_names = ['q_c', 'N_d']\n",
    "    dummy_feature_pool = ['r_eff', 'LWC', 'T', 'w', 'p']\n",
    "    \n",
    "    # Coefficients from KK2000\n",
    "    C = 1350.0\n",
    "    alpha = 2.47    # Exponent for q_c\n",
    "    beta = -1.79    # Exponent for N_d\n",
    "    \n",
    "    # Dimensional information: [Mass, Length, Time, Temperature]\n",
    "    _dimensions = {\n",
    "        'q_c':   [0, 0, 0, 0],      # Cloud water mixing ratio (kg/kg, dimensionless)\n",
    "        'N_d':   [0, -3, 0, 0],     # Droplet number concentration (m^-3)\n",
    "        'r_eff': [0, 1, 0, 0],      # Effective radius (m)\n",
    "        'LWC':   [1, -3, 0, 0],     # Liquid water content (kg/m^3)\n",
    "        'T':     [0, 0, 0, 1],      # Temperature (K)\n",
    "        'w':     [0, 1, -1, 0],     # Vertical velocity (m/s)\n",
    "        'p':     [1, -1, -2, 0],    # Pressure (Pa = kg/(m*s^2))\n",
    "    }\n",
    "    \n",
    "    _target_dims = [0, 0, -1, 0]    # Autoconversion rate (s^-1)\n",
    "    \n",
    "    # Physically realistic ranges\n",
    "    _ranges = {\n",
    "        'q_c':   (1e-4, 5e-3),      # Cloud water: 0.1 - 5 g/kg\n",
    "        'N_d':   (1e7, 5e8),        # Droplet concentration: 10 - 500 cm^-3\n",
    "        'r_eff': (5e-6, 25e-6),     # Effective radius: 5 - 25 um\n",
    "        'LWC':   (0.1, 2.0),        # Liquid water content: 0.1 - 2 g/m^3\n",
    "        'T':     (270, 300),        # Temperature: -3 to 27 C\n",
    "        'w':     (-5, 5),           # Vertical velocity: -5 to 5 m/s\n",
    "        'p':     (8e4, 1e5),        # Pressure: 800 - 1000 hPa\n",
    "    }\n",
    "    \n",
    "    def generate_true_features(self, n_samples: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate cloud water mixing ratio and droplet concentration.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        return {\n",
    "            'q_c': np.random.uniform(*self._ranges['q_c'], n_samples),\n",
    "            'N_d': np.random.uniform(*self._ranges['N_d'], n_samples),\n",
    "        }\n",
    "    \n",
    "    def generate_dummy_features(self, n_samples: int, n_dummy: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate irrelevant features (r_eff, LWC, T, w, p).\"\"\"\n",
    "        if n_dummy <= 0:\n",
    "            return {}\n",
    "        \n",
    "        np.random.seed(seed + 1000)  # Offset seed for dummy features\n",
    "        \n",
    "        selected = self.dummy_feature_pool[:min(n_dummy, len(self.dummy_feature_pool))]\n",
    "        return {\n",
    "            name: np.random.uniform(*self._ranges[name], n_samples)\n",
    "            for name in selected\n",
    "        }\n",
    "    \n",
    "    def compute_target(self, features: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Compute autoconversion rate: P = 1350 * q_c^2.47 * N_d^(-1.79)\"\"\"\n",
    "        q_c = features['q_c']\n",
    "        N_d = features['N_d']\n",
    "        return self.C * np.power(q_c, self.alpha) * np.power(N_d, self.beta)\n",
    "    \n",
    "    def get_variable_dimensions(self, feature_names: List[str]) -> Dict[str, List[float]]:\n",
    "        \"\"\"Return dimensions for specified features.\"\"\"\n",
    "        return {name: self._dimensions[name] for name in feature_names if name in self._dimensions}\n",
    "    \n",
    "    def get_target_dimensions(self) -> List[float]:\n",
    "        \"\"\"Return dimensions for target (s^-1).\"\"\"\n",
    "        return self._target_dims.copy()\n",
    "    \n",
    "    def get_physical_bounds(self) -> Dict[str, Dict[str, Optional[float]]]:\n",
    "        \"\"\"Return physical bounds for all variables.\"\"\"\n",
    "        bounds = {'target': {'min': 0, 'max': None}}  # Rate must be non-negative\n",
    "        for name, (low, high) in self._ranges.items():\n",
    "            bounds[name] = {'min': low * 0.1, 'max': high * 10}  # Allow some margin\n",
    "        return bounds\n",
    "\n",
    "# Test instantiation\n",
    "eq1 = KK2000Equation()\n",
    "print(f\"Equation 1: {eq1.full_name}\")\n",
    "print(f\"  Formula: {eq1.equation_str}\")\n",
    "print(f\"  Type: {eq1.equation_type}\")\n",
    "print(f\"  True features: {eq1.true_feature_names}\")\n",
    "print(f\"  Dummy pool: {eq1.dummy_feature_pool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Equation 2 - Newton's Law of Universal Gravitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EQUATION 2: NEWTON'S LAW OF UNIVERSAL GRAVITATION\n",
    "# ==============================================================================\n",
    "\n",
    "class NewtonGravityEquation(BaseTestEquation):\n",
    "    \"\"\"\n",
    "    Newton's Law of Universal Gravitation.\n",
    "    \n",
    "    Equation: F = G * m1 * m2 / r^2\n",
    "    \n",
    "    The fundamental law governing gravitational attraction between masses.\n",
    "    \n",
    "    Reference:\n",
    "        Feynman Lectures on Physics, Volume I, Chapter 7.1\n",
    "    \n",
    "    Characteristics:\n",
    "    - Type: Rational function (quotient)\n",
    "    - True variables: 3 (m1, m2, r)\n",
    "    - Integer exponents: Yes (1, 1, -2)\n",
    "    - Difficulty: Easy (tests inverse-square law discovery)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes\n",
    "    name = \"newton\"\n",
    "    full_name = \"Newton's Law of Universal Gravitation\"\n",
    "    equation_str = \"F = G * m1 * m2 / r^2\"\n",
    "    equation_type = \"rational\"\n",
    "    equation_index = 2\n",
    "    \n",
    "    true_feature_names = ['m1', 'm2', 'r']\n",
    "    dummy_feature_pool = ['v1', 'v2', 'T', 't', 'rho']\n",
    "    \n",
    "    # Gravitational constant\n",
    "    G = 6.674e-11  # N*m^2/kg^2\n",
    "    \n",
    "    # Dimensional information: [Mass, Length, Time, Temperature]\n",
    "    _dimensions = {\n",
    "        'm1':  [1, 0, 0, 0],       # Mass (kg)\n",
    "        'm2':  [1, 0, 0, 0],       # Mass (kg)\n",
    "        'r':   [0, 1, 0, 0],       # Distance (m)\n",
    "        'v1':  [0, 1, -1, 0],      # Velocity (m/s)\n",
    "        'v2':  [0, 1, -1, 0],      # Velocity (m/s)\n",
    "        'T':   [0, 0, 0, 1],       # Temperature (K)\n",
    "        't':   [0, 0, 1, 0],       # Time (s)\n",
    "        'rho': [1, -3, 0, 0],      # Density (kg/m^3)\n",
    "    }\n",
    "    \n",
    "    _target_dims = [1, 1, -2, 0]   # Force (N = kg*m/s^2)\n",
    "    \n",
    "    # Physically realistic ranges\n",
    "    _ranges = {\n",
    "        'm1':  (1, 1000),          # Mass 1: 1 - 1000 kg\n",
    "        'm2':  (1, 1000),          # Mass 2: 1 - 1000 kg\n",
    "        'r':   (1, 100),           # Distance: 1 - 100 m\n",
    "        'v1':  (0, 100),           # Velocity 1: 0 - 100 m/s\n",
    "        'v2':  (0, 100),           # Velocity 2: 0 - 100 m/s\n",
    "        'T':   (200, 400),         # Temperature: 200 - 400 K\n",
    "        't':   (0, 1000),          # Time: 0 - 1000 s\n",
    "        'rho': (1, 10),            # Density: 1 - 10 kg/m^3\n",
    "    }\n",
    "    \n",
    "    def generate_true_features(self, n_samples: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate masses and distance.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        return {\n",
    "            'm1': np.random.uniform(*self._ranges['m1'], n_samples),\n",
    "            'm2': np.random.uniform(*self._ranges['m2'], n_samples),\n",
    "            'r':  np.random.uniform(*self._ranges['r'], n_samples),\n",
    "        }\n",
    "    \n",
    "    def generate_dummy_features(self, n_samples: int, n_dummy: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate irrelevant features (velocities, temperature, time, density).\"\"\"\n",
    "        if n_dummy <= 0:\n",
    "            return {}\n",
    "        \n",
    "        np.random.seed(seed + 1000)\n",
    "        \n",
    "        selected = self.dummy_feature_pool[:min(n_dummy, len(self.dummy_feature_pool))]\n",
    "        return {\n",
    "            name: np.random.uniform(*self._ranges[name], n_samples)\n",
    "            for name in selected\n",
    "        }\n",
    "    \n",
    "    def compute_target(self, features: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Compute gravitational force: F = G * m1 * m2 / r^2\"\"\"\n",
    "        m1 = features['m1']\n",
    "        m2 = features['m2']\n",
    "        r = features['r']\n",
    "        return self.G * m1 * m2 / (r ** 2)\n",
    "    \n",
    "    def get_variable_dimensions(self, feature_names: List[str]) -> Dict[str, List[float]]:\n",
    "        \"\"\"Return dimensions for specified features.\"\"\"\n",
    "        return {name: self._dimensions[name] for name in feature_names if name in self._dimensions}\n",
    "    \n",
    "    def get_target_dimensions(self) -> List[float]:\n",
    "        \"\"\"Return dimensions for target (N = kg*m/s^2).\"\"\"\n",
    "        return self._target_dims.copy()\n",
    "    \n",
    "    def get_physical_bounds(self) -> Dict[str, Dict[str, Optional[float]]]:\n",
    "        \"\"\"Return physical bounds for all variables.\"\"\"\n",
    "        bounds = {'target': {'min': 0, 'max': None}}  # Force magnitude is non-negative\n",
    "        for name, (low, high) in self._ranges.items():\n",
    "            bounds[name] = {'min': low * 0.1, 'max': high * 10}\n",
    "        return bounds\n",
    "\n",
    "# Test instantiation\n",
    "eq2 = NewtonGravityEquation()\n",
    "print(f\"Equation 2: {eq2.full_name}\")\n",
    "print(f\"  Formula: {eq2.equation_str}\")\n",
    "print(f\"  Type: {eq2.equation_type}\")\n",
    "print(f\"  True features: {eq2.true_feature_names}\")\n",
    "print(f\"  Dummy pool: {eq2.dummy_feature_pool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Equation 3 - Ideal Gas Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EQUATION 3: IDEAL GAS LAW\n",
    "# ==============================================================================\n",
    "\n",
    "class IdealGasEquation(BaseTestEquation):\n",
    "    \"\"\"\n",
    "    Ideal Gas Law (solved for pressure).\n",
    "    \n",
    "    Equation: P = n * R * T / V\n",
    "    \n",
    "    The fundamental equation of state for ideal gases, relating\n",
    "    pressure to temperature, volume, and amount of substance.\n",
    "    \n",
    "    Reference:\n",
    "        Feynman Lectures on Physics, Volume I, Chapter 39.11\n",
    "    \n",
    "    Characteristics:\n",
    "    - Type: Rational function (product with division)\n",
    "    - True variables: 3 (n, T, V)\n",
    "    - Integer exponents: Yes (1, 1, -1)\n",
    "    - Difficulty: Easy (tests multi-variable product structure)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes\n",
    "    name = \"ideal_gas\"\n",
    "    full_name = \"Ideal Gas Law\"\n",
    "    equation_str = \"P = n * R * T / V\"\n",
    "    equation_type = \"rational\"\n",
    "    equation_index = 3\n",
    "    \n",
    "    true_feature_names = ['n', 'T', 'V']\n",
    "    dummy_feature_pool = ['m', 'rho', 'c_p', 'mu', 'k']\n",
    "    \n",
    "    # Universal gas constant\n",
    "    R = 8.314  # J/(mol*K)\n",
    "    \n",
    "    # Dimensional information: [Mass, Length, Time, Temperature]\n",
    "    # Note: mol is treated as dimensionless for simplicity\n",
    "    _dimensions = {\n",
    "        'n':   [0, 0, 0, 0],       # Amount of substance (mol, dimensionless)\n",
    "        'T':   [0, 0, 0, 1],       # Temperature (K)\n",
    "        'V':   [0, 3, 0, 0],       # Volume (m^3)\n",
    "        'm':   [1, 0, 0, 0],       # Mass (kg)\n",
    "        'rho': [1, -3, 0, 0],      # Density (kg/m^3)\n",
    "        'c_p': [0, 2, -2, -1],     # Specific heat (J/(kg*K) = m^2/(s^2*K))\n",
    "        'mu':  [1, -1, -1, 0],     # Dynamic viscosity (Pa*s = kg/(m*s))\n",
    "        'k':   [1, 1, -3, -1],     # Thermal conductivity (W/(m*K) = kg*m/(s^3*K))\n",
    "    }\n",
    "    \n",
    "    _target_dims = [1, -1, -2, 0]  # Pressure (Pa = kg/(m*s^2))\n",
    "    \n",
    "    # Physically realistic ranges\n",
    "    _ranges = {\n",
    "        'n':   (0.1, 10),          # Moles: 0.1 - 10 mol\n",
    "        'T':   (200, 500),         # Temperature: 200 - 500 K\n",
    "        'V':   (0.001, 1),         # Volume: 1 mL - 1 m^3\n",
    "        'm':   (0.01, 1),          # Mass: 10 g - 1 kg\n",
    "        'rho': (0.1, 10),          # Density: 0.1 - 10 kg/m^3\n",
    "        'c_p': (500, 2000),        # Specific heat: 500 - 2000 J/(kg*K)\n",
    "        'mu':  (1e-5, 1e-3),       # Viscosity: 1e-5 - 1e-3 Pa*s\n",
    "        'k':   (0.01, 1),          # Thermal conductivity: 0.01 - 1 W/(m*K)\n",
    "    }\n",
    "    \n",
    "    def generate_true_features(self, n_samples: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate amount of substance, temperature, and volume.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        return {\n",
    "            'n': np.random.uniform(*self._ranges['n'], n_samples),\n",
    "            'T': np.random.uniform(*self._ranges['T'], n_samples),\n",
    "            'V': np.random.uniform(*self._ranges['V'], n_samples),\n",
    "        }\n",
    "    \n",
    "    def generate_dummy_features(self, n_samples: int, n_dummy: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate irrelevant features (mass, density, specific heat, etc.).\"\"\"\n",
    "        if n_dummy <= 0:\n",
    "            return {}\n",
    "        \n",
    "        np.random.seed(seed + 1000)\n",
    "        \n",
    "        selected = self.dummy_feature_pool[:min(n_dummy, len(self.dummy_feature_pool))]\n",
    "        return {\n",
    "            name: np.random.uniform(*self._ranges[name], n_samples)\n",
    "            for name in selected\n",
    "        }\n",
    "    \n",
    "    def compute_target(self, features: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Compute pressure: P = n * R * T / V\"\"\"\n",
    "        n = features['n']\n",
    "        T = features['T']\n",
    "        V = features['V']\n",
    "        return n * self.R * T / V\n",
    "    \n",
    "    def get_variable_dimensions(self, feature_names: List[str]) -> Dict[str, List[float]]:\n",
    "        \"\"\"Return dimensions for specified features.\"\"\"\n",
    "        return {name: self._dimensions[name] for name in feature_names if name in self._dimensions}\n",
    "    \n",
    "    def get_target_dimensions(self) -> List[float]:\n",
    "        \"\"\"Return dimensions for target (Pa = kg/(m*s^2)).\"\"\"\n",
    "        return self._target_dims.copy()\n",
    "    \n",
    "    def get_physical_bounds(self) -> Dict[str, Dict[str, Optional[float]]]:\n",
    "        \"\"\"Return physical bounds for all variables.\"\"\"\n",
    "        bounds = {'target': {'min': 0, 'max': None}}  # Pressure is non-negative\n",
    "        for name, (low, high) in self._ranges.items():\n",
    "            bounds[name] = {'min': low * 0.1, 'max': high * 10}\n",
    "        return bounds\n",
    "\n",
    "# Test instantiation\n",
    "eq3 = IdealGasEquation()\n",
    "print(f\"Equation 3: {eq3.full_name}\")\n",
    "print(f\"  Formula: {eq3.equation_str}\")\n",
    "print(f\"  Type: {eq3.equation_type}\")\n",
    "print(f\"  True features: {eq3.true_feature_names}\")\n",
    "print(f\"  Dummy pool: {eq3.dummy_feature_pool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Equation 4 - Damped Harmonic Oscillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EQUATION 4: DAMPED HARMONIC OSCILLATION\n",
    "# ==============================================================================\n",
    "\n",
    "class DampedOscillationEquation(BaseTestEquation):\n",
    "    \"\"\"\n",
    "    Damped Harmonic Oscillation Displacement.\n",
    "    \n",
    "    Equation: x(t) = A * exp(-gamma * t) * cos(omega * t)\n",
    "    \n",
    "    The solution to a damped harmonic oscillator, showing\n",
    "    exponential decay modulated by oscillation.\n",
    "    \n",
    "    Reference:\n",
    "        Feynman Lectures on Physics, Volume I, Chapter 24\n",
    "    \n",
    "    Characteristics:\n",
    "    - Type: Nested/Composite function (exp * cos)\n",
    "    - True variables: 4 (A, gamma, omega, t)\n",
    "    - Transcendental functions: Yes (exp, cos)\n",
    "    - Difficulty: HARD (tests discovery of nested structures)\n",
    "    \n",
    "    Note: This is the most challenging equation in the benchmark,\n",
    "    as it requires discovering nested transcendental functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes\n",
    "    name = \"damped\"\n",
    "    full_name = \"Damped Harmonic Oscillation\"\n",
    "    equation_str = \"x = A * exp(-gamma * t) * cos(omega * t)\"\n",
    "    equation_type = \"nested_transcendental\"\n",
    "    equation_index = 4\n",
    "    \n",
    "    true_feature_names = ['A', 'gamma', 'omega', 't']\n",
    "    dummy_feature_pool = ['m', 'k', 'v_0', 'F_ext', 'theta']\n",
    "    \n",
    "    # Dimensional information: [Mass, Length, Time, Temperature]\n",
    "    _dimensions = {\n",
    "        'A':     [0, 1, 0, 0],      # Initial amplitude (m)\n",
    "        'gamma': [0, 0, -1, 0],     # Damping coefficient (s^-1)\n",
    "        'omega': [0, 0, -1, 0],     # Angular frequency (rad/s, s^-1)\n",
    "        't':     [0, 0, 1, 0],      # Time (s)\n",
    "        'm':     [1, 0, 0, 0],      # Mass (kg)\n",
    "        'k':     [1, 0, -2, 0],     # Spring constant (N/m = kg/s^2)\n",
    "        'v_0':   [0, 1, -1, 0],     # Initial velocity (m/s)\n",
    "        'F_ext': [1, 1, -2, 0],     # External force (N)\n",
    "        'theta': [0, 0, 0, 0],      # Phase angle (rad, dimensionless)\n",
    "    }\n",
    "    \n",
    "    _target_dims = [0, 1, 0, 0]    # Displacement (m)\n",
    "    \n",
    "    # Physically realistic ranges\n",
    "    _ranges = {\n",
    "        'A':     (0.1, 10),         # Amplitude: 0.1 - 10 m\n",
    "        'gamma': (0.01, 1),         # Damping: 0.01 - 1 s^-1\n",
    "        'omega': (0.1, 10),         # Frequency: 0.1 - 10 rad/s\n",
    "        't':     (0, 10),           # Time: 0 - 10 s\n",
    "        'm':     (0.1, 10),         # Mass: 0.1 - 10 kg\n",
    "        'k':     (1, 100),          # Spring constant: 1 - 100 N/m\n",
    "        'v_0':   (0, 10),           # Initial velocity: 0 - 10 m/s\n",
    "        'F_ext': (0, 10),           # External force: 0 - 10 N\n",
    "        'theta': (0, 2*np.pi),      # Phase angle: 0 - 2*pi rad\n",
    "    }\n",
    "    \n",
    "    def generate_true_features(self, n_samples: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate amplitude, damping, frequency, and time.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        return {\n",
    "            'A':     np.random.uniform(*self._ranges['A'], n_samples),\n",
    "            'gamma': np.random.uniform(*self._ranges['gamma'], n_samples),\n",
    "            'omega': np.random.uniform(*self._ranges['omega'], n_samples),\n",
    "            't':     np.random.uniform(*self._ranges['t'], n_samples),\n",
    "        }\n",
    "    \n",
    "    def generate_dummy_features(self, n_samples: int, n_dummy: int, seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate irrelevant features (mass, spring constant, etc.).\"\"\"\n",
    "        if n_dummy <= 0:\n",
    "            return {}\n",
    "        \n",
    "        np.random.seed(seed + 1000)\n",
    "        \n",
    "        selected = self.dummy_feature_pool[:min(n_dummy, len(self.dummy_feature_pool))]\n",
    "        return {\n",
    "            name: np.random.uniform(*self._ranges[name], n_samples)\n",
    "            for name in selected\n",
    "        }\n",
    "    \n",
    "    def compute_target(self, features: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Compute displacement: x = A * exp(-gamma * t) * cos(omega * t)\"\"\"\n",
    "        A = features['A']\n",
    "        gamma = features['gamma']\n",
    "        omega = features['omega']\n",
    "        t = features['t']\n",
    "        return A * np.exp(-gamma * t) * np.cos(omega * t)\n",
    "    \n",
    "    def add_noise(self, y: np.ndarray, noise_level: float, seed: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Add additive Gaussian noise for damped oscillation.\n",
    "        \n",
    "        Unlike the other equations (which have strictly positive targets),\n",
    "        the damped oscillation can have negative values. We use additive\n",
    "        noise scaled to the signal amplitude.\n",
    "        \"\"\"\n",
    "        if noise_level <= 0:\n",
    "            return y.copy()\n",
    "        \n",
    "        np.random.seed(seed + 2000)\n",
    "        \n",
    "        # Additive Gaussian noise scaled to signal standard deviation\n",
    "        noise_std = noise_level * np.std(y)\n",
    "        noise = np.random.normal(0, noise_std, len(y))\n",
    "        return y + noise\n",
    "    \n",
    "    def get_variable_dimensions(self, feature_names: List[str]) -> Dict[str, List[float]]:\n",
    "        \"\"\"Return dimensions for specified features.\"\"\"\n",
    "        return {name: self._dimensions[name] for name in feature_names if name in self._dimensions}\n",
    "    \n",
    "    def get_target_dimensions(self) -> List[float]:\n",
    "        \"\"\"Return dimensions for target (m).\"\"\"\n",
    "        return self._target_dims.copy()\n",
    "    \n",
    "    def get_physical_bounds(self) -> Dict[str, Dict[str, Optional[float]]]:\n",
    "        \"\"\"Return physical bounds for all variables.\"\"\"\n",
    "        # Displacement can be positive or negative (oscillation)\n",
    "        bounds = {'target': {'min': None, 'max': None}}\n",
    "        for name, (low, high) in self._ranges.items():\n",
    "            bounds[name] = {'min': low * 0.1, 'max': high * 10}\n",
    "        return bounds\n",
    "\n",
    "# Test instantiation\n",
    "eq4 = DampedOscillationEquation()\n",
    "print(f\"Equation 4: {eq4.full_name}\")\n",
    "print(f\"  Formula: {eq4.equation_str}\")\n",
    "print(f\"  Type: {eq4.equation_type}\")\n",
    "print(f\"  True features: {eq4.true_feature_names}\")\n",
    "print(f\"  Dummy pool: {eq4.dummy_feature_pool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Equation Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EQUATION REGISTRY\n",
    "# ==============================================================================\n",
    "\n",
    "# Central registry of all test equations\n",
    "EQUATION_REGISTRY = {\n",
    "    'kk2000': KK2000Equation,\n",
    "    'newton': NewtonGravityEquation,\n",
    "    'ideal_gas': IdealGasEquation,\n",
    "    'damped': DampedOscillationEquation,\n",
    "}\n",
    "\n",
    "def get_equation(name: str) -> BaseTestEquation:\n",
    "    \"\"\"\n",
    "    Get equation instance by name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Equation name ('kk2000', 'newton', 'ideal_gas', 'damped')\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    BaseTestEquation\n",
    "        Instantiated equation object\n",
    "    \"\"\"\n",
    "    if name not in EQUATION_REGISTRY:\n",
    "        raise ValueError(f\"Unknown equation: {name}. Available: {list(EQUATION_REGISTRY.keys())}\")\n",
    "    return EQUATION_REGISTRY[name]()\n",
    "\n",
    "def get_all_equations() -> List[BaseTestEquation]:\n",
    "    \"\"\"\n",
    "    Get instances of all registered equations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[BaseTestEquation]\n",
    "        List of all equation instances in order\n",
    "    \"\"\"\n",
    "    return [cls() for cls in EQUATION_REGISTRY.values()]\n",
    "\n",
    "# Print summary\n",
    "print(\"Equation Registry Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for name, cls in EQUATION_REGISTRY.items():\n",
    "    eq = cls()\n",
    "    print(f\"  [{eq.equation_index}] {name:12s} | {eq.equation_type:22s} | True vars: {len(eq.true_feature_names)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Benchmark Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BENCHMARK DATA GENERATOR CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class BenchmarkDataGenerator:\n",
    "    \"\"\"\n",
    "    Generator for benchmark test datasets.\n",
    "    \n",
    "    This class handles:\n",
    "    - Dataset generation with configurable parameters\n",
    "    - Saving datasets to .npz format with full metadata\n",
    "    - Loading existing datasets\n",
    "    - Batch generation for all experimental configurations\n",
    "    \n",
    "    File naming convention:\n",
    "        eq{N}_{name}_n{samples}_noise{level:.2f}_dummy{count}.npz\n",
    "    \n",
    "    Example:\n",
    "        eq1_kk2000_n500_noise0.05_dummy5.npz\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: Union[str, Path] = DATA_DIR):\n",
    "        \"\"\"\n",
    "        Initialize the data generator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dir : str or Path\n",
    "            Directory to save generated datasets\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    def generate_dataset(\n",
    "        self,\n",
    "        equation: BaseTestEquation,\n",
    "        n_samples: int = DEFAULT_N_SAMPLES,\n",
    "        noise_level: float = 0.0,\n",
    "        n_dummy: int = 0,\n",
    "        seed: int = RANDOM_SEED\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a single benchmark dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        equation : BaseTestEquation\n",
    "            Equation object defining the true relationship\n",
    "        n_samples : int\n",
    "            Number of samples to generate\n",
    "        noise_level : float\n",
    "            Noise level (0.05 = 5%)\n",
    "        n_dummy : int\n",
    "            Number of irrelevant features to include\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Dataset dictionary with all data and metadata\n",
    "        \"\"\"\n",
    "        # Generate true features\n",
    "        true_features = equation.generate_true_features(n_samples, seed)\n",
    "        \n",
    "        # Generate dummy features\n",
    "        dummy_features = equation.generate_dummy_features(n_samples, n_dummy, seed)\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = {**true_features, **dummy_features}\n",
    "        \n",
    "        # Get feature order (true features first, then dummy)\n",
    "        feature_names = list(true_features.keys()) + list(dummy_features.keys())\n",
    "        \n",
    "        # Build feature matrix X\n",
    "        X = np.column_stack([all_features[name] for name in feature_names])\n",
    "        \n",
    "        # Compute noise-free target\n",
    "        y_true = equation.compute_target(true_features)\n",
    "        \n",
    "        # Add noise\n",
    "        y = equation.add_noise(y_true, noise_level, seed)\n",
    "        \n",
    "        # Get dimensional information\n",
    "        variable_dimensions = equation.get_variable_dimensions(feature_names)\n",
    "        target_dimensions = equation.get_target_dimensions()\n",
    "        physical_bounds = equation.get_physical_bounds()\n",
    "        \n",
    "        # Get ground truth\n",
    "        ground_truth = equation.get_ground_truth()\n",
    "        \n",
    "        # Build dataset dictionary\n",
    "        dataset = {\n",
    "            # Data arrays\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'y_true': y_true,\n",
    "            \n",
    "            # Feature information\n",
    "            'feature_names': np.array(feature_names, dtype=object),\n",
    "            'true_features': np.array(equation.true_feature_names, dtype=object),\n",
    "            'dummy_features': np.array(list(dummy_features.keys()), dtype=object),\n",
    "            \n",
    "            # Equation information\n",
    "            'equation_name': equation.name,\n",
    "            'equation_index': equation.equation_index,\n",
    "            'equation_str': equation.equation_str,\n",
    "            'equation_type': equation.equation_type,\n",
    "            'full_name': equation.full_name,\n",
    "            \n",
    "            # Experiment parameters\n",
    "            'n_samples': n_samples,\n",
    "            'noise_level': noise_level,\n",
    "            'n_dummy': n_dummy,\n",
    "            'seed': seed,\n",
    "            \n",
    "            # Dimensional information (for UserInputs)\n",
    "            'variable_dimensions': variable_dimensions,\n",
    "            'target_dimensions': np.array(target_dimensions),\n",
    "            'physical_bounds': physical_bounds,\n",
    "        }\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_filename(\n",
    "        self,\n",
    "        equation: BaseTestEquation,\n",
    "        n_samples: int,\n",
    "        noise_level: float,\n",
    "        n_dummy: int\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate standardized filename for a dataset.\n",
    "        \n",
    "        Format: eq{N}_{name}_n{samples}_noise{level:.2f}_dummy{count}.npz\n",
    "        \"\"\"\n",
    "        return f\"eq{equation.equation_index}_{equation.name}_n{n_samples}_noise{noise_level:.2f}_dummy{n_dummy}.npz\"\n",
    "    \n",
    "    def save_dataset(self, dataset: Dict[str, Any], filename: str) -> Path:\n",
    "        \"\"\"\n",
    "        Save dataset to .npz file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dict[str, Any]\n",
    "            Dataset dictionary from generate_dataset()\n",
    "        filename : str\n",
    "            Output filename (without path)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Path\n",
    "            Full path to saved file\n",
    "        \"\"\"\n",
    "        filepath = self.output_dir / filename\n",
    "        \n",
    "        # Prepare data for saving\n",
    "        # Dictionaries need to be pickled\n",
    "        save_dict = {}\n",
    "        for key, value in dataset.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Pickle dictionaries to bytes, then store as array\n",
    "                save_dict[key + '_pkl'] = np.frombuffer(pickle.dumps(value), dtype=np.uint8)\n",
    "            else:\n",
    "                save_dict[key] = value\n",
    "        \n",
    "        np.savez(filepath, **save_dict)\n",
    "        return filepath\n",
    "    \n",
    "    def load_dataset(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Load dataset from .npz file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            Filename (with or without path)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Loaded dataset dictionary\n",
    "        \"\"\"\n",
    "        # Check if full path or just filename\n",
    "        filepath = Path(filename)\n",
    "        if not filepath.exists():\n",
    "            filepath = self.output_dir / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"Dataset not found: {filepath}\")\n",
    "        \n",
    "        # Load npz file\n",
    "        data = np.load(filepath, allow_pickle=True)\n",
    "        \n",
    "        # Reconstruct dictionary\n",
    "        dataset = {}\n",
    "        for key in data.files:\n",
    "            if key.endswith('_pkl'):\n",
    "                # Unpickle dictionaries\n",
    "                original_key = key[:-4]\n",
    "                dataset[original_key] = pickle.loads(data[key].tobytes())\n",
    "            else:\n",
    "                value = data[key]\n",
    "                # Convert 0-d arrays to scalars\n",
    "                if value.ndim == 0:\n",
    "                    dataset[key] = value.item()\n",
    "                else:\n",
    "                    dataset[key] = value\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def generate_and_save(\n",
    "        self,\n",
    "        equation: BaseTestEquation,\n",
    "        n_samples: int = DEFAULT_N_SAMPLES,\n",
    "        noise_level: float = 0.0,\n",
    "        n_dummy: int = 0,\n",
    "        seed: int = RANDOM_SEED\n",
    "    ) -> Path:\n",
    "        \"\"\"\n",
    "        Generate and save a dataset in one call.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Path\n",
    "            Path to saved file\n",
    "        \"\"\"\n",
    "        dataset = self.generate_dataset(equation, n_samples, noise_level, n_dummy, seed)\n",
    "        filename = self.get_filename(equation, n_samples, noise_level, n_dummy)\n",
    "        return self.save_dataset(dataset, filename)\n",
    "\n",
    "print(\"BenchmarkDataGenerator class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Generate Core Experiment Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GENERATE CORE EXPERIMENT DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_core_datasets(generator: BenchmarkDataGenerator, verbose: bool = True) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Generate all datasets for core experiments.\n",
    "    \n",
    "    Core experiments: 4 equations x 2 noise x 2 dummy = 16 configurations\n",
    "    Each with n=500 samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    generator : BenchmarkDataGenerator\n",
    "        Data generator instance\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Path]\n",
    "        List of generated file paths\n",
    "    \"\"\"\n",
    "    generated_files = []\n",
    "    equations = get_all_equations()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Generating Core Experiment Datasets\")\n",
    "        print(\"=\" * 60)\n",
    "        total = len(equations) * len(NOISE_LEVELS) * len(DUMMY_COUNTS)\n",
    "        print(f\"Total datasets: {total}\")\n",
    "        print()\n",
    "    \n",
    "    count = 0\n",
    "    for eq in equations:\n",
    "        for noise_level in NOISE_LEVELS:\n",
    "            for n_dummy in DUMMY_COUNTS:\n",
    "                # Generate and save\n",
    "                filepath = generator.generate_and_save(\n",
    "                    equation=eq,\n",
    "                    n_samples=DEFAULT_N_SAMPLES,\n",
    "                    noise_level=noise_level,\n",
    "                    n_dummy=n_dummy,\n",
    "                    seed=RANDOM_SEED\n",
    "                )\n",
    "                generated_files.append(filepath)\n",
    "                count += 1\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  [{count:3d}] {filepath.name}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(f\"Generated {len(generated_files)} core datasets.\")\n",
    "    \n",
    "    return generated_files\n",
    "\n",
    "# Generate core datasets\n",
    "generator = BenchmarkDataGenerator(DATA_DIR)\n",
    "core_files = generate_core_datasets(generator, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Generate Supplementary Experiment Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GENERATE SUPPLEMENTARY EXPERIMENT DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_supplementary_datasets(generator: BenchmarkDataGenerator, verbose: bool = True) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Generate datasets for supplementary experiments (sample size sensitivity).\n",
    "    \n",
    "    Supplementary: 4 equations x 2 sizes (250, 750) = 8 configurations\n",
    "    Fixed: noise=5%, dummy=5\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    generator : BenchmarkDataGenerator\n",
    "        Data generator instance\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Path]\n",
    "        List of generated file paths\n",
    "    \"\"\"\n",
    "    generated_files = []\n",
    "    equations = get_all_equations()\n",
    "    \n",
    "    # Fixed parameters for supplementary experiments\n",
    "    fixed_noise = 0.05\n",
    "    fixed_dummy = 5\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nGenerating Supplementary Experiment Datasets\")\n",
    "        print(\"=\" * 60)\n",
    "        total = len(equations) * len(SAMPLE_SIZES_SUPPLEMENTARY)\n",
    "        print(f\"Total datasets: {total}\")\n",
    "        print(f\"Fixed parameters: noise={fixed_noise}, dummy={fixed_dummy}\")\n",
    "        print()\n",
    "    \n",
    "    count = 0\n",
    "    for eq in equations:\n",
    "        for n_samples in SAMPLE_SIZES_SUPPLEMENTARY:\n",
    "            # Generate and save\n",
    "            filepath = generator.generate_and_save(\n",
    "                equation=eq,\n",
    "                n_samples=n_samples,\n",
    "                noise_level=fixed_noise,\n",
    "                n_dummy=fixed_dummy,\n",
    "                seed=RANDOM_SEED\n",
    "            )\n",
    "            generated_files.append(filepath)\n",
    "            count += 1\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  [{count:3d}] {filepath.name}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(f\"Generated {len(generated_files)} supplementary datasets.\")\n",
    "    \n",
    "    return generated_files\n",
    "\n",
    "# Generate supplementary datasets\n",
    "supp_files = generate_supplementary_datasets(generator, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Data Verification and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VERIFY GENERATED DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "def verify_datasets(generator: BenchmarkDataGenerator, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Verify all generated datasets and create summary table.\n",
    "    \"\"\"\n",
    "    # List all npz files\n",
    "    files = sorted(generator.output_dir.glob('*.npz'))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nDataset Verification\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Found {len(files)} dataset files in {generator.output_dir}\")\n",
    "        print()\n",
    "    \n",
    "    # Build summary\n",
    "    summary_data = []\n",
    "    \n",
    "    for filepath in files:\n",
    "        try:\n",
    "            dataset = generator.load_dataset(filepath.name)\n",
    "            \n",
    "            # Extract info\n",
    "            row = {\n",
    "                'filename': filepath.name,\n",
    "                'equation': dataset['equation_name'],\n",
    "                'n_samples': dataset['n_samples'],\n",
    "                'noise_level': dataset['noise_level'],\n",
    "                'n_dummy': dataset['n_dummy'],\n",
    "                'n_features': dataset['X'].shape[1],\n",
    "                'n_true': len(dataset['true_features']),\n",
    "                'y_mean': np.mean(dataset['y']),\n",
    "                'y_std': np.std(dataset['y']),\n",
    "                'y_min': np.min(dataset['y']),\n",
    "                'y_max': np.max(dataset['y']),\n",
    "                'has_dims': 'variable_dimensions' in dataset,\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR loading {filepath.name}: {e}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    if verbose and len(summary_df) > 0:\n",
    "        # Print summary statistics\n",
    "        print(\"Summary by Equation:\")\n",
    "        eq_summary = summary_df.groupby('equation').agg({\n",
    "            'filename': 'count',\n",
    "            'n_samples': 'mean',\n",
    "            'n_features': 'mean',\n",
    "        }).round(1)\n",
    "        eq_summary.columns = ['Count', 'Avg Samples', 'Avg Features']\n",
    "        print(eq_summary.to_string())\n",
    "        print()\n",
    "        \n",
    "        print(\"Summary by Configuration:\")\n",
    "        config_summary = summary_df.groupby(['noise_level', 'n_dummy']).size().unstack(fill_value=0)\n",
    "        print(config_summary.to_string())\n",
    "        print()\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Verify datasets\n",
    "summary_df = verify_datasets(generator, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DISPLAY FULL DATASET SUMMARY TABLE\n",
    "# ==============================================================================\n",
    "\n",
    "# Display formatted table\n",
    "print(\"\\nComplete Dataset Summary:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Format for display\n",
    "display_df = summary_df[['filename', 'equation', 'n_samples', 'noise_level', \n",
    "                          'n_dummy', 'n_features', 'n_true']].copy()\n",
    "display_df['noise_level'] = display_df['noise_level'].map(lambda x: f\"{x:.0%}\")\n",
    "\n",
    "# Print\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "print(f\"Total datasets: {len(summary_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Sample Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZE SAMPLE DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_sample_datasets(generator: BenchmarkDataGenerator):\n",
    "    \"\"\"\n",
    "    Create visualization of sample datasets for each equation.\n",
    "    \"\"\"\n",
    "    equations = get_all_equations()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    fig.suptitle('Sample Dataset Visualization (n=500, noise=5%, dummy=5)', fontsize=14)\n",
    "    \n",
    "    for i, eq in enumerate(equations):\n",
    "        # Load dataset with noise and dummy features\n",
    "        filename = generator.get_filename(eq, 500, 0.05, 5)\n",
    "        dataset = generator.load_dataset(filename)\n",
    "        \n",
    "        X = dataset['X']\n",
    "        y = dataset['y']\n",
    "        y_true = dataset['y_true']\n",
    "        feature_names = list(dataset['feature_names'])\n",
    "        \n",
    "        # Top row: True vs Noisy target\n",
    "        ax1 = axes[0, i]\n",
    "        ax1.scatter(y_true, y, alpha=0.3, s=5, c='blue')\n",
    "        ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \n",
    "                 'r--', linewidth=1, label='y=y_true')\n",
    "        ax1.set_xlabel('y_true')\n",
    "        ax1.set_ylabel('y (noisy)')\n",
    "        ax1.set_title(f'{eq.name}\\n{eq.equation_type}')\n",
    "        ax1.legend(loc='upper left', fontsize=8)\n",
    "        \n",
    "        # Calculate R^2 between noisy and true\n",
    "        ss_res = np.sum((y - y_true) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot\n",
    "        ax1.text(0.95, 0.05, f'R^2={r2:.4f}', transform=ax1.transAxes, \n",
    "                 fontsize=8, ha='right', va='bottom')\n",
    "        \n",
    "        # Bottom row: Feature importance (correlation with target)\n",
    "        ax2 = axes[1, i]\n",
    "        correlations = [np.corrcoef(X[:, j], y)[0, 1] for j in range(X.shape[1])]\n",
    "        colors = ['green' if name in eq.true_feature_names else 'red' \n",
    "                  for name in feature_names]\n",
    "        \n",
    "        bars = ax2.bar(range(len(feature_names)), np.abs(correlations), color=colors, alpha=0.7)\n",
    "        ax2.set_xticks(range(len(feature_names)))\n",
    "        ax2.set_xticklabels(feature_names, rotation=45, ha='right', fontsize=8)\n",
    "        ax2.set_ylabel('|Correlation with y|')\n",
    "        ax2.set_title(f'Feature Correlations\\nGreen=True, Red=Dummy')\n",
    "        ax2.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = generator.output_dir.parent / 'results' / 'figures' / 'data_overview.png'\n",
    "    fig_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nFigure saved to: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate visualization\n",
    "plot_sample_datasets(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13: Test Data Loading Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEST DATA LOADING INTERFACE\n",
    "# ==============================================================================\n",
    "\n",
    "def test_data_interface():\n",
    "    \"\"\"\n",
    "    Test the data loading interface to ensure datasets can be\n",
    "    properly loaded and used for experiments.\n",
    "    \"\"\"\n",
    "    print(\"Testing Data Loading Interface\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load a sample dataset\n",
    "    generator = BenchmarkDataGenerator(DATA_DIR)\n",
    "    filename = 'eq1_kk2000_n500_noise0.05_dummy5.npz'\n",
    "    \n",
    "    print(f\"\\nLoading: {filename}\")\n",
    "    dataset = generator.load_dataset(filename)\n",
    "    \n",
    "    # Print structure\n",
    "    print(\"\\nDataset Structure:\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in dataset.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"  {key:25s} : ndarray{value.shape} dtype={value.dtype}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"  {key:25s} : dict with {len(value)} keys\")\n",
    "        else:\n",
    "            print(f\"  {key:25s} : {type(value).__name__} = {value}\")\n",
    "    \n",
    "    # Verify dimensions\n",
    "    print(\"\\nDimensional Information:\")\n",
    "    print(\"-\" * 40)\n",
    "    for var, dims in dataset['variable_dimensions'].items():\n",
    "        print(f\"  {var:10s} : [M={dims[0]:+.0f}, L={dims[1]:+.0f}, T={dims[2]:+.0f}, Th={dims[3]:+.0f}]\")\n",
    "    print(f\"  {'target':10s} : [M={dataset['target_dimensions'][0]:+.0f}, L={dataset['target_dimensions'][1]:+.0f}, T={dataset['target_dimensions'][2]:+.0f}, Th={dataset['target_dimensions'][3]:+.0f}]\")\n",
    "    \n",
    "    print(\"\\nData loading interface test PASSED.\")\n",
    "\n",
    "# Run test\n",
    "test_data_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 14: Export Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# UTILITY FUNCTIONS FOR EXPERIMENTS NOTEBOOK\n",
    "# ==============================================================================\n",
    "\n",
    "def list_all_datasets(data_dir: Union[str, Path] = DATA_DIR) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    List all available datasets with their configurations.\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    files = sorted(data_dir.glob('*.npz'))\n",
    "    \n",
    "    records = []\n",
    "    for f in files:\n",
    "        # Parse filename: eq{N}_{name}_n{samples}_noise{level}_dummy{count}.npz\n",
    "        parts = f.stem.split('_')\n",
    "        eq_idx = int(parts[0][2:])  # eq1 -> 1\n",
    "        eq_name = parts[1]\n",
    "        n_samples = int(parts[2][1:])  # n500 -> 500\n",
    "        noise = float(parts[3][5:])  # noise0.05 -> 0.05\n",
    "        n_dummy = int(parts[4][5:])  # dummy5 -> 5\n",
    "        \n",
    "        # Determine experiment type\n",
    "        if n_samples == DEFAULT_N_SAMPLES:\n",
    "            exp_type = 'core'\n",
    "        else:\n",
    "            exp_type = 'supplementary'\n",
    "        \n",
    "        records.append({\n",
    "            'filename': f.name,\n",
    "            'filepath': str(f),\n",
    "            'equation': eq_name,\n",
    "            'equation_index': eq_idx,\n",
    "            'n_samples': n_samples,\n",
    "            'noise_level': noise,\n",
    "            'n_dummy': n_dummy,\n",
    "            'experiment_type': exp_type,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def get_core_experiment_configs() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get all configurations for core experiments.\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    \n",
    "    for eq in equations:\n",
    "        for noise in NOISE_LEVELS:\n",
    "            for dummy in DUMMY_COUNTS:\n",
    "                for with_dims in [True, False]:\n",
    "                    configs.append({\n",
    "                        'equation_name': eq,\n",
    "                        'n_samples': DEFAULT_N_SAMPLES,\n",
    "                        'noise_level': noise,\n",
    "                        'n_dummy': dummy,\n",
    "                        'with_dims': with_dims,\n",
    "                    })\n",
    "    \n",
    "    return configs\n",
    "\n",
    "def get_supplementary_experiment_configs() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get all configurations for supplementary experiments.\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    \n",
    "    for eq in equations:\n",
    "        for n_samples in SAMPLE_SIZES_SUPPLEMENTARY:\n",
    "            configs.append({\n",
    "                'equation_name': eq,\n",
    "                'n_samples': n_samples,\n",
    "                'noise_level': 0.05,\n",
    "                'n_dummy': 5,\n",
    "                'with_dims': True,\n",
    "            })\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Print summary\n",
    "print(\"Utility Functions Defined:\")\n",
    "print(\"  - list_all_datasets(data_dir) -> DataFrame\")\n",
    "print(\"  - get_core_experiment_configs() -> List[Dict]\")\n",
    "print(\"  - get_supplementary_experiment_configs() -> List[Dict]\")\n",
    "print()\n",
    "print(f\"Core experiments: {len(get_core_experiment_configs())} configurations\")\n",
    "print(f\"Supplementary experiments: {len(get_supplementary_experiment_configs())} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 15: Final Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY AND VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA GENERATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Count files\n",
    "all_files = list(DATA_DIR.glob('*.npz'))\n",
    "print(f\"Total datasets generated: {len(all_files)}\")\n",
    "print(f\"Output directory: {DATA_DIR.resolve()}\")\n",
    "print()\n",
    "\n",
    "# List all generated files\n",
    "print(\"Generated files:\")\n",
    "for f in sorted(all_files):\n",
    "    print(f\"  - {f.name}\")\n",
    "print()\n",
    "\n",
    "# Expected vs actual\n",
    "expected_core = 4 * 2 * 2  # 4 eq x 2 noise x 2 dummy = 16\n",
    "expected_supp = 4 * 2      # 4 eq x 2 sizes = 8\n",
    "expected_total = expected_core + expected_supp\n",
    "\n",
    "print(f\"Expected datasets: {expected_total} ({expected_core} core + {expected_supp} supplementary)\")\n",
    "print(f\"Actual datasets:   {len(all_files)}\")\n",
    "print()\n",
    "\n",
    "if len(all_files) == expected_total:\n",
    "    print(\"[PASSED] All expected datasets generated successfully!\")\n",
    "elif len(all_files) > 0:\n",
    "    print(f\"[OK] Generated {len(all_files)} datasets.\")\n",
    "else:\n",
    "    print(\"[WARNING] No datasets found!\")\n",
    "\n",
    "# Validate one sample file\n",
    "if all_files:\n",
    "    sample_file = all_files[0]\n",
    "    data = np.load(sample_file)\n",
    "    print()\n",
    "    print(f\"Sample file validation ({sample_file.name}):\")\n",
    "    print(f\"  Keys: {list(data.keys())}\")\n",
    "    print(f\"  X shape: {data['X'].shape}\")\n",
    "    print(f\"  y shape: {data['y'].shape}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"Ready for Experiments.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Quick Reference\n",
    "\n",
    "### Equation Summary\n",
    "\n",
    "| # | Name | Formula | Type | True Vars | Difficulty |\n",
    "|---|------|---------|------|-----------|------------|\n",
    "| 1 | KK2000 | P = 1350*q_c^2.47*N_d^-1.79 | power_law | 2 | Medium |\n",
    "| 2 | Newton | F = G*m1*m2/r^2 | rational | 3 | Easy |\n",
    "| 3 | Ideal Gas | P = nRT/V | rational | 3 | Easy |\n",
    "| 4 | Damped | x = A*exp(-gamma*t)*cos(omega*t) | nested | 4 | Hard |\n",
    "\n",
    "### File Naming Convention\n",
    "\n",
    "```\n",
    "eq{N}_{name}_n{samples}_noise{level:.2f}_dummy{count}.npz\n",
    "```\n",
    "\n",
    "### Dataset Contents\n",
    "\n",
    "- `X`: Feature matrix (n_samples, n_features)\n",
    "- `y`: Target with noise\n",
    "- `y_true`: Noise-free target\n",
    "- `feature_names`: List of feature names\n",
    "- `true_features`: Features in true equation\n",
    "- `dummy_features`: Irrelevant features\n",
    "- `variable_dimensions`: Dict for UserInputs\n",
    "- `target_dimensions`: [M, L, T, Theta]\n",
    "- `physical_bounds`: Physical constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
