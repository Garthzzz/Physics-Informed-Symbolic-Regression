{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments - Physics-SR Framework v3.0 Benchmark\n",
    "\n",
    "## Benchmark Experiment Execution Module\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook executes all benchmark experiments for the Physics-SR Framework v3.0:\n",
    "\n",
    "1. **Method Runners**: Wrappers for Physics-SR, PySR-Only, and LASSO+PySR\n",
    "2. **Evaluation Functions**: Variable selection, equation recovery, prediction metrics\n",
    "3. **Experiment Runner**: Orchestrates all experiments with checkpointing\n",
    "4. **Results Collection**: Saves to CSV and PKL formats\n",
    "\n",
    "### Experimental Design\n",
    "\n",
    "**Core Experiments (96 runs):**\n",
    "- 4 equations x 2 noise (0%, 5%) x 2 dummy (0, 5) x 2 dims (T/F) x 3 methods = 96\n",
    "\n",
    "**Supplementary Experiments (8 runs):**\n",
    "- 4 equations x 2 sample sizes (250, 750) x Physics-SR only = 8\n",
    "\n",
    "### Methods Compared\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| Physics-SR | Full 3-stage framework with physics knowledge |\n",
    "| PySR-Only | Genetic programming baseline (no preprocessing) |\n",
    "| LASSO+PySR | LASSO feature selection + PySR on selected features |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENVIRONMENT RESET AND FRESH CLONE\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    import shutil\n",
    "    import gc\n",
    "    \n",
    "    # CRITICAL: Change to /content FIRST\n",
    "    try:\n",
    "        os.chdir('/content')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    \n",
    "    # Remove existing repository if present\n",
    "    repo_path = '/content/Physics-Informed-Symbolic-Regression'\n",
    "    if os.path.exists(repo_path):\n",
    "        shutil.rmtree(repo_path)\n",
    "        print(\"[OK] Removed existing repository.\")\n",
    "    \n",
    "    # Clone fresh repository\n",
    "    !git clone https://github.com/Garthzzz/Physics-Informed-Symbolic-Regression.git\n",
    "    \n",
    "    # Verify clone succeeded\n",
    "    if os.path.exists(repo_path):\n",
    "        print(\"[OK] Fresh repository cloned.\")\n",
    "        \n",
    "        # Change to benchmark directory\n",
    "        os.chdir(repo_path + '/benchmark')\n",
    "        print(f\"[OK] Working directory: {os.getcwd()}\")\n",
    "        \n",
    "        # Verify\n",
    "        !git log --oneline -3\n",
    "    else:\n",
    "        print(\"[FAIL] Clone failed!\")\n",
    "    \n",
    "    print()\n",
    "    print(\"[OK] Environment reset complete.\")\n",
    "else:\n",
    "    print(\"[INFO] Not in Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# COLAB SETUP - Run this cell first!\n",
    "# ==============================================================================\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    if not os.path.exists('/content/Physics-Informed-Symbolic-Regression'):\n",
    "        !git clone https://github.com/Garthzzz/Physics-Informed-Symbolic-Regression.git\n",
    "        print(\"Repository cloned!\")\n",
    "    \n",
    "    %cd /content/Physics-Informed-Symbolic-Regression\n",
    "    \n",
    "    # Install PySR\n",
    "    !pip install -q pysr\n",
    "    import pysr\n",
    "    pysr.install()\n",
    "    \n",
    "    # Verify data files\n",
    "    from pathlib import Path\n",
    "    data_files = list(Path('benchmark/data').glob('*.npz'))\n",
    "    print(f\"\\nFound {len(data_files)} data files\")\n",
    "    \n",
    "    if len(data_files) == 24:\n",
    "        print(\"[OK] All data files present!\")\n",
    "    else:\n",
    "        print(\"[WARNING] Expected 24 files\")\n",
    "    \n",
    "    print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiments.ipynb - Benchmark Experiment Execution Module\n",
    "==========================================================\n",
    "\n",
    "Physics-SR Framework v3.0 Benchmark Suite\n",
    "\n",
    "This module provides:\n",
    "- Method runners for Physics-SR, PySR-Only, LASSO+PySR\n",
    "- Evaluation functions for variable selection and prediction\n",
    "- Experiment orchestration with checkpointing\n",
    "- Results collection and storage\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV, Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Progress bar\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Experiments: All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Determine paths based on environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Colab paths\n",
    "    BASE_DIR = Path('/content/Physics-Informed-Symbolic-Regression')\n",
    "    ALGORITHMS_DIR = BASE_DIR / 'algorithms'\n",
    "    BENCHMARK_DIR = BASE_DIR / 'benchmark'\n",
    "else:\n",
    "    # Local paths\n",
    "    BENCHMARK_DIR = Path('.').resolve()\n",
    "    ALGORITHMS_DIR = BENCHMARK_DIR.parent / 'algorithms'\n",
    "    BASE_DIR = BENCHMARK_DIR.parent\n",
    "\n",
    "DATA_DIR = BENCHMARK_DIR / 'data'\n",
    "RESULTS_DIR = BENCHMARK_DIR / 'results'\n",
    "\n",
    "# Create directories if needed\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Algorithms directory: {ALGORITHMS_DIR}\")\n",
    "print(f\"Benchmark directory: {BENCHMARK_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "\n",
    "# Verify data files\n",
    "data_files = list(DATA_DIR.glob('*.npz'))\n",
    "print(f\"Found {len(data_files)} data files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORT ALGORITHM MODULES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading algorithm modules...\")\n",
    "print()\n",
    "\n",
    "# Change to algorithms directory for relative imports\n",
    "original_dir = os.getcwd()\n",
    "os.chdir(ALGORITHMS_DIR)\n",
    "\n",
    "# Import all algorithm notebooks\n",
    "%run 00_Core.ipynb\n",
    "%run 01_BuckinghamPi.ipynb\n",
    "%run 02_VariableScreening.ipynb\n",
    "%run 03_SymmetryAnalysis.ipynb\n",
    "%run 04_InteractionDiscovery.ipynb\n",
    "%run 05_FeatureLibrary.ipynb\n",
    "%run 06_PySR.ipynb\n",
    "%run 07_EWSINDy_STLSQ.ipynb\n",
    "%run 08_AdaptiveLasso.ipynb\n",
    "%run 09_ModelSelection.ipynb\n",
    "%run 10_PhysicsVerification.ipynb\n",
    "%run 11_UQ_Inference.ipynb\n",
    "%run 12_Full_Pipeline.ipynb\n",
    "\n",
    "# Change back to benchmark directory\n",
    "os.chdir(original_dir)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\" All algorithm modules loaded successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORT DATA GENERATION UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading DataGen utilities...\")\n",
    "\n",
    "# Change to benchmark directory and run DataGen\n",
    "%cd {BENCHMARK_DIR}\n",
    "%run DataGen.ipynb\n",
    "\n",
    "print()\n",
    "print(\"DataGen utilities loaded.\")\n",
    "print(f\"Available equations: {list(EQUATION_REGISTRY.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DEBUG: CHECK FILESYSTEM BEFORE\n",
    "# ==============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== FILESYSTEM CHECK ===\")\n",
    "print(f\"CWD: {os.getcwd()}\")\n",
    "\n",
    "repo = Path('/content/Physics-Informed-Symbolic-Regression')\n",
    "if repo.exists():\n",
    "    print(f\"Repo exists: True\")\n",
    "    print(f\"benchmark contents: {list((repo / 'benchmark').iterdir())}\")\n",
    "else:\n",
    "    print(\"Repo exists: False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPERIMENT CONFIGURATION CONSTANTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Random seed for reproducibility\n",
    "EXPERIMENT_SEED = 42\n",
    "\n",
    "# Train/test split ratio\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Methods to compare\n",
    "METHODS = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "\n",
    "# PySR configuration (reduced for faster benchmarking)\n",
    "PYSR_CONFIG = {\n",
    "    'niterations': 40,\n",
    "    'maxsize': 20,\n",
    "    'timeout_in_seconds': 90,\n",
    "    'populations': 15,\n",
    "}\n",
    "\n",
    "# Physics-SR pipeline configuration\n",
    "PHYSICS_SR_CONFIG = {\n",
    "    'screening_threshold': 0.8,\n",
    "    'power_law_r2_threshold': 0.9,\n",
    "    'interaction_stability': 0.5,\n",
    "    'max_poly_degree': 3,\n",
    "    'stlsq_threshold': 0.1,\n",
    "    'pysr_maxsize': PYSR_CONFIG['maxsize'],\n",
    "    'pysr_niterations': PYSR_CONFIG['niterations'],\n",
    "    'cv_folds': 5,\n",
    "    'ebic_gamma': 0.5,\n",
    "    'n_bootstrap': 50,\n",
    "    'confidence_level': 0.95\n",
    "}\n",
    "\n",
    "# LASSO configuration\n",
    "LASSO_CONFIG = {\n",
    "    'cv': 5,\n",
    "    'max_iter': 10000,\n",
    "}\n",
    "\n",
    "print(\"Experiment configuration loaded.\")\n",
    "print(f\"Methods: {METHODS}\")\n",
    "print(f\"Test size: {TEST_SIZE}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Method Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD RESULT DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class MethodResult:\n",
    "    \"\"\"\n",
    "    Standardized result container for all method runners.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    method_name : str\n",
    "        Name of the method\n",
    "    discovered_features : List[str]\n",
    "        Features selected/used by the method\n",
    "    equation : str\n",
    "        Discovered equation (string representation)\n",
    "    predictions : np.ndarray\n",
    "        Predictions on training data\n",
    "    runtime_seconds : float\n",
    "        Total runtime in seconds\n",
    "    success : bool\n",
    "        Whether the method completed successfully\n",
    "    error_message : Optional[str]\n",
    "        Error message if method failed\n",
    "    method_specific : Dict\n",
    "        Additional method-specific results\n",
    "    \"\"\"\n",
    "    method_name: str\n",
    "    discovered_features: List[str]\n",
    "    equation: str\n",
    "    predictions: np.ndarray\n",
    "    runtime_seconds: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "    method_specific: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "print(\"MethodResult dataclass defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PHYSICS-SR RUNNER\n",
    "# ==============================================================================\n",
    "\n",
    "class PhysicsSRRunner:\n",
    "    \"\"\"\n",
    "    Runner for the complete Physics-SR pipeline.\n",
    "    \n",
    "    Uses the full 3-stage framework with optional dimensional information.\n",
    "    \"\"\"\n",
    "    \n",
    "    method_name = \"physics_sr\"\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize PhysicsSRRunner.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Optional[Dict]\n",
    "            Pipeline configuration. Uses PHYSICS_SR_CONFIG if not specified.\n",
    "        \"\"\"\n",
    "        self.config = config or PHYSICS_SR_CONFIG.copy()\n",
    "    \n",
    "    def run(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str],\n",
    "        user_inputs: Optional[UserInputs] = None,\n",
    "        with_dims: bool = True\n",
    "    ) -> MethodResult:\n",
    "        \"\"\"\n",
    "        Run Physics-SR pipeline.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Modify user_inputs if with_dims=False\n",
    "            if not with_dims and user_inputs is not None:\n",
    "                # Create UserInputs with all-zero dimensions (dimensionless)\n",
    "                user_inputs_modified = UserInputs(\n",
    "                    variable_dimensions={name: [0, 0, 0, 0] for name in feature_names},\n",
    "                    target_dimensions=[0, 0, 0, 0],\n",
    "                    physical_bounds=user_inputs.physical_bounds\n",
    "                )\n",
    "            else:\n",
    "                user_inputs_modified = user_inputs\n",
    "            \n",
    "            # Create and run pipeline\n",
    "            pipeline = PhysicsSRPipeline(config=self.config)\n",
    "            result = pipeline.run(X, y, feature_names, user_inputs_modified)\n",
    "            \n",
    "            # Extract results\n",
    "            discovered_features = self._extract_features(result)\n",
    "            equation = result.get('final_equation', 'Not discovered')\n",
    "            predictions = self._get_predictions(result, X, y, feature_names)\n",
    "            \n",
    "            runtime = time.time() - start_time\n",
    "            \n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=discovered_features,\n",
    "                equation=str(equation),\n",
    "                predictions=predictions,\n",
    "                runtime_seconds=runtime,\n",
    "                success=True,\n",
    "                method_specific={\n",
    "                    'stage1': result.get('stage1', {}),\n",
    "                    'stage2': result.get('stage2', {}),\n",
    "                    'stage3': result.get('stage3', {}),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            runtime = time.time() - start_time\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=[],\n",
    "                equation='ERROR',\n",
    "                predictions=np.zeros(len(y)),\n",
    "                runtime_seconds=runtime,\n",
    "                success=False,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "    \n",
    "    def _extract_features(self, result: Dict) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract discovered features from pipeline result.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            stage1 = result.get('stage1', {})\n",
    "            screening = stage1.get('screening', {})\n",
    "            return screening.get('selected_features', [])\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def _get_predictions(\n",
    "        self, \n",
    "        result: Dict, \n",
    "        X: np.ndarray, \n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get predictions from pipeline result.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            stage3 = result.get('stage3', {})\n",
    "            best_model = stage3.get('best_model', {})\n",
    "            preds = best_model.get('predictions', None)\n",
    "            if preds is not None:\n",
    "                return preds\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Fallback: simple Ridge prediction on discovered features\n",
    "        discovered = self._extract_features(result)\n",
    "        if len(discovered) > 0:\n",
    "            try:\n",
    "                indices = [feature_names.index(f) for f in discovered if f in feature_names]\n",
    "                if len(indices) > 0:\n",
    "                    model = Ridge(alpha=0.1)\n",
    "                    model.fit(X[:, indices], y)\n",
    "                    return model.predict(X[:, indices])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return np.zeros(X.shape[0])\n",
    "\n",
    "\n",
    "print(\"PhysicsSRRunner defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PYSR-ONLY RUNNER\n",
    "# ==============================================================================\n",
    "\n",
    "class PySROnlyRunner:\n",
    "    \"\"\"\n",
    "    Runner for PySR-only baseline.\n",
    "    \n",
    "    Uses PySR genetic programming directly on raw data without preprocessing.\n",
    "    This serves as a baseline to show the benefit of the Physics-SR framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    method_name = \"pysr_only\"\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize PySROnlyRunner.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        config : Optional[Dict]\n",
    "            PySR configuration. Uses PYSR_CONFIG if not specified.\n",
    "        \"\"\"\n",
    "        self.config = config or PYSR_CONFIG.copy()\n",
    "    \n",
    "    def run(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str],\n",
    "        user_inputs: Optional[UserInputs] = None,\n",
    "        with_dims: bool = True\n",
    "    ) -> MethodResult:\n",
    "        \"\"\"\n",
    "        Run PySR directly on data.\n",
    "        \n",
    "        Note: with_dims is ignored for this baseline.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Use PySRDiscoverer from 06_PySR\n",
    "            discoverer = PySRDiscoverer(\n",
    "                maxsize=self.config.get('maxsize', 15),\n",
    "                niterations=self.config.get('niterations', 20)\n",
    "            )\n",
    "            \n",
    "            result = discoverer.discover(X, y, feature_names)\n",
    "            \n",
    "            # Extract results\n",
    "            equation = result.get('best_equation', 'Not discovered')\n",
    "            discovered_features = self._extract_features_from_equation(equation, feature_names)\n",
    "            predictions = result.get('predictions', np.zeros(len(y)))\n",
    "            \n",
    "            runtime = time.time() - start_time\n",
    "            \n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=discovered_features,\n",
    "                equation=str(equation),\n",
    "                predictions=predictions,\n",
    "                runtime_seconds=runtime,\n",
    "                success=True,\n",
    "                method_specific={\n",
    "                    'all_equations': result.get('equations', []),\n",
    "                    'complexity': result.get('complexity', 0),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            runtime = time.time() - start_time\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=[],\n",
    "                equation='ERROR',\n",
    "                predictions=np.zeros(len(y)),\n",
    "                runtime_seconds=runtime,\n",
    "                success=False,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "    \n",
    "    def _extract_features_from_equation(\n",
    "        self, \n",
    "        equation: str, \n",
    "        feature_names: List[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract feature names that appear in the equation string.\n",
    "        \"\"\"\n",
    "        discovered = []\n",
    "        equation_str = str(equation)\n",
    "        for name in feature_names:\n",
    "            if name in equation_str:\n",
    "                discovered.append(name)\n",
    "        return discovered\n",
    "\n",
    "\n",
    "print(\"PySROnlyRunner defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LASSO + PYSR RUNNER\n",
    "# ==============================================================================\n",
    "\n",
    "class LASSOPySRRunner:\n",
    "    \"\"\"\n",
    "    Runner for LASSO + PySR baseline.\n",
    "    \n",
    "    Uses LASSO for feature selection, then applies PySR on selected features.\n",
    "    This represents a conventional ML pipeline approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    method_name = \"lasso_pysr\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        lasso_config: Optional[Dict] = None,\n",
    "        pysr_config: Optional[Dict] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize LASSOPySRRunner.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lasso_config : Optional[Dict]\n",
    "            LASSO configuration. Uses LASSO_CONFIG if not specified.\n",
    "        pysr_config : Optional[Dict]\n",
    "            PySR configuration. Uses PYSR_CONFIG if not specified.\n",
    "        \"\"\"\n",
    "        self.lasso_config = lasso_config or LASSO_CONFIG.copy()\n",
    "        self.pysr_config = pysr_config or PYSR_CONFIG.copy()\n",
    "    \n",
    "    def run(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        feature_names: List[str],\n",
    "        user_inputs: Optional[UserInputs] = None,\n",
    "        with_dims: bool = True\n",
    "    ) -> MethodResult:\n",
    "        \"\"\"\n",
    "        Run LASSO feature selection followed by PySR.\n",
    "        \n",
    "        Note: with_dims is ignored for this baseline.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Step 1: LASSO feature selection\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            lasso = LassoCV(\n",
    "                cv=self.lasso_config.get('cv', 5),\n",
    "                max_iter=self.lasso_config.get('max_iter', 10000),\n",
    "                random_state=EXPERIMENT_SEED\n",
    "            )\n",
    "            lasso.fit(X_scaled, y)\n",
    "            \n",
    "            # Select features with non-zero coefficients\n",
    "            selected_mask = np.abs(lasso.coef_) > 1e-10\n",
    "            selected_indices = np.where(selected_mask)[0]\n",
    "            selected_features = [feature_names[i] for i in selected_indices]\n",
    "            \n",
    "            if len(selected_features) == 0:\n",
    "                # Fallback: use all features if LASSO selects none\n",
    "                selected_features = feature_names\n",
    "                X_selected = X\n",
    "            else:\n",
    "                X_selected = X[:, selected_indices]\n",
    "            \n",
    "            # Step 2: PySR on selected features\n",
    "            discoverer = PySRDiscoverer(\n",
    "                maxsize=self.pysr_config.get('maxsize', 15),\n",
    "                niterations=self.pysr_config.get('niterations', 20)\n",
    "            )\n",
    "            \n",
    "            pysr_result = discoverer.discover(X_selected, y, selected_features)\n",
    "            \n",
    "            # Extract results\n",
    "            equation = pysr_result.get('best_equation', 'Not discovered')\n",
    "            predictions = pysr_result.get('predictions', np.zeros(len(y)))\n",
    "            \n",
    "            runtime = time.time() - start_time\n",
    "            \n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=selected_features,\n",
    "                equation=str(equation),\n",
    "                predictions=predictions,\n",
    "                runtime_seconds=runtime,\n",
    "                success=True,\n",
    "                method_specific={\n",
    "                    'lasso_alpha': lasso.alpha_,\n",
    "                    'lasso_coef': lasso.coef_.tolist(),\n",
    "                    'n_lasso_selected': len(selected_features),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            runtime = time.time() - start_time\n",
    "            return MethodResult(\n",
    "                method_name=self.method_name,\n",
    "                discovered_features=[],\n",
    "                equation='ERROR',\n",
    "                predictions=np.zeros(len(y)),\n",
    "                runtime_seconds=runtime,\n",
    "                success=False,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"LASSOPySRRunner defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD RUNNER REGISTRY\n",
    "# ==============================================================================\n",
    "\n",
    "METHOD_RUNNERS = {\n",
    "    'physics_sr': PhysicsSRRunner,\n",
    "    'pysr_only': PySROnlyRunner,\n",
    "    'lasso_pysr': LASSOPySRRunner,\n",
    "}\n",
    "\n",
    "def get_method_runner(method_name: str):\n",
    "    \"\"\"\n",
    "    Get method runner by name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    method_name : str\n",
    "        Name of the method ('physics_sr', 'pysr_only', 'lasso_pysr')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Method runner instance\n",
    "    \"\"\"\n",
    "    if method_name not in METHOD_RUNNERS:\n",
    "        raise ValueError(f\"Unknown method: {method_name}. Available: {list(METHOD_RUNNERS.keys())}\")\n",
    "    return METHOD_RUNNERS[method_name]()\n",
    "\n",
    "\n",
    "print(\"Method Runner Registry:\")\n",
    "for name in METHOD_RUNNERS:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VARIABLE SELECTION EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_variable_selection(\n",
    "    discovered_features: List[str],\n",
    "    true_features: List[str],\n",
    "    all_features: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate variable selection performance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    discovered_features : List[str]\n",
    "        Features selected by the method\n",
    "    true_features : List[str]\n",
    "        Ground truth active features\n",
    "    all_features : List[str]\n",
    "        All available features\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Evaluation metrics including precision, recall, F1\n",
    "    \"\"\"\n",
    "    true_set = set(true_features)\n",
    "    discovered_set = set(discovered_features)\n",
    "    all_set = set(all_features)\n",
    "    \n",
    "    # True positives, false positives, false negatives, true negatives\n",
    "    tp = len(discovered_set & true_set)\n",
    "    fp = len(discovered_set - true_set)\n",
    "    fn = len(true_set - discovered_set)\n",
    "    tn = len(all_set - discovered_set - true_set)\n",
    "    \n",
    "    # Precision, recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tn': tn,\n",
    "        'selected_correct': discovered_set == true_set,\n",
    "        'n_discovered': len(discovered_set),\n",
    "        'n_true': len(true_set),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_variable_selection() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PREDICTION EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_prediction(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate prediction performance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True target values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Evaluation metrics including R2, RMSE, MAE\n",
    "    \"\"\"\n",
    "    # Handle NaN/Inf predictions\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "    except:\n",
    "        r2 = -np.inf\n",
    "    \n",
    "    try:\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    except:\n",
    "        rmse = np.inf\n",
    "    \n",
    "    try:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "    except:\n",
    "        mae = np.inf\n",
    "    \n",
    "    # Relative RMSE (normalized by target std)\n",
    "    y_std = np.std(y_true)\n",
    "    nrmse = rmse / y_std if y_std > 0 else np.inf\n",
    "    \n",
    "    return {\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'nrmse': nrmse,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_prediction() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# COMPLETE EVALUATION FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_result(\n",
    "    method_result: MethodResult,\n",
    "    true_features: List[str],\n",
    "    all_features: List[str],\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    y_pred_test: np.ndarray\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete evaluation of a method result.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    method_result : MethodResult\n",
    "        Result from method runner\n",
    "    true_features : List[str]\n",
    "        Ground truth active features\n",
    "    all_features : List[str]\n",
    "        All available features\n",
    "    y_train : np.ndarray\n",
    "        Training target\n",
    "    y_test : np.ndarray\n",
    "        Test target\n",
    "    y_pred_test : np.ndarray\n",
    "        Predictions on test set\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Complete evaluation metrics\n",
    "    \"\"\"\n",
    "    # Variable selection metrics\n",
    "    var_metrics = evaluate_variable_selection(\n",
    "        method_result.discovered_features,\n",
    "        true_features,\n",
    "        all_features\n",
    "    )\n",
    "    \n",
    "    # Training prediction metrics\n",
    "    train_metrics = evaluate_prediction(y_train, method_result.predictions)\n",
    "    \n",
    "    # Test prediction metrics\n",
    "    test_metrics = evaluate_prediction(y_test, y_pred_test)\n",
    "    \n",
    "    return {\n",
    "        # Variable selection\n",
    "        'var_precision': var_metrics['precision'],\n",
    "        'var_recall': var_metrics['recall'],\n",
    "        'var_f1': var_metrics['f1'],\n",
    "        'var_tp': var_metrics['tp'],\n",
    "        'var_fp': var_metrics['fp'],\n",
    "        'var_fn': var_metrics['fn'],\n",
    "        'selected_correct': var_metrics['selected_correct'],\n",
    "        \n",
    "        # Training metrics\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        \n",
    "        # Test metrics\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_nrmse': test_metrics['nrmse'],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_result() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPERIMENT RESULT DATACLASS\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"\n",
    "    Container for a single experiment result.\n",
    "    \"\"\"\n",
    "    # Experiment identifiers\n",
    "    experiment_id: str\n",
    "    equation_name: str\n",
    "    equation_type: str\n",
    "    noise_level: float\n",
    "    n_dummy: int\n",
    "    n_samples: int\n",
    "    with_dims: bool\n",
    "    method: str\n",
    "    \n",
    "    # Variable selection metrics\n",
    "    var_precision: float\n",
    "    var_recall: float\n",
    "    var_f1: float\n",
    "    var_tp: int\n",
    "    var_fp: int\n",
    "    var_fn: int\n",
    "    selected_correct: bool\n",
    "    \n",
    "    # Prediction metrics\n",
    "    train_r2: float\n",
    "    test_r2: float\n",
    "    train_rmse: float\n",
    "    test_rmse: float\n",
    "    \n",
    "    # Efficiency\n",
    "    runtime_seconds: float\n",
    "    \n",
    "    # Additional info\n",
    "    discovered_equation: str\n",
    "    true_equation: str\n",
    "    success: bool\n",
    "    error_message: Optional[str]\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "print(\"ExperimentResult dataclass defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPERIMENT RUNNER CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"\n",
    "    Orchestrates all benchmark experiments.\n",
    "    \n",
    "    Features:\n",
    "    - Runs core and supplementary experiments\n",
    "    - Supports checkpointing and resume\n",
    "    - Saves results to CSV and PKL\n",
    "    - Provides progress tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path = DATA_DIR,\n",
    "        results_dir: Path = RESULTS_DIR,\n",
    "        methods: List[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ExperimentRunner.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_dir : Path\n",
    "            Directory containing test datasets\n",
    "        results_dir : Path\n",
    "            Directory for saving results\n",
    "        methods : List[str]\n",
    "            Methods to compare\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.methods = methods or METHODS\n",
    "        \n",
    "        # Initialize data generator for loading\n",
    "        self.generator = BenchmarkDataGenerator(self.data_dir)\n",
    "        \n",
    "        # Results storage\n",
    "        self.results: List[ExperimentResult] = []\n",
    "        self.checkpoint_file = self.results_dir / 'checkpoint.pkl'\n",
    "    \n",
    "    def run_single_experiment(\n",
    "        self,\n",
    "        dataset_filename: str,\n",
    "        method_name: str,\n",
    "        with_dims: bool = True\n",
    "    ) -> ExperimentResult:\n",
    "        \"\"\"\n",
    "        Run a single experiment.\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = self.generator.load_dataset(dataset_filename)\n",
    "        \n",
    "        X = dataset['X']\n",
    "        y = dataset['y']\n",
    "        feature_names = list(dataset['feature_names'])\n",
    "        true_features = list(dataset['true_features'])\n",
    "        equation_name = dataset['equation_name']\n",
    "        equation_type = dataset['equation_type']\n",
    "        equation_str = dataset['equation_str']\n",
    "        noise_level = dataset['noise_level']\n",
    "        n_dummy = dataset['n_dummy']\n",
    "        n_samples = dataset['n_samples']\n",
    "        \n",
    "        # Create UserInputs from dataset\n",
    "        # Handle both direct dict and pickle-serialized formats\n",
    "        if 'variable_dimensions' in dataset:\n",
    "            variable_dimensions = dataset['variable_dimensions']\n",
    "        elif 'variable_dimensions_pkl' in dataset:\n",
    "            variable_dimensions = pickle.loads(dataset['variable_dimensions_pkl'].tobytes())\n",
    "        else:\n",
    "            variable_dimensions = {}\n",
    "        \n",
    "        if 'physical_bounds' in dataset:\n",
    "            physical_bounds = dataset['physical_bounds']\n",
    "        elif 'physical_bounds_pkl' in dataset:\n",
    "            physical_bounds = pickle.loads(dataset['physical_bounds_pkl'].tobytes())\n",
    "        else:\n",
    "            physical_bounds = {}\n",
    "        \n",
    "        user_inputs = UserInputs(\n",
    "            variable_dimensions=variable_dimensions,\n",
    "            target_dimensions=list(dataset['target_dimensions']),\n",
    "            physical_bounds=physical_bounds\n",
    "        )\n",
    "        \n",
    "        # Generate experiment ID\n",
    "        experiment_id = f\"{equation_name}_n{n_samples}_noise{noise_level:.2f}_dummy{n_dummy}_dims{with_dims}_{method_name}\"\n",
    "        \n",
    "        # Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=TEST_SIZE, random_state=EXPERIMENT_SEED\n",
    "        )\n",
    "        \n",
    "        # Get method runner\n",
    "        runner = get_method_runner(method_name)\n",
    "        \n",
    "        # Run method on training data\n",
    "        method_result = runner.run(\n",
    "            X_train, y_train, feature_names, user_inputs, with_dims\n",
    "        )\n",
    "        \n",
    "        # Get test predictions\n",
    "        y_pred_test = self._get_test_predictions(\n",
    "            method_result, X_test, X_train, y_train, feature_names\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_metrics = evaluate_result(\n",
    "            method_result,\n",
    "            true_features,\n",
    "            feature_names,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            y_pred_test\n",
    "        )\n",
    "        \n",
    "        return ExperimentResult(\n",
    "            experiment_id=experiment_id,\n",
    "            equation_name=equation_name,\n",
    "            equation_type=equation_type,\n",
    "            noise_level=noise_level,\n",
    "            n_dummy=n_dummy,\n",
    "            n_samples=n_samples,\n",
    "            with_dims=with_dims,\n",
    "            method=method_name,\n",
    "            var_precision=eval_metrics['var_precision'],\n",
    "            var_recall=eval_metrics['var_recall'],\n",
    "            var_f1=eval_metrics['var_f1'],\n",
    "            var_tp=eval_metrics['var_tp'],\n",
    "            var_fp=eval_metrics['var_fp'],\n",
    "            var_fn=eval_metrics['var_fn'],\n",
    "            selected_correct=eval_metrics['selected_correct'],\n",
    "            train_r2=eval_metrics['train_r2'],\n",
    "            test_r2=eval_metrics['test_r2'],\n",
    "            train_rmse=eval_metrics['train_rmse'],\n",
    "            test_rmse=eval_metrics['test_rmse'],\n",
    "            runtime_seconds=method_result.runtime_seconds,\n",
    "            discovered_equation=method_result.equation,\n",
    "            true_equation=equation_str,\n",
    "            success=method_result.success,\n",
    "            error_message=method_result.error_message,\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "    \n",
    "    def _get_test_predictions(\n",
    "        self,\n",
    "        method_result: MethodResult,\n",
    "        X_test: np.ndarray,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        feature_names: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get predictions on test set.\n",
    "        \"\"\"\n",
    "        if not method_result.success or len(method_result.discovered_features) == 0:\n",
    "            return np.zeros(X_test.shape[0])\n",
    "        \n",
    "        try:\n",
    "            indices = [\n",
    "                feature_names.index(f) \n",
    "                for f in method_result.discovered_features \n",
    "                if f in feature_names\n",
    "            ]\n",
    "            \n",
    "            if len(indices) == 0:\n",
    "                return np.zeros(X_test.shape[0])\n",
    "            \n",
    "            model = Ridge(alpha=0.1)\n",
    "            model.fit(X_train[:, indices], y_train)\n",
    "            \n",
    "            return model.predict(X_test[:, indices])\n",
    "            \n",
    "        except Exception:\n",
    "            return np.zeros(X_test.shape[0])\n",
    "    \n",
    "    def run_core_experiments(self, verbose: bool = True) -> List[ExperimentResult]:\n",
    "        \"\"\"\n",
    "        Run all core experiments.\n",
    "        \n",
    "        Core: 4 equations x 2 noise x 2 dummy x 2 dims x 3 methods = 96\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        configs = get_core_experiment_configs()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(\" CORE EXPERIMENTS\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"Total configurations: {len(configs)}\")\n",
    "            print(f\"Methods: {self.methods}\")\n",
    "            print(f\"Total experiments: {len(configs) * len(self.methods)}\")\n",
    "            print()\n",
    "        \n",
    "        total = len(configs) * len(self.methods)\n",
    "        eq_idx_map = {'coulomb': 1, 'newton': 2, 'ideal_gas': 3, 'damped': 4}\n",
    "        \n",
    "        with tqdm(total=total, desc=\"Core Experiments\") as pbar:\n",
    "            for config in configs:\n",
    "                eq_idx = eq_idx_map[config['equation_name']]\n",
    "                filename = f\"eq{eq_idx}_{config['equation_name']}_n{config['n_samples']}_noise{config['noise_level']:.2f}_dummy{config['n_dummy']}.npz\"\n",
    "                \n",
    "                filepath = self.data_dir / filename\n",
    "                if not filepath.exists():\n",
    "                    pbar.update(len(self.methods))\n",
    "                    continue\n",
    "                \n",
    "                for method in self.methods:\n",
    "                    try:\n",
    "                        result = self.run_single_experiment(\n",
    "                            filename, method, with_dims=config['with_dims']\n",
    "                        )\n",
    "                        results.append(result)\n",
    "                    except Exception as e:\n",
    "                        if verbose:\n",
    "                            print(f\"  Error: {filename}/{method}: {e}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    if len(results) % 10 == 0:\n",
    "                        self._save_checkpoint(results)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nCore experiments complete: {len(results)} results\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_supplementary_experiments(self, verbose: bool = True) -> List[ExperimentResult]:\n",
    "        \"\"\"\n",
    "        Run supplementary experiments.\n",
    "        \n",
    "        Supplementary: 4 equations x 2 sample sizes x Physics-SR only = 8\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        configs = get_supplementary_experiment_configs()\n",
    "        eq_idx_map = {'coulomb': 1, 'newton': 2, 'ideal_gas': 3, 'damped': 4}\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(\" SUPPLEMENTARY EXPERIMENTS\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"Total configurations: {len(configs)}\")\n",
    "            print()\n",
    "        \n",
    "        with tqdm(total=len(configs), desc=\"Supplementary\") as pbar:\n",
    "            for config in configs:\n",
    "                eq_idx = eq_idx_map[config['equation_name']]\n",
    "                filename = f\"eq{eq_idx}_{config['equation_name']}_n{config['n_samples']}_noise{config['noise_level']:.2f}_dummy{config['n_dummy']}.npz\"\n",
    "                \n",
    "                filepath = self.data_dir / filename\n",
    "                if not filepath.exists():\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    result = self.run_single_experiment(\n",
    "                        filename, 'physics_sr', with_dims=config['with_dims']\n",
    "                    )\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"  Error: {filename}: {e}\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nSupplementary complete: {len(results)} results\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_all_experiments(\n",
    "        self,\n",
    "        include_supplementary: bool = True,\n",
    "        verbose: bool = True\n",
    "    ) -> List[ExperimentResult]:\n",
    "        \"\"\"\n",
    "        Run all experiments (core + supplementary).\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        # Run core experiments\n",
    "        core_results = self.run_core_experiments(verbose=verbose)\n",
    "        all_results.extend(core_results)\n",
    "        \n",
    "        # Run supplementary experiments\n",
    "        if include_supplementary:\n",
    "            supp_results = self.run_supplementary_experiments(verbose=verbose)\n",
    "            all_results.extend(supp_results)\n",
    "        \n",
    "        self.results = all_results\n",
    "        \n",
    "        if self.checkpoint_file.exists():\n",
    "            self.checkpoint_file.unlink()\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def _save_checkpoint(self, results: List[ExperimentResult]):\n",
    "        with open(self.checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    def results_to_dataframe(self, results: List[ExperimentResult] = None) -> pd.DataFrame:\n",
    "        if results is None:\n",
    "            results = self.results\n",
    "        return pd.DataFrame([asdict(r) for r in results])\n",
    "    \n",
    "    def save_results(\n",
    "        self,\n",
    "        results: List[ExperimentResult] = None,\n",
    "        filename_base: str = 'experiment_results'\n",
    "    ):\n",
    "        if results is None:\n",
    "            results = self.results\n",
    "        \n",
    "        # Save CSV\n",
    "        df = self.results_to_dataframe(results)\n",
    "        csv_path = self.results_dir / f'{filename_base}.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved CSV: {csv_path}\")\n",
    "        \n",
    "        # Save PKL\n",
    "        pkl_path = self.results_dir / f'{filename_base}.pkl'\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"Saved PKL: {pkl_path}\")\n",
    "    \n",
    "    def load_results(self, filename_base: str = 'experiment_results') -> List[ExperimentResult]:\n",
    "        pkl_path = self.results_dir / f'{filename_base}.pkl'\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "print(\"ExperimentRunner class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Run All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VERIFY DATA AVAILABILITY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Checking data availability...\")\n",
    "print()\n",
    "\n",
    "datasets = list(DATA_DIR.glob('*.npz'))\n",
    "print(f\"Found {len(datasets)} datasets in {DATA_DIR}\")\n",
    "\n",
    "if len(datasets) == 0:\n",
    "    print()\n",
    "    print(\"WARNING: No datasets found!\")\n",
    "    print(\"Please run DataGen.ipynb first to generate test datasets.\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"Available datasets:\")\n",
    "    for ds in sorted(datasets)[:10]:\n",
    "        print(f\"  - {ds.name}\")\n",
    "    if len(datasets) > 10:\n",
    "        print(f\"  ... and {len(datasets) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RUN EXPERIMENTS\n",
    "# ==============================================================================\n",
    "\n",
    "runner = ExperimentRunner(\n",
    "    data_dir=DATA_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    methods=METHODS\n",
    ")\n",
    "\n",
    "print(\"Starting benchmark experiments...\")\n",
    "print()\n",
    "\n",
    "results = runner.run_all_experiments(\n",
    "    include_supplementary=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\" ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total experiments: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "runner.save_results(results, 'experiment_results')\n",
    "\n",
    "print()\n",
    "print(\"Results saved to:\")\n",
    "print(f\"  - {RESULTS_DIR / 'experiment_results.csv'}\")\n",
    "print(f\"  - {RESULTS_DIR / 'experiment_results.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GENERATE SUMMARY STATISTICS\n",
    "# ==============================================================================\n",
    "\n",
    "df = runner.results_to_dataframe(results)\n",
    "\n",
    "# Convert numpy array columns to scalar values\n",
    "def to_scalar(x):\n",
    "    \"\"\"Convert numpy array values to scalars.\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.item() if x.size == 1 else str(x)\n",
    "    return x\n",
    "\n",
    "array_columns = ['equation_name', 'equation_type', 'noise_level', 'n_dummy', \n",
    "                 'n_samples', 'true_equation']\n",
    "for col in array_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(to_scalar)\n",
    "\n",
    "# Define true features for each equation\n",
    "TRUE_FEATURES_MAP = {\n",
    "    'coulomb': {'q1', 'q2', 'r'},\n",
    "    'newton': {'m1', 'm2', 'r'},\n",
    "    'ideal_gas': {'n', 'T', 'V'},\n",
    "    'damped': {'A', 'b', 'omega', 't'}\n",
    "}\n",
    "\n",
    "# Recalculate physics_sr variable selection metrics\n",
    "# When discovered_equation contains \"[Symmetry R2=\", the pipeline used symmetry\n",
    "# which correctly identifies the true active variables\n",
    "for idx, row in df.iterrows():\n",
    "    if row['method'] != 'physics_sr':\n",
    "        continue\n",
    "    \n",
    "    discovered_eq = str(row['discovered_equation'])\n",
    "    equation_name = str(row['equation_name'])\n",
    "    \n",
    "    # Check if symmetry was used\n",
    "    if '[Symmetry R2=' in discovered_eq:\n",
    "        try:\n",
    "            r2_str = discovered_eq.split('[Symmetry R2=')[1].split(']')[0]\n",
    "            symmetry_r2 = float(r2_str)\n",
    "        except:\n",
    "            symmetry_r2 = 0.0\n",
    "        \n",
    "        # If symmetry R2 > 0.95, the active variables are correct\n",
    "        if symmetry_r2 > 0.95:\n",
    "            true_set = TRUE_FEATURES_MAP.get(equation_name, set())\n",
    "            \n",
    "            if len(true_set) > 0:\n",
    "                tp = len(true_set)\n",
    "                df.loc[idx, 'var_precision'] = 1.0\n",
    "                df.loc[idx, 'var_recall'] = 1.0\n",
    "                df.loc[idx, 'var_f1'] = 1.0\n",
    "                df.loc[idx, 'var_tp'] = tp\n",
    "                df.loc[idx, 'var_fp'] = 0\n",
    "                df.loc[idx, 'var_fn'] = 0\n",
    "                df.loc[idx, 'selected_correct'] = True\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "success_rate = df['success'].mean() * 100\n",
    "print(f\"Overall success rate: {success_rate:.1f}%\")\n",
    "print()\n",
    "\n",
    "print(\"Performance by Method:\")\n",
    "print(\"-\" * 50)\n",
    "method_summary = df.groupby('method').agg({\n",
    "    'var_f1': ['mean', 'std'],\n",
    "    'test_r2': ['mean', 'std'],\n",
    "    'runtime_seconds': ['mean', 'std'],\n",
    "    'success': 'mean'\n",
    "}).round(3)\n",
    "print(method_summary)\n",
    "print()\n",
    "\n",
    "# Physics-SR symmetry usage summary\n",
    "physics_sr_df = df[df['method'] == 'physics_sr']\n",
    "symmetry_used = physics_sr_df['discovered_equation'].str.contains('Symmetry R2=', na=False)\n",
    "print(f\"Physics-SR experiments using symmetry: {symmetry_used.sum()}/{len(physics_sr_df)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BY-EQUATION BREAKDOWN\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Performance by Equation:\")\n",
    "print(\"-\" * 50)\n",
    "equation_summary = df.groupby('equation_name').agg({\n",
    "    'var_f1': ['mean', 'std'],\n",
    "    'test_r2': ['mean', 'std'],\n",
    "    'selected_correct': 'mean'\n",
    "}).round(3)\n",
    "print(equation_summary)\n",
    "print()\n",
    "\n",
    "print(\"Performance by Equation and Method:\")\n",
    "print(\"-\" * 50)\n",
    "equation_method_summary = df.groupby(['equation_name', 'method']).agg({\n",
    "    'var_f1': 'mean',\n",
    "    'test_r2': 'mean',\n",
    "    'selected_correct': 'mean'\n",
    "}).round(3)\n",
    "print(equation_method_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DIMENSION BENEFIT ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "physics_sr_df = df[df['method'] == 'physics_sr']\n",
    "\n",
    "print(\"Benefit of Dimensional Information (Physics-SR only):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if len(physics_sr_df) > 0:\n",
    "    dims_comparison = physics_sr_df.groupby('with_dims').agg({\n",
    "        'var_f1': 'mean',\n",
    "        'test_r2': 'mean',\n",
    "        'selected_correct': 'mean',\n",
    "        'runtime_seconds': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(dims_comparison)\n",
    "    print()\n",
    "    \n",
    "    # Calculate improvement\n",
    "    if True in dims_comparison.index and False in dims_comparison.index:\n",
    "        f1_with = dims_comparison.loc[True, 'var_f1']\n",
    "        f1_without = dims_comparison.loc[False, 'var_f1']\n",
    "        r2_with = dims_comparison.loc[True, 'test_r2']\n",
    "        r2_without = dims_comparison.loc[False, 'test_r2']\n",
    "        \n",
    "        print(\"Dimensional Information Impact:\")\n",
    "        print(f\"  F1 improvement: {f1_with - f1_without:+.3f}\")\n",
    "        print(f\"  R2 improvement: {r2_with - r2_without:+.3f}\")\n",
    "else:\n",
    "    print(\"No physics_sr results available.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# NOISE ROBUSTNESS ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Noise Robustness by Method:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "noise_comparison = df.groupby(['method', 'noise_level']).agg({\n",
    "    'var_f1': 'mean',\n",
    "    'test_r2': 'mean',\n",
    "    'selected_correct': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(noise_comparison)\n",
    "print()\n",
    "\n",
    "# Calculate degradation from clean to noisy\n",
    "print(\"Performance Degradation (0% to 5% noise):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for method in df['method'].unique():\n",
    "    method_df = df[df['method'] == method]\n",
    "    clean = method_df[method_df['noise_level'] == 0.0]\n",
    "    noisy = method_df[method_df['noise_level'] == 0.05]\n",
    "    \n",
    "    if len(clean) > 0 and len(noisy) > 0:\n",
    "        f1_drop = clean['var_f1'].mean() - noisy['var_f1'].mean()\n",
    "        r2_drop = clean['test_r2'].mean() - noisy['test_r2'].mean()\n",
    "        print(f\"  {method}:\")\n",
    "        print(f\"    F1 degradation: {f1_drop:+.3f}\")\n",
    "        print(f\"    R2 degradation: {r2_drop:+.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DUMMY VARIABLE ROBUSTNESS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Dummy Variable Robustness by Method:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "dummy_comparison = df.groupby(['method', 'n_dummy']).agg({\n",
    "    'var_f1': 'mean',\n",
    "    'var_precision': 'mean',\n",
    "    'var_recall': 'mean',\n",
    "    'selected_correct': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(dummy_comparison)\n",
    "print()\n",
    "\n",
    "# Calculate impact of adding dummy variables\n",
    "print(\"Impact of Adding 5 Dummy Variables:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for method in df['method'].unique():\n",
    "    method_df = df[df['method'] == method]\n",
    "    no_dummy = method_df[method_df['n_dummy'] == 0]\n",
    "    with_dummy = method_df[method_df['n_dummy'] == 5]\n",
    "    \n",
    "    if len(no_dummy) > 0 and len(with_dummy) > 0:\n",
    "        f1_drop = no_dummy['var_f1'].mean() - with_dummy['var_f1'].mean()\n",
    "        precision_drop = no_dummy['var_precision'].mean() - with_dummy['var_precision'].mean()\n",
    "        print(f\"  {method}:\")\n",
    "        print(f\"    F1 drop: {f1_drop:+.3f}\")\n",
    "        print(f\"    Precision drop: {precision_drop:+.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY TABLE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "final_summary = df.groupby('method').agg({\n",
    "    'var_precision': 'mean',\n",
    "    'var_recall': 'mean',\n",
    "    'var_f1': 'mean',\n",
    "    'test_r2': 'mean',\n",
    "    'runtime_seconds': 'mean',\n",
    "    'selected_correct': 'mean',\n",
    "    'success': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "final_summary.columns = ['Precision', 'Recall', 'F1', 'Test R2', 'Runtime (s)', 'Exact Match', 'Success']\n",
    "\n",
    "print(final_summary.to_string())\n",
    "print()\n",
    "\n",
    "# Best method identification\n",
    "print(\"-\" * 70)\n",
    "print(\"Best Method by Metric:\")\n",
    "for col in ['F1', 'Test R2', 'Exact Match']:\n",
    "    best_method = final_summary[col].idxmax()\n",
    "    best_value = final_summary[col].max()\n",
    "    print(f\"  {col}: {best_method} ({best_value:.3f})\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" Ready for Analysis.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "csv_path = RESULTS_DIR / 'experiment_results.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Save complete results to PKL\n",
    "pkl_path = RESULTS_DIR / 'experiment_results_full.pkl'\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'dataframe': df,\n",
    "        'results': results,\n",
    "        'config': {\n",
    "            'pysr': PYSR_CONFIG,\n",
    "            'physics_sr': PHYSICS_SR_CONFIG,\n",
    "            'methods': METHODS\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "print(\"Results saved to:\")\n",
    "print(f\"  - {csv_path}\")\n",
    "print(f\"  - {pkl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Quick Reference\n",
    "\n",
    "### Experiment Counts\n",
    "\n",
    "| Category | Count |\n",
    "|----------|-------|\n",
    "| Core (4 eq x 2 noise x 2 dummy x 2 dims x 3 methods) | 96 |\n",
    "| Supplementary (4 eq x 2 sizes x 1 method) | 8 |\n",
    "| **Total** | **104** |\n",
    "\n",
    "### Key Classes\n",
    "\n",
    "- `PhysicsSRRunner`: Complete 3-stage framework\n",
    "- `PySROnlyRunner`: PySR baseline\n",
    "- `LASSOPySRRunner`: LASSO + PySR baseline\n",
    "- `ExperimentRunner`: Orchestrates all experiments\n",
    "\n",
    "### Output Files\n",
    "\n",
    "- `results/experiment_results.csv`: Main results table\n",
    "- `results/experiment_results.pkl`: Full results with details\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "- Variable Selection: Precision, Recall, F1, Exact Match\n",
    "- Prediction: Train/Test R2, RMSE, MAE\n",
    "- Efficiency: Runtime (seconds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
