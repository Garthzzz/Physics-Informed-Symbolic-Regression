{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Physics-SR Framework v4.1 Benchmark\n",
    "\n",
    "## Results Analysis and Visualization Module\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Contact:** zz3239@columbia.edu  \n",
    "**Date:** January 2026  \n",
    "**Version:** 4.1 (Structure-Guided Feature Library + Computational Optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook analyzes and visualizes the benchmark results from Experiments.ipynb:\n",
    "\n",
    "1. **Summary Statistics**: Overall, by-factor, and by-equation-type performance metrics\n",
    "2. **Core Visualizations**: 7 figures for main results\n",
    "3. **v4.1 Library Analysis**: 4 new figures for augmented library composition\n",
    "4. **Supplementary Visualizations**: 2 figures for additional analysis\n",
    "5. **LaTeX Tables**: 4 publication-ready tables\n",
    "6. **Statistical Tests**: Significance testing between methods\n",
    "\n",
    "### Test Equations (AI Feynman Benchmark)\n",
    "\n",
    "| # | Name | AI Feynman ID | Equation | Type |\n",
    "|---|------|---------------|----------|------|\n",
    "| 1 | Coulomb | I.12.2 | F = k * q1 * q2 / r^2 | Rational |\n",
    "| 2 | Cosines | I.29.16 | x = sqrt(x1^2 + x2^2 - 2*x1*x2*cos(theta1-theta2)) | Nested Trig |\n",
    "| 3 | Barometric | I.40.1 | n = n0 * exp(-m*g*x/(k_B*T)) | Exponential |\n",
    "| 4 | DotProduct | I.11.19 | A = x1*y1 + x2*y2 + x3*y3 | Polynomial |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENVIRONMENT RESET AND FRESH CLONE\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    import shutil\n",
    "    import gc\n",
    "    \n",
    "    repo_path = '/content/Physics-Informed-Symbolic-Regression'\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    if not current_dir.startswith(repo_path) or not os.path.exists(repo_path + '/.git'):\n",
    "        os.chdir('/content')\n",
    "        gc.collect()\n",
    "        \n",
    "        if os.path.exists(repo_path):\n",
    "            shutil.rmtree(repo_path)\n",
    "            print(\"[OK] Removed existing repository.\")\n",
    "        \n",
    "        !git clone https://github.com/Garthzzz/Physics-Informed-Symbolic-Regression.git\n",
    "        \n",
    "        if os.path.exists(repo_path + '/.git'):\n",
    "            os.chdir(repo_path + '/benchmark')\n",
    "            print(f\"[OK] Working directory: {os.getcwd()}\")\n",
    "        else:\n",
    "            print(\"[FAIL] Clone incomplete!\")\n",
    "        \n",
    "        print(\"[OK] Environment reset complete.\")\n",
    "    else:\n",
    "        print(\"[SKIP] Already in valid repository.\")\n",
    "else:\n",
    "    print(\"[INFO] Not in Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analysis.ipynb - Results Analysis and Visualization Module\n",
    "Physics-SR Framework v4.1 Benchmark Suite\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Contact: zz3239@columbia.edu\n",
    "Version: 4.1\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Analysis v4.1: All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    BASE_DIR = Path('/content/Physics-Informed-Symbolic-Regression')\n",
    "    BENCHMARK_DIR = BASE_DIR / 'benchmark'\n",
    "else:\n",
    "    BENCHMARK_DIR = Path('.').resolve()\n",
    "    BASE_DIR = BENCHMARK_DIR.parent\n",
    "\n",
    "RESULTS_DIR = BENCHMARK_DIR / 'results'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "TABLES_DIR = RESULTS_DIR / 'tables'\n",
    "DATA_DIR = BENCHMARK_DIR / 'data'\n",
    "\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TABLES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")\n",
    "print(f\"Tables directory: {TABLES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PLOTTING CONFIGURATION (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "METHOD_COLORS = {\n",
    "    'physics_sr': '#2E86AB',\n",
    "    'pysr_only': '#E94F37',\n",
    "    'lasso_pysr': '#F39C12',\n",
    "}\n",
    "\n",
    "METHOD_NAMES = {\n",
    "    'physics_sr': 'Physics-SR',\n",
    "    'pysr_only': 'PySR-Only',\n",
    "    'lasso_pysr': 'LASSO+PySR',\n",
    "}\n",
    "\n",
    "EQUATION_NAMES = {\n",
    "    'coulomb': 'Coulomb',\n",
    "    'cosines': 'Cosines',\n",
    "    'barometric': 'Barometric',\n",
    "    'dotproduct': 'DotProduct',\n",
    "}\n",
    "\n",
    "EQUATION_TYPE_NAMES = {\n",
    "    'rational': 'Rational',\n",
    "    'nested_trigonometric': 'Nested Trig',\n",
    "    'exponential': 'Exponential',\n",
    "    'polynomial_interaction': 'Polynomial',\n",
    "}\n",
    "\n",
    "AI_FEYNMAN_IDS = {\n",
    "    'coulomb': 'I.12.2',\n",
    "    'cosines': 'I.29.16',\n",
    "    'barometric': 'I.40.1',\n",
    "    'dotproduct': 'I.11.19',\n",
    "}\n",
    "\n",
    "LIBRARY_COLORS = {\n",
    "    'pysr': '#2ecc71',\n",
    "    'variant': '#3498db',\n",
    "    'poly': '#9b59b6',\n",
    "    'op': '#e74c3c',\n",
    "}\n",
    "\n",
    "TIMING_COLORS = {\n",
    "    'stage1': '#3498db',\n",
    "    'pysr': '#e74c3c',\n",
    "    'library': '#2ecc71',\n",
    "    'ewsindy': '#9b59b6',\n",
    "    'stage3': '#f39c12',\n",
    "}\n",
    "\n",
    "FIGURE_SIZES = {\n",
    "    'single': (8, 6),\n",
    "    'wide': (12, 6),\n",
    "    'tall': (8, 10),\n",
    "    'square': (8, 8),\n",
    "    'large': (14, 10),\n",
    "}\n",
    "\n",
    "FIGURE_DPI = 300\n",
    "\n",
    "FONTSIZE_TITLE = 14\n",
    "FONTSIZE_LABEL = 12\n",
    "FONTSIZE_TICK = 10\n",
    "FONTSIZE_LEGEND = 10\n",
    "FONTSIZE_ANNOTATION = 8\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': FONTSIZE_TICK,\n",
    "    'axes.titlesize': FONTSIZE_TITLE,\n",
    "    'axes.labelsize': FONTSIZE_LABEL,\n",
    "    'xtick.labelsize': FONTSIZE_TICK,\n",
    "    'ytick.labelsize': FONTSIZE_TICK,\n",
    "    'legend.fontsize': FONTSIZE_LEGEND,\n",
    "    'figure.titlesize': FONTSIZE_TITLE,\n",
    "})\n",
    "\n",
    "print(\"Plotting configuration v4.1 set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD EXPERIMENT RESULTS (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_results_csv(filepath: Path = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load experiment results from CSV file (v4.1).\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = RESULTS_DIR / 'experiment_results.csv'\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Results file not found: {filepath}\\n\"\n",
    "            f\"Please run Experiments.ipynb first to generate results.\"\n",
    "        )\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    if 'with_dims' in df.columns:\n",
    "        df['with_dims'] = df['with_dims'].astype(bool)\n",
    "    if 'selected_correct' in df.columns:\n",
    "        df['selected_correct'] = df['selected_correct'].astype(bool)\n",
    "    if 'success' in df.columns:\n",
    "        df['success'] = df['success'].astype(bool)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} experiment results from {filepath}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_results_pkl(filepath: Path = None) -> Optional[List]:\n",
    "    \"\"\"\n",
    "    Load detailed experiment results from PKL file (v4.1).\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = RESULTS_DIR / 'experiment_results.pkl'\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"Warning: PKL file not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        print(f\"Loaded detailed results from {filepath}\")\n",
    "        return results\n",
    "    except (AttributeError, ModuleNotFoundError) as e:\n",
    "        print(f\"Warning: Cannot load PKL file: {e}\")\n",
    "        print(\"Continuing with CSV data only.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Load functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA VALIDATION (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "def validate_results(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate results DataFrame and return summary (v4.1).\n",
    "    \"\"\"\n",
    "    validation = {\n",
    "        'n_experiments': len(df),\n",
    "        'n_successful': df['success'].sum() if 'success' in df.columns else len(df),\n",
    "        'success_rate': df['success'].mean() * 100 if 'success' in df.columns else 100.0,\n",
    "        'methods': df['method'].unique().tolist(),\n",
    "        'equations': df['equation_name'].unique().tolist(),\n",
    "        'noise_levels': sorted(df['noise_level'].unique().tolist()),\n",
    "        'dummy_counts': sorted(df['n_dummy'].unique().tolist()),\n",
    "        'sample_sizes': sorted(df['n_samples'].unique().tolist()) if 'n_samples' in df.columns else [500],\n",
    "        'ai_feynman_ids': df['ai_feynman_id'].unique().tolist() if 'ai_feynman_id' in df.columns else [],\n",
    "        'has_library_info': 'library_n_total' in df.columns,\n",
    "        'has_timing_info': 'timing_stage1' in df.columns,\n",
    "    }\n",
    "    return validation\n",
    "\n",
    "\n",
    "def get_available_equations(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get list of equations available in DataFrame, ordered consistently.\n",
    "    \"\"\"\n",
    "    preferred_order = ['coulomb', 'cosines', 'barometric', 'dotproduct']\n",
    "    available = df['equation_name'].unique().tolist()\n",
    "    return [eq for eq in preferred_order if eq in available] + \\\n",
    "           [eq for eq in available if eq not in preferred_order]\n",
    "\n",
    "\n",
    "def add_equation_type_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add equation type column if not present (v4.1).\n",
    "    \"\"\"\n",
    "    if 'eq_type' not in df.columns:\n",
    "        type_map = {\n",
    "            'coulomb': 'rational',\n",
    "            'cosines': 'nested_trigonometric',\n",
    "            'barometric': 'exponential',\n",
    "            'dotproduct': 'polynomial_interaction',\n",
    "        }\n",
    "        df['eq_type'] = df['equation_name'].map(type_map)\n",
    "        print(\"Added eq_type column.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Validation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD DATA\n",
    "# ==============================================================================\n",
    "\n",
    "df = load_results_csv()\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "detailed_results = load_results_pkl()\n",
    "\n",
    "df = add_equation_type_column(df)\n",
    "\n",
    "validation = validate_results(df)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA VALIDATION SUMMARY (v4.1)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total experiments: {validation['n_experiments']}\")\n",
    "print(f\"Successful experiments: {validation['n_successful']}\")\n",
    "print(f\"Success rate: {validation['success_rate']:.1f}%\")\n",
    "print(f\"Methods: {validation['methods']}\")\n",
    "print(f\"Equations: {validation['equations']}\")\n",
    "print(f\"AI Feynman IDs: {validation['ai_feynman_ids']}\")\n",
    "print(f\"Noise levels: {validation['noise_levels']}\")\n",
    "print(f\"Dummy counts: {validation['dummy_counts']}\")\n",
    "print(f\"Sample sizes: {validation['sample_sizes']}\")\n",
    "print(f\"Has library info: {validation['has_library_info']}\")\n",
    "print(f\"Has timing info: {validation['has_timing_info']}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD COMPARISON (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_method_comparison(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute performance comparison by method (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    comparison = []\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        method_df = core_df[core_df['method'] == method]\n",
    "        if len(method_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        row = {\n",
    "            'Method': METHOD_NAMES.get(method, method),\n",
    "            'N': len(method_df),\n",
    "            'F1': f\"{method_df['var_f1'].mean():.3f} +/- {method_df['var_f1'].std():.3f}\",\n",
    "            'Precision': f\"{method_df['var_precision'].mean():.3f} +/- {method_df['var_precision'].std():.3f}\",\n",
    "            'Recall': f\"{method_df['var_recall'].mean():.3f} +/- {method_df['var_recall'].std():.3f}\",\n",
    "            'Test R2': f\"{method_df['test_r2'].mean():.3f} +/- {method_df['test_r2'].std():.3f}\",\n",
    "            'Runtime (s)': f\"{method_df['runtime_seconds'].mean():.1f} +/- {method_df['runtime_seconds'].std():.1f}\",\n",
    "            'Exact Match': f\"{method_df['selected_correct'].mean()*100:.1f}%\",\n",
    "        }\n",
    "        comparison.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "method_comparison = compute_method_comparison(df)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"METHOD COMPARISON (v4.1 - All Equations)\")\n",
    "print(\"=\" * 100)\n",
    "display(method_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BY-EQUATION COMPARISON (v4.1 with AI Feynman IDs)\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_equation_comparison(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute performance comparison by equation with AI Feynman IDs (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    pivot = core_df.pivot_table(\n",
    "        index='equation_name',\n",
    "        columns='method',\n",
    "        values=['var_f1', 'test_r2', 'selected_correct'],\n",
    "        aggfunc='mean'\n",
    "    ).round(3)\n",
    "    \n",
    "    return pivot\n",
    "\n",
    "\n",
    "equation_comparison = compute_equation_comparison(df)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE BY EQUATION (v4.1)\")\n",
    "print(\"=\" * 80)\n",
    "display(equation_comparison)\n",
    "\n",
    "print(\"\\nAI Feynman ID Mapping:\")\n",
    "for eq_name in get_available_equations(df):\n",
    "    ai_id = AI_FEYNMAN_IDS.get(eq_name, 'Unknown')\n",
    "    print(f\"  {eq_name}: {ai_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD COMPARISON BY EQUATION TYPE (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_method_comparison_by_type(df: pd.DataFrame, eq_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute performance comparison by method for specific equation type (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    type_df = core_df[core_df['eq_type'] == eq_type]\n",
    "    \n",
    "    comparison = []\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        method_df = type_df[type_df['method'] == method]\n",
    "        if len(method_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        row = {\n",
    "            'Method': METHOD_NAMES.get(method, method),\n",
    "            'N': len(method_df),\n",
    "            'F1': f\"{method_df['var_f1'].mean():.3f}\",\n",
    "            'Precision': f\"{method_df['var_precision'].mean():.3f}\",\n",
    "            'Recall': f\"{method_df['var_recall'].mean():.3f}\",\n",
    "            'Test R2': f\"{method_df['test_r2'].mean():.3f}\",\n",
    "            'Exact Match': f\"{method_df['selected_correct'].mean()*100:.1f}%\",\n",
    "        }\n",
    "        comparison.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"METHOD COMPARISON BY EQUATION TYPE (v4.1)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for eq_type in df['eq_type'].unique():\n",
    "    if pd.isna(eq_type):\n",
    "        continue\n",
    "    print(f\"\\n[{EQUATION_TYPE_NAMES.get(eq_type, eq_type)}]\")\n",
    "    display(compute_method_comparison_by_type(df, eq_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TESTS (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_statistical_tests(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform statistical significance tests between methods (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    physics_sr_f1 = core_df[core_df['method'] == 'physics_sr']['var_f1'].values\n",
    "    pysr_only_f1 = core_df[core_df['method'] == 'pysr_only']['var_f1'].values\n",
    "    lasso_pysr_f1 = core_df[core_df['method'] == 'lasso_pysr']['var_f1'].values\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if len(physics_sr_f1) > 0 and len(pysr_only_f1) > 0:\n",
    "        t_stat, p_value = stats.ttest_ind(physics_sr_f1, pysr_only_f1)\n",
    "        effect_size = (physics_sr_f1.mean() - pysr_only_f1.mean()) / np.sqrt(\n",
    "            (physics_sr_f1.std()**2 + pysr_only_f1.std()**2) / 2\n",
    "        )\n",
    "        results['physics_sr_vs_pysr_only'] = {\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'cohens_d': effect_size,\n",
    "        }\n",
    "    \n",
    "    if len(physics_sr_f1) > 0 and len(lasso_pysr_f1) > 0:\n",
    "        t_stat, p_value = stats.ttest_ind(physics_sr_f1, lasso_pysr_f1)\n",
    "        effect_size = (physics_sr_f1.mean() - lasso_pysr_f1.mean()) / np.sqrt(\n",
    "            (physics_sr_f1.std()**2 + lasso_pysr_f1.std()**2) / 2\n",
    "        )\n",
    "        results['physics_sr_vs_lasso_pysr'] = {\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'cohens_d': effect_size,\n",
    "        }\n",
    "    \n",
    "    if len(physics_sr_f1) > 0 and len(pysr_only_f1) > 0 and len(lasso_pysr_f1) > 0:\n",
    "        f_stat, p_value = stats.f_oneway(physics_sr_f1, pysr_only_f1, lasso_pysr_f1)\n",
    "        results['anova'] = {\n",
    "            'f_statistic': f_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "stat_tests = compute_statistical_tests(df)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (Variable Selection F1)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'anova' in stat_tests:\n",
    "    print(\"\\n[ANOVA - Overall]\")\n",
    "    print(f\"  F-statistic: {stat_tests['anova']['f_statistic']:.4f}\")\n",
    "    print(f\"  p-value: {stat_tests['anova']['p_value']:.6f}\")\n",
    "    print(f\"  Significant (p < 0.05): {stat_tests['anova']['significant']}\")\n",
    "\n",
    "print(\"\\n[Pairwise t-tests with Effect Size]\")\n",
    "for comparison, result in stat_tests.items():\n",
    "    if comparison != 'anova':\n",
    "        print(f\"\\n  {comparison.replace('_', ' ').title()}:\")\n",
    "        print(f\"    t-statistic: {result['t_statistic']:.4f}\")\n",
    "        print(f\"    p-value: {result['p_value']:.6f}\")\n",
    "        print(f\"    Significant (p < 0.05): {result['significant']}\")\n",
    "        print(f\"    Cohen's d: {result['cohens_d']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Core Result Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 1: VARIABLE SELECTION F1 VS NOISE LEVEL\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_vs_noise(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing F1 score vs noise level, grouped by method (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        means = subset.groupby('noise_level')['var_f1'].mean()\n",
    "        stds = subset.groupby('noise_level')['var_f1'].std()\n",
    "        \n",
    "        ax.errorbar(\n",
    "            means.index * 100,\n",
    "            means.values,\n",
    "            yerr=stds.values,\n",
    "            label=METHOD_NAMES[method],\n",
    "            marker='o',\n",
    "            markersize=8,\n",
    "            capsize=5,\n",
    "            color=METHOD_COLORS[method],\n",
    "            linewidth=2,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Noise Level (%)')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection Performance vs Noise Level (v4.1)')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig1 = plot_f1_vs_noise(df, save_path=FIGURES_DIR / 'fig1_f1_vs_noise.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 2: VARIABLE SELECTION F1 VS DUMMY COUNT\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_vs_dummy(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing F1 score vs dummy feature count, grouped by method (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        means = subset.groupby('n_dummy')['var_f1'].mean()\n",
    "        stds = subset.groupby('n_dummy')['var_f1'].std()\n",
    "        \n",
    "        ax.errorbar(\n",
    "            means.index,\n",
    "            means.values,\n",
    "            yerr=stds.values,\n",
    "            label=METHOD_NAMES[method],\n",
    "            marker='s',\n",
    "            markersize=8,\n",
    "            capsize=5,\n",
    "            color=METHOD_COLORS[method],\n",
    "            linewidth=2,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Number of Dummy Features')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection Performance vs Dummy Features (v4.1)')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig2 = plot_f1_vs_dummy(df, save_path=FIGURES_DIR / 'fig2_f1_vs_dummy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 3: TEST R2 COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_test_r2_comparison(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Bar chart comparing Test R2 by method (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    methods = [m for m in methods if m in core_df['method'].unique()]\n",
    "    x = np.arange(len(methods))\n",
    "    \n",
    "    means = []\n",
    "    stds = []\n",
    "    colors = []\n",
    "    for method in methods:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        means.append(subset['test_r2'].mean())\n",
    "        stds.append(subset['test_r2'].std())\n",
    "        colors.append(METHOD_COLORS[method])\n",
    "    \n",
    "    bars = ax.bar(x, means, yerr=stds, color=colors, capsize=5, alpha=0.8)\n",
    "    \n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax.annotate(\n",
    "            f'{mean:.3f}', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "            xytext=(0, 3), textcoords='offset points', ha='center', fontsize=10,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Test R2 Score')\n",
    "    ax.set_title('Prediction Accuracy Comparison (v4.1)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([METHOD_NAMES[m] for m in methods])\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig3 = plot_test_r2_comparison(df, save_path=FIGURES_DIR / 'fig3_test_r2_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 4: DIMENSIONAL INFORMATION BENEFIT\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_dims_benefit(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart showing performance with vs without dimensional info (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    physics_sr_df = core_df[core_df['method'] == 'physics_sr']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    equations = get_available_equations(physics_sr_df)\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.35\n",
    "    \n",
    "    with_dims_means = []\n",
    "    without_dims_means = []\n",
    "    for equation in equations:\n",
    "        subset_with = physics_sr_df[(physics_sr_df['equation_name'] == equation) & (physics_sr_df['with_dims'] == True)]\n",
    "        subset_without = physics_sr_df[(physics_sr_df['equation_name'] == equation) & (physics_sr_df['with_dims'] == False)]\n",
    "        with_dims_means.append(subset_with['var_f1'].mean() if len(subset_with) > 0 else 0)\n",
    "        without_dims_means.append(subset_without['var_f1'].mean() if len(subset_without) > 0 else 0)\n",
    "    \n",
    "    ax.bar(x - width/2, with_dims_means, width, label='With Dimensions', color='#2E86AB', alpha=0.8)\n",
    "    ax.bar(x + width/2, without_dims_means, width, label='Without Dimensions', color='#E94F37', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Benefit of Dimensional Information (Physics-SR v4.1)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.15)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig4 = plot_dims_benefit(df, save_path=FIGURES_DIR / 'fig4_dims_benefit.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 5: RUNTIME COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_runtime_comparison(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Bar chart comparing runtime by method (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    methods = [m for m in methods if m in core_df['method'].unique()]\n",
    "    x = np.arange(len(methods))\n",
    "    \n",
    "    means = []\n",
    "    stds = []\n",
    "    colors = []\n",
    "    for method in methods:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        means.append(subset['runtime_seconds'].mean())\n",
    "        stds.append(subset['runtime_seconds'].std())\n",
    "        colors.append(METHOD_COLORS[method])\n",
    "    \n",
    "    bars = ax.bar(x, means, yerr=stds, color=colors, capsize=5, alpha=0.8)\n",
    "    \n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax.annotate(\n",
    "            f'{mean:.1f}s', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "            xytext=(0, 3), textcoords='offset points', ha='center', fontsize=10,\n",
    "        )\n",
    "    \n",
    "    ax.axhline(y=180, color='red', linestyle='--', label='Colab Pro Budget (180s)', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Runtime (seconds)')\n",
    "    ax.set_title('Computational Cost Comparison (v4.1)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([METHOD_NAMES[m] for m in methods])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig5 = plot_runtime_comparison(df, save_path=FIGURES_DIR / 'fig5_runtime.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 6: COMPREHENSIVE HEATMAP\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_comprehensive_heatmap(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Heatmap showing F1 scores across all experimental conditions (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    core_df['row_label'] = core_df['equation_name'].map(EQUATION_NAMES) + ' | ' + core_df['method'].map(METHOD_NAMES)\n",
    "    core_df['col_label'] = (\n",
    "        'Noise=' + (core_df['noise_level'] * 100).astype(int).astype(str) + '%, ' +\n",
    "        'Dummy=' + core_df['n_dummy'].astype(str) + ', ' +\n",
    "        'Dims=' + core_df['with_dims'].map({True: 'T', False: 'F'})\n",
    "    )\n",
    "    \n",
    "    pivot = core_df.pivot_table(\n",
    "        index='row_label',\n",
    "        columns='col_label',\n",
    "        values='var_f1',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    equations = [EQUATION_NAMES.get(eq, eq) for eq in get_available_equations(core_df)]\n",
    "    methods = ['Physics-SR', 'PySR-Only', 'LASSO+PySR']\n",
    "    row_order = [f\"{eq} | {m}\" for eq in equations for m in methods]\n",
    "    pivot = pivot.reindex([r for r in row_order if r in pivot.index])\n",
    "    \n",
    "    col_order = sorted(pivot.columns)\n",
    "    pivot = pivot[col_order]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['large'])\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='RdYlGn',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'F1 Score'},\n",
    "        annot_kws={'size': 8},\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Experimental Condition')\n",
    "    ax.set_ylabel('Equation | Method')\n",
    "    ax.set_title('Variable Selection F1 Score Across All Conditions (v4.1)')\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig6 = plot_comprehensive_heatmap(df, save_path=FIGURES_DIR / 'fig6_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 7: F1 COMPARISON BY EQUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_comparison_by_equation(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart comparing F1 by method and equation (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['wide'])\n",
    "    \n",
    "    equations = get_available_equations(core_df)\n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        means = []\n",
    "        stds = []\n",
    "        for eq in equations:\n",
    "            subset = core_df[(core_df['method'] == method) & (core_df['equation_name'] == eq)]\n",
    "            if len(subset) > 0:\n",
    "                means.append(subset['var_f1'].mean())\n",
    "                stds.append(subset['var_f1'].std())\n",
    "            else:\n",
    "                means.append(0)\n",
    "                stds.append(0)\n",
    "        \n",
    "        ax.bar(\n",
    "            x + i * width - width, means, width, yerr=stds,\n",
    "            label=METHOD_NAMES[method], color=METHOD_COLORS[method],\n",
    "            capsize=3, alpha=0.8,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection F1 by Method and Equation (v4.1)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"{EQUATION_NAMES.get(eq, eq)}\\n({AI_FEYNMAN_IDS.get(eq, '')})\" for eq in equations])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.15)\n",
    "    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig7 = plot_f1_comparison_by_equation(df, save_path=FIGURES_DIR / 'fig7_f1_by_equation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: v4.1 Library Analysis Visualizations (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 8: LIBRARY COMPOSITION BY EQUATION (v4.1 NEW)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_library_composition(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Stacked bar chart showing 4-layer library composition by equation (v4.1 NEW).\n",
    "    \n",
    "    Only for Physics-SR method.\n",
    "    \"\"\"\n",
    "    physics_sr = df[df['method'] == 'physics_sr'].copy()\n",
    "    \n",
    "    if 'library_n_pysr' not in physics_sr.columns:\n",
    "        print(\"Warning: Library composition columns not found in results.\")\n",
    "        fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "        ax.text(0.5, 0.5, 'Library composition data not available', ha='center', va='center', fontsize=14)\n",
    "        ax.set_title('Library Composition (Not Available)')\n",
    "        return fig\n",
    "    \n",
    "    grouped = physics_sr.groupby('equation_name').agg({\n",
    "        'library_n_pysr': 'mean',\n",
    "        'library_n_variant': 'mean',\n",
    "        'library_n_poly': 'mean',\n",
    "        'library_n_op': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    equations = grouped['equation_name']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.6\n",
    "    \n",
    "    bottom = np.zeros(len(equations))\n",
    "    colors = [LIBRARY_COLORS['pysr'], LIBRARY_COLORS['variant'], LIBRARY_COLORS['poly'], LIBRARY_COLORS['op']]\n",
    "    labels = ['[PySR] Exact', '[Var] Variants', '[Poly] Polynomial', '[Op] Operators']\n",
    "    \n",
    "    for col, color, label in zip(\n",
    "        ['library_n_pysr', 'library_n_variant', 'library_n_poly', 'library_n_op'],\n",
    "        colors, labels\n",
    "    ):\n",
    "        values = grouped[col].fillna(0).values\n",
    "        ax.bar(x, values, width, bottom=bottom, label=label, color=color)\n",
    "        bottom += values\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Number of Library Terms')\n",
    "    ax.set_title('v4.0 Augmented Library Composition by Equation')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig8 = plot_library_composition(df, save_path=FIGURES_DIR / 'fig8_library_composition.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 9: SELECTION SOURCES BY EQUATION (v4.1 NEW)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_selection_sources(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Stacked bar chart showing where selected terms came from (v4.1 NEW).\n",
    "    \n",
    "    This visualizes the v4.0 innovation: PySR terms can be kept,\n",
    "    polynomial terms can fill gaps.\n",
    "    \"\"\"\n",
    "    physics_sr = df[df['method'] == 'physics_sr'].copy()\n",
    "    \n",
    "    if 'selected_from_pysr' not in physics_sr.columns:\n",
    "        print(\"Warning: Selection source columns not found in results.\")\n",
    "        fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "        ax.text(0.5, 0.5, 'Selection source data not available', ha='center', va='center', fontsize=14)\n",
    "        ax.set_title('Selection Sources (Not Available)')\n",
    "        return fig\n",
    "    \n",
    "    grouped = physics_sr.groupby('equation_name').agg({\n",
    "        'selected_from_pysr': 'mean',\n",
    "        'selected_from_variant': 'mean',\n",
    "        'selected_from_poly': 'mean',\n",
    "        'selected_from_op': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    equations = grouped['equation_name']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.6\n",
    "    \n",
    "    bottom = np.zeros(len(equations))\n",
    "    colors = [LIBRARY_COLORS['pysr'], LIBRARY_COLORS['variant'], LIBRARY_COLORS['poly'], LIBRARY_COLORS['op']]\n",
    "    labels = ['From PySR', 'From Variants', 'From Polynomial', 'From Operators']\n",
    "    \n",
    "    for col, color, label in zip(\n",
    "        ['selected_from_pysr', 'selected_from_variant', 'selected_from_poly', 'selected_from_op'],\n",
    "        colors, labels\n",
    "    ):\n",
    "        values = grouped[col].fillna(0).values\n",
    "        ax.bar(x, values, width, bottom=bottom, label=label, color=color)\n",
    "        bottom += values\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Number of Selected Terms')\n",
    "    ax.set_title('Source of Final Equation Terms (v4.0 Structure-Guided)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig9 = plot_selection_sources(df, save_path=FIGURES_DIR / 'fig9_selection_sources.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 10: PYSR VS POLYNOMIAL CONTRIBUTION (v4.1 NEW)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_pysr_vs_polynomial(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart comparing PySR vs Polynomial layer contribution (v4.1 NEW).\n",
    "    \"\"\"\n",
    "    physics_sr = df[df['method'] == 'physics_sr'].copy()\n",
    "    \n",
    "    if 'selected_from_pysr' not in physics_sr.columns or 'selected_from_poly' not in physics_sr.columns:\n",
    "        print(\"Warning: Selection source columns not found in results.\")\n",
    "        fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "        ax.text(0.5, 0.5, 'Data not available', ha='center', va='center', fontsize=14)\n",
    "        ax.set_title('PySR vs Polynomial Contribution (Not Available)')\n",
    "        return fig\n",
    "    \n",
    "    grouped = physics_sr.groupby('equation_name').agg({\n",
    "        'selected_from_pysr': 'mean',\n",
    "        'selected_from_poly': 'mean',\n",
    "        'library_n_pysr': 'mean',\n",
    "        'library_n_poly': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=FIGURE_SIZES['wide'])\n",
    "    \n",
    "    equations = grouped['equation_name']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Left: Library size comparison\n",
    "    ax1 = axes[0]\n",
    "    ax1.bar(x - width/2, grouped['library_n_pysr'].fillna(0), width, label='PySR Layer', color=LIBRARY_COLORS['pysr'])\n",
    "    ax1.bar(x + width/2, grouped['library_n_poly'].fillna(0), width, label='Polynomial Layer', color=LIBRARY_COLORS['poly'])\n",
    "    ax1.set_xlabel('Equation')\n",
    "    ax1.set_ylabel('Number of Terms')\n",
    "    ax1.set_title('Library Size: PySR vs Polynomial')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Right: Selection comparison\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(x - width/2, grouped['selected_from_pysr'].fillna(0), width, label='Selected from PySR', color=LIBRARY_COLORS['pysr'])\n",
    "    ax2.bar(x + width/2, grouped['selected_from_poly'].fillna(0), width, label='Selected from Polynomial', color=LIBRARY_COLORS['poly'])\n",
    "    ax2.set_xlabel('Equation')\n",
    "    ax2.set_ylabel('Number of Selected Terms')\n",
    "    ax2.set_title('Selection: PySR vs Polynomial')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('v4.0 Structure-Guided: PySR vs Polynomial Layer Analysis', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig10 = plot_pysr_vs_polynomial(df, save_path=FIGURES_DIR / 'fig10_pysr_vs_polynomial.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 11: TIMING PROFILE BREAKDOWN (v4.1 NEW)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_timing_profile(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Stacked bar chart showing timing breakdown by stage (v4.1 NEW).\n",
    "    \n",
    "    Only for Physics-SR method.\n",
    "    \"\"\"\n",
    "    physics_sr = df[df['method'] == 'physics_sr'].copy()\n",
    "    \n",
    "    if 'timing_stage1' not in physics_sr.columns:\n",
    "        print(\"Warning: Timing profile columns not found in results.\")\n",
    "        fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "        ax.text(0.5, 0.5, 'Timing profile data not available', ha='center', va='center', fontsize=14)\n",
    "        ax.set_title('Timing Profile (Not Available)')\n",
    "        return fig\n",
    "    \n",
    "    grouped = physics_sr.groupby('equation_name').agg({\n",
    "        'timing_stage1': 'mean',\n",
    "        'timing_pysr': 'mean',\n",
    "        'timing_library': 'mean',\n",
    "        'timing_ewsindy': 'mean',\n",
    "        'timing_stage3': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    equations = grouped['equation_name']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.6\n",
    "    \n",
    "    bottom = np.zeros(len(equations))\n",
    "    colors = list(TIMING_COLORS.values())\n",
    "    labels = ['Stage 1 (Screening)', 'PySR Discovery', 'Library Build', 'E-WSINDy', 'Stage 3 (UQ)']\n",
    "    \n",
    "    for col, color, label in zip(\n",
    "        ['timing_stage1', 'timing_pysr', 'timing_library', 'timing_ewsindy', 'timing_stage3'],\n",
    "        colors, labels\n",
    "    ):\n",
    "        values = grouped[col].fillna(0).values\n",
    "        ax.bar(x, values, width, bottom=bottom, label=label, color=color)\n",
    "        bottom += values\n",
    "    \n",
    "    ax.axhline(y=180, color='red', linestyle='--', label='Colab Pro Budget (180s)', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_title('v4.1 Computational Profile by Equation')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig11 = plot_timing_profile(df, save_path=FIGURES_DIR / 'fig11_timing_profile.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Supplementary Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 12: SAMPLE SIZE SENSITIVITY (PHYSICS-SR ONLY)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_sample_size_sensitivity(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing performance vs sample size for Physics-SR (v4.1).\n",
    "    \"\"\"\n",
    "    physics_sr_df = df[df['method'] == 'physics_sr'].copy()\n",
    "    \n",
    "    if 'n_samples' not in physics_sr_df.columns or len(physics_sr_df['n_samples'].unique()) < 2:\n",
    "        print(\"Warning: Not enough sample size variation for sensitivity plot.\")\n",
    "        fig, ax = plt.subplots(figsize=FIGURE_SIZES['wide'])\n",
    "        ax.text(0.5, 0.5, 'Insufficient sample size variation', ha='center', va='center', fontsize=14)\n",
    "        ax.set_title('Sample Size Sensitivity (Not Available)')\n",
    "        return fig\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=FIGURE_SIZES['wide'])\n",
    "    equations = get_available_equations(physics_sr_df)\n",
    "    \n",
    "    styles = {\n",
    "        'coulomb':   {'marker': 'o', 'markersize': 10, 'linewidth': 2.5, 'color': '#2E86AB'},\n",
    "        'cosines':   {'marker': 's', 'markersize': 8,  'linewidth': 2.0, 'color': '#E94F37'},\n",
    "        'barometric': {'marker': '^', 'markersize': 8,  'linewidth': 2.0, 'color': '#4CAF50'},\n",
    "        'dotproduct': {'marker': 'd', 'markersize': 8,  'linewidth': 2.0, 'color': '#9C27B0'},\n",
    "    }\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    for equation in equations:\n",
    "        subset = physics_sr_df[physics_sr_df['equation_name'] == equation]\n",
    "        if len(subset) > 0 and len(subset['n_samples'].unique()) > 1:\n",
    "            means = subset.groupby('n_samples')['var_f1'].mean()\n",
    "            style = styles.get(equation, {'marker': 'o', 'markersize': 8, 'linewidth': 2.0, 'color': 'gray'})\n",
    "            ax1.plot(\n",
    "                means.index,\n",
    "                means.values,\n",
    "                marker=style['marker'],\n",
    "                markersize=style['markersize'],\n",
    "                linewidth=style['linewidth'],\n",
    "                color=style['color'],\n",
    "                label=EQUATION_NAMES.get(equation, equation),\n",
    "            )\n",
    "    \n",
    "    ax1.set_xlabel('Sample Size (n)')\n",
    "    ax1.set_ylabel('Variable Selection F1 Score')\n",
    "    ax1.set_title('F1 Score vs Sample Size')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    for equation in equations:\n",
    "        subset = physics_sr_df[physics_sr_df['equation_name'] == equation]\n",
    "        if len(subset) > 0 and len(subset['n_samples'].unique()) > 1:\n",
    "            means = subset.groupby('n_samples')['test_r2'].mean()\n",
    "            style = styles.get(equation, {'marker': 'o', 'markersize': 8, 'linewidth': 2.0, 'color': 'gray'})\n",
    "            ax2.plot(\n",
    "                means.index,\n",
    "                means.values,\n",
    "                marker=style['marker'],\n",
    "                markersize=style['markersize'],\n",
    "                linewidth=style['linewidth'],\n",
    "                color=style['color'],\n",
    "                label=EQUATION_NAMES.get(equation, equation),\n",
    "            )\n",
    "    \n",
    "    ax2.set_xlabel('Sample Size (n)')\n",
    "    ax2.set_ylabel('Test R2 Score')\n",
    "    ax2.set_title('Test R2 vs Sample Size')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Sample Size Sensitivity (Physics-SR v4.1)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig12 = plot_sample_size_sensitivity(df, save_path=FIGURES_DIR / 'fig12_sample_size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 13: NOISE ROBUSTNESS BY EQUATION TYPE\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_noise_robustness_by_type(df: pd.DataFrame, save_path: Optional[Path] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Compare noise robustness across equation types (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=FIGURE_SIZES['wide'])\n",
    "    \n",
    "    eq_types = [t for t in core_df['eq_type'].unique() if pd.notna(t)]\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    for eq_type in eq_types:\n",
    "        subset = core_df[(core_df['eq_type'] == eq_type) & (core_df['method'] == 'physics_sr')]\n",
    "        if len(subset) > 0:\n",
    "            means = subset.groupby('noise_level')['var_f1'].mean()\n",
    "            ax1.plot(\n",
    "                means.index * 100,\n",
    "                means.values,\n",
    "                marker='o',\n",
    "                markersize=8,\n",
    "                linewidth=2,\n",
    "                label=EQUATION_TYPE_NAMES.get(eq_type, eq_type),\n",
    "            )\n",
    "    \n",
    "    ax1.set_xlabel('Noise Level (%)')\n",
    "    ax1.set_ylabel('Variable Selection F1 Score')\n",
    "    ax1.set_title('Physics-SR Noise Robustness by Equation Type')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) > 0:\n",
    "            means = subset.groupby('noise_level')['var_f1'].mean()\n",
    "            ax2.plot(\n",
    "                means.index * 100,\n",
    "                means.values,\n",
    "                marker='o',\n",
    "                markersize=8,\n",
    "                linewidth=2,\n",
    "                color=METHOD_COLORS[method],\n",
    "                label=METHOD_NAMES[method],\n",
    "            )\n",
    "    \n",
    "    ax2.set_xlabel('Noise Level (%)')\n",
    "    ax2.set_ylabel('Variable Selection F1 Score')\n",
    "    ax2.set_title('Method Comparison: Noise Robustness')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Noise Robustness Analysis (v4.1)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig13 = plot_noise_robustness_by_type(df, save_path=FIGURES_DIR / 'fig13_noise_robustness.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: LaTeX Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 1: MAIN RESULTS SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_main_results_table(df: pd.DataFrame, save_path: Optional[Path] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for main results (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Main Benchmark Results (v4.1 - Core Experiments)}\n",
    "\\label{tab:main_results}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "Method & Variable Selection F1 & Test $R^2$ & Runtime (s) \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        \n",
    "        f1_mean = subset['var_f1'].mean()\n",
    "        f1_std = subset['var_f1'].std()\n",
    "        r2_mean = subset['test_r2'].mean()\n",
    "        r2_std = subset['test_r2'].std()\n",
    "        rt_mean = subset['runtime_seconds'].mean()\n",
    "        rt_std = subset['runtime_seconds'].std()\n",
    "        \n",
    "        method_display = METHOD_NAMES.get(method, method)\n",
    "        \n",
    "        latex += f\"{method_display} & {f1_mean:.3f} $\\\\pm$ {f1_std:.3f} & \"\n",
    "        latex += f\"{r2_mean:.3f} $\\\\pm$ {r2_std:.3f} & \"\n",
    "        latex += f\"{rt_mean:.1f} $\\\\pm$ {rt_std:.1f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(latex)\n",
    "        print(f\"Table saved to {save_path}\")\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "table1_latex = generate_main_results_table(df, save_path=TABLES_DIR / 'table1_main_results.tex')\n",
    "print(\"TABLE 1: Main Results Summary (v4.1)\")\n",
    "print(\"=\" * 60)\n",
    "print(table1_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 2: VARIABLE SELECTION METRICS BY METHOD\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_variable_selection_table(df: pd.DataFrame, save_path: Optional[Path] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for variable selection metrics (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Variable Selection Performance (v4.1)}\n",
    "\\label{tab:var_selection}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "Method & Precision & Recall & F1 & Exact Match \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        prec = subset['var_precision'].mean()\n",
    "        rec = subset['var_recall'].mean()\n",
    "        f1 = subset['var_f1'].mean()\n",
    "        exact = subset['selected_correct'].mean() * 100\n",
    "        \n",
    "        method_display = METHOD_NAMES.get(method, method)\n",
    "        latex += f\"{method_display} & {prec:.3f} & {rec:.3f} & {f1:.3f} & {exact:.1f}\\\\% \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(latex)\n",
    "        print(f\"Table saved to {save_path}\")\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "table2_latex = generate_variable_selection_table(df, save_path=TABLES_DIR / 'table2_var_selection.tex')\n",
    "print(\"\\nTABLE 2: Variable Selection Metrics (v4.1)\")\n",
    "print(\"=\" * 60)\n",
    "print(table2_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 3: PREDICTION ACCURACY BY EQUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_prediction_table(df: pd.DataFrame, save_path: Optional[Path] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for prediction accuracy by equation (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Prediction Accuracy by Equation (v4.1)}\n",
    "\\label{tab:prediction}\n",
    "\\begin{tabular}{llccc}\n",
    "\\toprule\n",
    "Equation & AI Feynman ID & Physics-SR & PySR-Only & LASSO+PySR \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for eq_name in get_available_equations(core_df):\n",
    "        ai_id = AI_FEYNMAN_IDS.get(eq_name, 'Unknown')\n",
    "        eq_display = EQUATION_NAMES.get(eq_name, eq_name)\n",
    "        \n",
    "        r2_values = []\n",
    "        for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "            subset = core_df[(core_df['equation_name'] == eq_name) & (core_df['method'] == method)]\n",
    "            if len(subset) > 0:\n",
    "                r2_values.append(f\"{subset['test_r2'].mean():.3f}\")\n",
    "            else:\n",
    "                r2_values.append(\"--\")\n",
    "        \n",
    "        latex += f\"{eq_display} & {ai_id} & {r2_values[0]} & {r2_values[1]} & {r2_values[2]} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(latex)\n",
    "        print(f\"Table saved to {save_path}\")\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "table3_latex = generate_prediction_table(df, save_path=TABLES_DIR / 'table3_prediction.tex')\n",
    "print(\"\\nTABLE 3: Prediction Accuracy by Equation (v4.1)\")\n",
    "print(\"=\" * 60)\n",
    "print(table3_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 4: LIBRARY COMPOSITION (v4.1 NEW)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_library_table(df: pd.DataFrame, save_path: Optional[Path] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for library composition analysis (v4.1 NEW).\n",
    "    \"\"\"\n",
    "    physics_sr = df[df['method'] == 'physics_sr'].copy()\n",
    "    \n",
    "    if 'library_n_pysr' not in physics_sr.columns:\n",
    "        print(\"Warning: Library composition data not available.\")\n",
    "        return \"% Library composition data not available\"\n",
    "    \n",
    "    grouped = physics_sr.groupby('equation_name').agg({\n",
    "        'library_n_total': 'mean',\n",
    "        'library_n_pysr': 'mean',\n",
    "        'library_n_variant': 'mean',\n",
    "        'library_n_poly': 'mean',\n",
    "        'library_n_op': 'mean',\n",
    "        'selected_from_pysr': 'mean',\n",
    "        'selected_from_poly': 'mean',\n",
    "    }).round(1)\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{v4.0 Augmented Library Composition and Selection Analysis}\n",
    "\\label{tab:library}\n",
    "\\begin{tabular}{l|cccc|c|cc}\n",
    "\\toprule\n",
    "Equation & [PySR] & [Var] & [Poly] & [Op] & Total & Sel. PySR & Sel. Poly \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for eq_name in get_available_equations(df):\n",
    "        if eq_name not in grouped.index:\n",
    "            continue\n",
    "        row = grouped.loc[eq_name]\n",
    "        eq_display = EQUATION_NAMES.get(eq_name, eq_name)\n",
    "        \n",
    "        latex += f\"{eq_display} & \"\n",
    "        latex += f\"{row.get('library_n_pysr', 0):.0f} & \"\n",
    "        latex += f\"{row.get('library_n_variant', 0):.0f} & \"\n",
    "        latex += f\"{row.get('library_n_poly', 0):.0f} & \"\n",
    "        latex += f\"{row.get('library_n_op', 0):.0f} & \"\n",
    "        latex += f\"{row.get('library_n_total', 0):.0f} & \"\n",
    "        latex += f\"{row.get('selected_from_pysr', 0):.1f} & \"\n",
    "        latex += f\"{row.get('selected_from_poly', 0):.1f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(latex)\n",
    "        print(f\"Table saved to {save_path}\")\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "table4_latex = generate_library_table(df, save_path=TABLES_DIR / 'table4_library.tex')\n",
    "print(\"\\nTABLE 4: Library Composition (v4.1 NEW)\")\n",
    "print(\"=\" * 60)\n",
    "print(table4_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PAIRED COMPARISON TESTS\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_paired_comparisons(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform paired comparison tests between Physics-SR and baselines (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    equations = get_available_equations(core_df)\n",
    "    \n",
    "    for baseline in ['pysr_only', 'lasso_pysr']:\n",
    "        physics_sr_scores = []\n",
    "        baseline_scores = []\n",
    "        \n",
    "        for eq in equations:\n",
    "            ps = core_df[(core_df['method'] == 'physics_sr') & (core_df['equation_name'] == eq)]['var_f1'].mean()\n",
    "            bl = core_df[(core_df['method'] == baseline) & (core_df['equation_name'] == eq)]['var_f1'].mean()\n",
    "            if not np.isnan(ps) and not np.isnan(bl):\n",
    "                physics_sr_scores.append(ps)\n",
    "                baseline_scores.append(bl)\n",
    "        \n",
    "        if len(physics_sr_scores) > 1:\n",
    "            t_stat, p_value = stats.ttest_rel(physics_sr_scores, baseline_scores)\n",
    "            \n",
    "            try:\n",
    "                wilcoxon_stat, wilcoxon_p = stats.wilcoxon(physics_sr_scores, baseline_scores)\n",
    "            except:\n",
    "                wilcoxon_stat, wilcoxon_p = np.nan, np.nan\n",
    "            \n",
    "            diff = np.array(physics_sr_scores) - np.array(baseline_scores)\n",
    "            mean_diff = np.mean(diff)\n",
    "            ci_low, ci_high = stats.t.interval(0.95, len(diff)-1, loc=mean_diff, scale=stats.sem(diff))\n",
    "            \n",
    "            results[f'physics_sr_vs_{baseline}'] = {\n",
    "                'paired_t_stat': t_stat,\n",
    "                'paired_t_pvalue': p_value,\n",
    "                'wilcoxon_stat': wilcoxon_stat,\n",
    "                'wilcoxon_pvalue': wilcoxon_p,\n",
    "                'mean_difference': mean_diff,\n",
    "                'ci_95_low': ci_low,\n",
    "                'ci_95_high': ci_high,\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "paired_tests = compute_paired_comparisons(df)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PAIRED COMPARISON TESTS (v4.1)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for comparison, result in paired_tests.items():\n",
    "    print(f\"\\n{comparison.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Paired t-test: t={result['paired_t_stat']:.4f}, p={result['paired_t_pvalue']:.6f}\")\n",
    "    print(f\"  Wilcoxon test: W={result['wilcoxon_stat']:.4f}, p={result['wilcoxon_pvalue']:.6f}\")\n",
    "    print(f\"  Mean difference: {result['mean_difference']:.4f}\")\n",
    "    print(f\"  95% CI: [{result['ci_95_low']:.4f}, {result['ci_95_high']:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EFFECT SIZE CALCULATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_effect_sizes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate Cohen's d effect sizes for all pairwise comparisons (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    methods = [m for m in methods if m in core_df['method'].unique()]\n",
    "    \n",
    "    effect_sizes = []\n",
    "    \n",
    "    for i, m1 in enumerate(methods):\n",
    "        for m2 in methods[i+1:]:\n",
    "            scores1 = core_df[core_df['method'] == m1]['var_f1'].values\n",
    "            scores2 = core_df[core_df['method'] == m2]['var_f1'].values\n",
    "            \n",
    "            pooled_std = np.sqrt((scores1.std()**2 + scores2.std()**2) / 2)\n",
    "            cohens_d = (scores1.mean() - scores2.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            interpretation = 'negligible'\n",
    "            if abs(cohens_d) >= 0.8:\n",
    "                interpretation = 'large'\n",
    "            elif abs(cohens_d) >= 0.5:\n",
    "                interpretation = 'medium'\n",
    "            elif abs(cohens_d) >= 0.2:\n",
    "                interpretation = 'small'\n",
    "            \n",
    "            effect_sizes.append({\n",
    "                'Comparison': f\"{METHOD_NAMES[m1]} vs {METHOD_NAMES[m2]}\",\n",
    "                'Mean1': f\"{scores1.mean():.3f}\",\n",
    "                'Mean2': f\"{scores2.mean():.3f}\",\n",
    "                'Cohen\\'s d': f\"{cohens_d:.3f}\",\n",
    "                'Interpretation': interpretation,\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(effect_sizes)\n",
    "\n",
    "\n",
    "effect_size_df = compute_effect_sizes(df)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EFFECT SIZE ANALYSIS (Cohen's d)\")\n",
    "print(\"=\" * 80)\n",
    "display(effect_size_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# KEY FINDINGS SUMMARY (v4.1)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_key_findings(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Generate key findings summary (v4.1).\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\" KEY FINDINGS SUMMARY (v4.1)\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # 1. Overall performance\n",
    "    print(\"1. OVERALL PERFORMANCE\")\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) > 0:\n",
    "            print(f\"   {METHOD_NAMES[method]:15s}: F1={subset['var_f1'].mean():.3f}, R2={subset['test_r2'].mean():.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # 2. Best method\n",
    "    best_method_f1 = core_df.groupby('method')['var_f1'].mean().idxmax()\n",
    "    best_f1 = core_df.groupby('method')['var_f1'].mean().max()\n",
    "    print(f\"2. BEST METHOD (by F1): {METHOD_NAMES[best_method_f1]} ({best_f1:.3f})\")\n",
    "    print()\n",
    "    \n",
    "    # 3. Dimensional analysis benefit\n",
    "    physics_sr = core_df[core_df['method'] == 'physics_sr']\n",
    "    if len(physics_sr) > 0:\n",
    "        with_dims = physics_sr[physics_sr['with_dims'] == True]['var_f1'].mean()\n",
    "        without_dims = physics_sr[physics_sr['with_dims'] == False]['var_f1'].mean()\n",
    "        improvement = with_dims - without_dims\n",
    "        print(f\"3. DIMENSIONAL ANALYSIS BENEFIT\")\n",
    "        print(f\"   With dimensions:    F1={with_dims:.3f}\")\n",
    "        print(f\"   Without dimensions: F1={without_dims:.3f}\")\n",
    "        print(f\"   Improvement:        +{improvement:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    # 4. Noise robustness\n",
    "    print(\"4. NOISE ROBUSTNESS (0% -> 5% noise degradation)\")\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) > 0:\n",
    "            f1_0 = subset[subset['noise_level'] == 0.0]['var_f1'].mean()\n",
    "            f1_5 = subset[subset['noise_level'] == 0.05]['var_f1'].mean()\n",
    "            degradation = f1_0 - f1_5\n",
    "            print(f\"   {METHOD_NAMES[method]:15s}: {degradation:+.3f} (F1 drop)\")\n",
    "    print()\n",
    "    \n",
    "    # 5. Runtime comparison\n",
    "    print(\"5. COMPUTATIONAL EFFICIENCY\")\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) > 0:\n",
    "            rt = subset['runtime_seconds'].mean()\n",
    "            print(f\"   {METHOD_NAMES[method]:15s}: {rt:.1f}s average\")\n",
    "    print()\n",
    "    \n",
    "    # 6. v4.0 Library benefit (if available)\n",
    "    if 'library_n_pysr' in df.columns:\n",
    "        print(\"6. v4.0 STRUCTURE-GUIDED LIBRARY BENEFIT\")\n",
    "        physics_sr = df[df['method'] == 'physics_sr']\n",
    "        if len(physics_sr) > 0:\n",
    "            avg_pysr = physics_sr['library_n_pysr'].mean()\n",
    "            avg_poly = physics_sr['library_n_poly'].mean()\n",
    "            avg_total = physics_sr['library_n_total'].mean()\n",
    "            print(f\"   Average PySR terms:      {avg_pysr:.1f}\")\n",
    "            print(f\"   Average Polynomial terms: {avg_poly:.1f}\")\n",
    "            print(f\"   Average Total library:   {avg_total:.1f}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "generate_key_findings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL OUTPUT SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" ANALYSIS COMPLETE (v4.1)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"OUTPUT FILES GENERATED:\")\n",
    "print()\n",
    "\n",
    "# List figures\n",
    "print(\"FIGURES:\")\n",
    "for fig_file in sorted(FIGURES_DIR.glob('*.png')):\n",
    "    print(f\"  - {fig_file}\")\n",
    "print()\n",
    "\n",
    "# List tables\n",
    "print(\"TABLES:\")\n",
    "for table_file in sorted(TABLES_DIR.glob('*.tex')):\n",
    "    print(f\"  - {table_file}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" Ready for publication!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Module Summary\n",
    "\n",
    "### Analysis.ipynb v4.1 - Complete\n",
    "\n",
    "**Sections:**\n",
    "1. Header and Imports\n",
    "2. Load Results (experiment_results.csv/pkl)\n",
    "3. Summary Statistics (method comparison, equation comparison, statistical tests)\n",
    "4. Core Visualizations (Figures 1-7)\n",
    "5. v4.1 Library Analysis (Figures 8-11 - NEW)\n",
    "6. Supplementary Visualizations (Figures 12-13)\n",
    "7. LaTeX Tables (4 tables including library composition)\n",
    "8. Statistical Analysis (paired tests, effect sizes)\n",
    "9. Conclusions (key findings summary)\n",
    "\n",
    "**v4.1 New Features:**\n",
    "- AI Feynman ID tracking for all equations\n",
    "- Library composition visualizations (Figures 8-11)\n",
    "- Selection source analysis\n",
    "- Timing profile breakdown\n",
    "- Table 4: Library Composition LaTeX table\n",
    "\n",
    "**Output Files:**\n",
    "- 13 PNG figures in results/figures/\n",
    "- 4 LaTeX tables in results/tables/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}