{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Physics-SR Framework v3.0 Benchmark\n",
    "\n",
    "## Results Analysis and Visualization Module\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook analyzes and visualizes the benchmark results from Experiments.ipynb:\n",
    "\n",
    "1. **Summary Statistics**: Overall, by-factor, and by-equation-type performance metrics\n",
    "2. **Core Visualizations**: 8 figures for main results\n",
    "3. **Supplementary Visualizations**: 3 figures for additional analysis\n",
    "4. **LaTeX Tables**: Publication-ready tables for academic papers\n",
    "5. **Statistical Tests**: Significance testing between methods\n",
    "\n",
    "### Input Files\n",
    "\n",
    "- `results/experiment_results.csv`: Main results table\n",
    "- `results/experiment_results_full.pkl`: Detailed results with nested data\n",
    "\n",
    "### Output Files\n",
    "\n",
    "- `results/figures/*.png`: 11 visualization figures\n",
    "- `results/tables/*.tex`: 5 LaTeX tables\n",
    "- `results/summary_table.csv`: Final summary table\n",
    "\n",
    "### Test Equations\n",
    "\n",
    "| # | Name | Equation | Type | Category |\n",
    "|---|------|----------|------|----------|\n",
    "| 1 | Coulomb | F = k * q1 * q2 / r^2 | Rational | Power-Law |\n",
    "| 2 | Newton | F = G * m1 * m2 / r^2 | Rational | Power-Law |\n",
    "| 3 | Ideal Gas | P = n * R * T / V | Rational | Power-Law |\n",
    "| 4 | Damped | x = A * exp(-b*t) * cos(omega*t) | Nested | Transcendental |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENVIRONMENT RESET AND FRESH CLONE\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    import shutil\n",
    "    import gc\n",
    "    \n",
    "    repo_path = '/content/Physics-Informed-Symbolic-Regression'\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    if not current_dir.startswith(repo_path) or not os.path.exists(repo_path + '/.git'):\n",
    "        os.chdir('/content')\n",
    "        gc.collect()\n",
    "        \n",
    "        if os.path.exists(repo_path):\n",
    "            shutil.rmtree(repo_path)\n",
    "            print(\"[OK] Removed existing repository.\")\n",
    "        \n",
    "        !git clone https://github.com/Garthzzz/Physics-Informed-Symbolic-Regression.git\n",
    "        \n",
    "        if os.path.exists(repo_path + '/.git'):\n",
    "            os.chdir(repo_path + '/benchmark')\n",
    "            print(f\"[OK] Working directory: {os.getcwd()}\")\n",
    "        else:\n",
    "            print(\"[FAIL] Clone incomplete!\")\n",
    "        \n",
    "        print(\"[OK] Environment reset complete.\")\n",
    "    else:\n",
    "        print(\"[SKIP] Already in valid repository.\")\n",
    "else:\n",
    "    print(\"[INFO] Not in Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analysis.ipynb - Results Analysis and Visualization Module\n",
    "===========================================================\n",
    "\n",
    "Physics-SR Framework v3.0 Benchmark Suite\n",
    "\n",
    "This module provides:\n",
    "- Summary statistics computation\n",
    "- Visualization functions for benchmark results\n",
    "- LaTeX table generation for publications\n",
    "- Statistical significance testing\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Analysis: All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Determine paths based on environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Colab paths\n",
    "    BASE_DIR = Path('/content/Physics-Informed-Symbolic-Regression')\n",
    "    BENCHMARK_DIR = BASE_DIR / 'benchmark'\n",
    "else:\n",
    "    # Local paths\n",
    "    BENCHMARK_DIR = Path('.').resolve()\n",
    "    BASE_DIR = BENCHMARK_DIR.parent\n",
    "\n",
    "RESULTS_DIR = BENCHMARK_DIR / 'results'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "TABLES_DIR = RESULTS_DIR / 'tables'\n",
    "DATA_DIR = BENCHMARK_DIR / 'data'\n",
    "\n",
    "# Create directories if needed\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TABLES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Benchmark directory: {BENCHMARK_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")\n",
    "print(f\"Tables directory: {TABLES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PLOTTING CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Color palette for methods\n",
    "METHOD_COLORS = {\n",
    "    'physics_sr': '#2E86AB',    # Blue\n",
    "    'pysr_only': '#E94F37',     # Red\n",
    "    'lasso_pysr': '#F39C12',    # Orange\n",
    "}\n",
    "\n",
    "# Display names for methods\n",
    "METHOD_NAMES = {\n",
    "    'physics_sr': 'Physics-SR',\n",
    "    'pysr_only': 'PySR-Only',\n",
    "    'lasso_pysr': 'LASSO+PySR',\n",
    "}\n",
    "\n",
    "# Display names for equations\n",
    "EQUATION_NAMES = {\n",
    "    'coulomb': 'Coulomb',\n",
    "    'newton': 'Newton',\n",
    "    'ideal_gas': 'Ideal Gas',\n",
    "    'damped': 'Damped Osc.',\n",
    "}\n",
    "\n",
    "# Display names for equation types\n",
    "EQUATION_TYPE_NAMES = {\n",
    "    'power_law': 'Power-Law',\n",
    "    'nested_transcendental': 'Nested Transcendental',\n",
    "}\n",
    "\n",
    "# Figure size defaults\n",
    "FIGURE_SIZES = {\n",
    "    'single': (8, 6),\n",
    "    'wide': (12, 6),\n",
    "    'tall': (8, 10),\n",
    "    'square': (8, 8),\n",
    "    'large': (14, 10),\n",
    "}\n",
    "\n",
    "# DPI for saved figures\n",
    "FIGURE_DPI = 300\n",
    "\n",
    "# Font sizes\n",
    "FONTSIZE_TITLE = 14\n",
    "FONTSIZE_LABEL = 12\n",
    "FONTSIZE_TICK = 10\n",
    "FONTSIZE_LEGEND = 10\n",
    "FONTSIZE_ANNOTATION = 8\n",
    "\n",
    "# Set default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': FONTSIZE_TICK,\n",
    "    'axes.titlesize': FONTSIZE_TITLE,\n",
    "    'axes.labelsize': FONTSIZE_LABEL,\n",
    "    'xtick.labelsize': FONTSIZE_TICK,\n",
    "    'ytick.labelsize': FONTSIZE_TICK,\n",
    "    'legend.fontsize': FONTSIZE_LEGEND,\n",
    "    'figure.titlesize': FONTSIZE_TITLE,\n",
    "})\n",
    "\n",
    "print(\"Plotting configuration set.\")\n",
    "print(f\"Method colors: {METHOD_COLORS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD EXPERIMENT RESULTS (CSV)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_results_csv(filepath: Path = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load experiment results from CSV file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path, optional\n",
    "        Path to CSV file. Defaults to results/experiment_results.csv\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If results file does not exist\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = RESULTS_DIR / 'experiment_results.csv'\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Results file not found: {filepath}\\n\"\n",
    "            f\"Please run Experiments.ipynb first to generate results.\"\n",
    "        )\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert types\n",
    "    if 'with_dims' in df.columns:\n",
    "        df['with_dims'] = df['with_dims'].astype(bool)\n",
    "    if 'selected_correct' in df.columns:\n",
    "        df['selected_correct'] = df['selected_correct'].astype(bool)\n",
    "    if 'success' in df.columns:\n",
    "        df['success'] = df['success'].astype(bool)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} experiment results from {filepath}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"load_results_csv() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD EXPERIMENT RESULTS (PKL)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_results_pkl(filepath: Path = None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load detailed experiment results from PKL file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path, optional\n",
    "        Path to PKL file. Defaults to results/experiment_results_full.pkl\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Optional[Dict[str, Any]]\n",
    "        Detailed results or None if file not found or cannot be loaded\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The PKL file may contain custom classes (e.g., ExperimentResult) that\n",
    "    are not defined in this notebook. In such cases, the function returns\n",
    "    None and prints a warning. The CSV file contains all necessary data\n",
    "    for analysis.\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = RESULTS_DIR / 'experiment_results_full.pkl'\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"Warning: PKL file not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        print(f\"Loaded detailed results from {filepath}\")\n",
    "        return results\n",
    "    except (AttributeError, ModuleNotFoundError) as e:\n",
    "        print(f\"Warning: Cannot load PKL file (missing class definition): {e}\")\n",
    "        print(\"Continuing with CSV data only. This is sufficient for all analyses.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"load_results_pkl() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "def validate_results(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate results DataFrame and return summary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Validation summary\n",
    "    \"\"\"\n",
    "    validation = {\n",
    "        'n_experiments': len(df),\n",
    "        'n_successful': df['success'].sum() if 'success' in df.columns else len(df),\n",
    "        'success_rate': df['success'].mean() * 100 if 'success' in df.columns else 100.0,\n",
    "        'methods': df['method'].unique().tolist(),\n",
    "        'equations': df['equation_name'].unique().tolist(),\n",
    "        'noise_levels': sorted(df['noise_level'].unique().tolist()),\n",
    "        'dummy_counts': sorted(df['n_dummy'].unique().tolist()),\n",
    "        'sample_sizes': sorted(df['n_samples'].unique().tolist()) if 'n_samples' in df.columns else [500],\n",
    "    }\n",
    "    return validation\n",
    "\n",
    "\n",
    "def get_available_equations(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get list of equations available in DataFrame, ordered consistently.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of equation names in preferred order\n",
    "    \"\"\"\n",
    "    preferred_order = ['coulomb', 'newton', 'ideal_gas', 'damped']\n",
    "    available = df['equation_name'].unique().tolist()\n",
    "    # Return in preferred order, only including those that exist\n",
    "    return [eq for eq in preferred_order if eq in available] + \\\n",
    "           [eq for eq in available if eq not in preferred_order]\n",
    "\n",
    "\n",
    "print(\"validate_results() defined.\")\n",
    "print(\"get_available_equations() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD DATA\n",
    "# ==============================================================================\n",
    "\n",
    "# Load results\n",
    "df = load_results_csv()\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Load detailed results\n",
    "detailed_results = load_results_pkl()\n",
    "\n",
    "# Validate\n",
    "validation = validate_results(df)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total experiments: {validation['n_experiments']}\")\n",
    "print(f\"Successful experiments: {validation['n_successful']}\")\n",
    "print(f\"Success rate: {validation['success_rate']:.1f}%\")\n",
    "print(f\"Methods: {validation['methods']}\")\n",
    "print(f\"Equations: {validation['equations']}\")\n",
    "print(f\"Noise levels: {validation['noise_levels']}\")\n",
    "print(f\"Dummy counts: {validation['dummy_counts']}\")\n",
    "print(f\"Sample sizes: {validation['sample_sizes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ADD EQUATION TYPE COLUMN\n",
    "# ==============================================================================\n",
    "\n",
    "if 'eq_type' not in df.columns:\n",
    "    df['eq_type'] = df['equation_name'].map({\n",
    "        'coulomb': 'power_law',\n",
    "        'newton': 'power_law',\n",
    "        'ideal_gas': 'power_law',\n",
    "        'damped': 'nested_transcendental'\n",
    "    })\n",
    "    print(\"Added eq_type column to DataFrame.\")\n",
    "\n",
    "print(\"\\nEquation type distribution:\")\n",
    "print(df['eq_type'].value_counts())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_method_comparison(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute performance comparison by method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Method comparison table\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    comparison = []\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        method_df = core_df[core_df['method'] == method]\n",
    "        if len(method_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        row = {\n",
    "            'Method': METHOD_NAMES.get(method, method),\n",
    "            'N': len(method_df),\n",
    "            'F1': f\"{method_df['var_f1'].mean():.3f} +/- {method_df['var_f1'].std():.3f}\",\n",
    "            'Precision': f\"{method_df['var_precision'].mean():.3f} +/- {method_df['var_precision'].std():.3f}\",\n",
    "            'Recall': f\"{method_df['var_recall'].mean():.3f} +/- {method_df['var_recall'].std():.3f}\",\n",
    "            'Test R2': f\"{method_df['test_r2'].mean():.3f} +/- {method_df['test_r2'].std():.3f}\",\n",
    "            'Runtime (s)': f\"{method_df['runtime_seconds'].mean():.1f} +/- {method_df['runtime_seconds'].std():.1f}\",\n",
    "            'Exact Match': f\"{method_df['selected_correct'].mean()*100:.1f}%\",\n",
    "        }\n",
    "        comparison.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "# Compute and display\n",
    "method_comparison = compute_method_comparison(df)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"METHOD COMPARISON (All Equations)\")\n",
    "print(\"=\"*100)\n",
    "display(method_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD COMPARISON BY EQUATION TYPE\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_method_comparison_by_type(df: pd.DataFrame, eq_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute performance comparison by method for specific equation type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    eq_type : str\n",
    "        Equation type ('power_law' or 'nested_transcendental')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Method comparison table for specified equation type\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    type_df = core_df[core_df['eq_type'] == eq_type]\n",
    "    \n",
    "    comparison = []\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        method_df = type_df[type_df['method'] == method]\n",
    "        if len(method_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        row = {\n",
    "            'Method': METHOD_NAMES.get(method, method),\n",
    "            'N': len(method_df),\n",
    "            'F1': f\"{method_df['var_f1'].mean():.3f} +/- {method_df['var_f1'].std():.3f}\",\n",
    "            'Precision': f\"{method_df['var_precision'].mean():.3f}\",\n",
    "            'Recall': f\"{method_df['var_recall'].mean():.3f}\",\n",
    "            'Test R2': f\"{method_df['test_r2'].mean():.3f}\",\n",
    "            'Exact Match': f\"{method_df['selected_correct'].mean()*100:.1f}%\",\n",
    "        }\n",
    "        comparison.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "# Display by type\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"METHOD COMPARISON (Power-Law Equations Only)\")\n",
    "print(\"=\"*100)\n",
    "display(compute_method_comparison_by_type(df, 'power_law'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"METHOD COMPARISON (Nested Transcendental)\")\n",
    "print(\"=\"*100)\n",
    "display(compute_method_comparison_by_type(df, 'nested_transcendental'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BY-EQUATION COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_equation_comparison(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute performance comparison by equation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Equation comparison table\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    # Pivot table\n",
    "    comparison = core_df.pivot_table(\n",
    "        index='equation_name',\n",
    "        columns='method',\n",
    "        values=['var_f1', 'test_r2', 'selected_correct'],\n",
    "        aggfunc='mean'\n",
    "    ).round(3)\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "\n",
    "# Compute and display\n",
    "equation_comparison = compute_equation_comparison(df)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY EQUATION\")\n",
    "print(\"=\"*80)\n",
    "display(equation_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TESTS\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_statistical_tests(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform statistical significance tests between methods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Test results\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    # Extract F1 scores by method\n",
    "    physics_sr_f1 = core_df[core_df['method'] == 'physics_sr']['var_f1'].values\n",
    "    pysr_only_f1 = core_df[core_df['method'] == 'pysr_only']['var_f1'].values\n",
    "    lasso_pysr_f1 = core_df[core_df['method'] == 'lasso_pysr']['var_f1'].values\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Physics-SR vs PySR-Only\n",
    "    if len(physics_sr_f1) > 0 and len(pysr_only_f1) > 0:\n",
    "        t_stat, p_value = stats.ttest_ind(physics_sr_f1, pysr_only_f1)\n",
    "        results['physics_sr_vs_pysr_only'] = {\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "        }\n",
    "    \n",
    "    # Physics-SR vs LASSO+PySR\n",
    "    if len(physics_sr_f1) > 0 and len(lasso_pysr_f1) > 0:\n",
    "        t_stat, p_value = stats.ttest_ind(physics_sr_f1, lasso_pysr_f1)\n",
    "        results['physics_sr_vs_lasso_pysr'] = {\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "        }\n",
    "    \n",
    "    # LASSO+PySR vs PySR-Only\n",
    "    if len(lasso_pysr_f1) > 0 and len(pysr_only_f1) > 0:\n",
    "        t_stat, p_value = stats.ttest_ind(lasso_pysr_f1, pysr_only_f1)\n",
    "        results['lasso_pysr_vs_pysr_only'] = {\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "        }\n",
    "    \n",
    "    # ANOVA for overall comparison\n",
    "    if len(physics_sr_f1) > 0 and len(pysr_only_f1) > 0 and len(lasso_pysr_f1) > 0:\n",
    "        f_stat, p_value = stats.f_oneway(physics_sr_f1, pysr_only_f1, lasso_pysr_f1)\n",
    "        results['anova'] = {\n",
    "            'f_statistic': f_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Compute and display\n",
    "stat_tests = compute_statistical_tests(df)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (Variable Selection F1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'anova' in stat_tests:\n",
    "    print(\"\\n[ANOVA - Overall]\")\n",
    "    print(f\"  F-statistic: {stat_tests['anova']['f_statistic']:.4f}\")\n",
    "    print(f\"  p-value: {stat_tests['anova']['p_value']:.6f}\")\n",
    "    print(f\"  Significant (p < 0.05): {stat_tests['anova']['significant']}\")\n",
    "\n",
    "print(\"\\n[Pairwise t-tests]\")\n",
    "for comparison, result in stat_tests.items():\n",
    "    if comparison != 'anova':\n",
    "        print(f\"\\n  {comparison.replace('_', ' ').title()}:\")\n",
    "        print(f\"    t-statistic: {result['t_statistic']:.4f}\")\n",
    "        print(f\"    p-value: {result['p_value']:.6f}\")\n",
    "        print(f\"    Significant (p < 0.05): {result['significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Core Result Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 1: VARIABLE SELECTION F1 VS NOISE LEVEL\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_vs_noise(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing F1 score vs noise level, grouped by method.\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        means = subset.groupby('noise_level')['var_f1'].mean()\n",
    "        stds = subset.groupby('noise_level')['var_f1'].std()\n",
    "        \n",
    "        ax.errorbar(\n",
    "            means.index * 100,  # Convert to percentage\n",
    "            means.values,\n",
    "            yerr=stds.values,\n",
    "            label=METHOD_NAMES[method],\n",
    "            marker='o',\n",
    "            markersize=8,\n",
    "            capsize=5,\n",
    "            color=METHOD_COLORS[method],\n",
    "            linewidth=2,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Noise Level (%)')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection Performance vs Noise Level')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig1 = plot_f1_vs_noise(df, save_path=FIGURES_DIR / 'fig1_f1_vs_noise.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 2: VARIABLE SELECTION F1 VS DUMMY COUNT\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_vs_dummy(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing F1 score vs dummy feature count, grouped by method.\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        means = subset.groupby('n_dummy')['var_f1'].mean()\n",
    "        stds = subset.groupby('n_dummy')['var_f1'].std()\n",
    "        \n",
    "        ax.errorbar(\n",
    "            means.index,\n",
    "            means.values,\n",
    "            yerr=stds.values,\n",
    "            label=METHOD_NAMES[method],\n",
    "            marker='s',\n",
    "            markersize=8,\n",
    "            capsize=5,\n",
    "            color=METHOD_COLORS[method],\n",
    "            linewidth=2,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Number of Dummy Features')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection Performance vs Dummy Features')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig2 = plot_f1_vs_dummy(df, save_path=FIGURES_DIR / 'fig2_f1_vs_dummy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 3: TEST R2 COMPARISON (BAR CHART)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_r2_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart comparing Test R2 by method and equation.\n",
    "    Uses dynamic equation list from data.\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['wide'])\n",
    "    \n",
    "    # Get equations dynamically from data\n",
    "    equations = get_available_equations(core_df)\n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        means = []\n",
    "        stds = []\n",
    "        for equation in equations:\n",
    "            subset = core_df[(core_df['method'] == method) & (core_df['equation_name'] == equation)]\n",
    "            if len(subset) > 0:\n",
    "                means.append(subset['test_r2'].mean())\n",
    "                stds.append(subset['test_r2'].std())\n",
    "            else:\n",
    "                means.append(0)\n",
    "                stds.append(0)\n",
    "        \n",
    "        ax.bar(\n",
    "            x + i * width - width,\n",
    "            means,\n",
    "            width,\n",
    "            yerr=stds,\n",
    "            label=METHOD_NAMES[method],\n",
    "            color=METHOD_COLORS[method],\n",
    "            capsize=3,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Test R$^2$')\n",
    "    ax.set_title('Prediction Accuracy by Method and Equation')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig3 = plot_r2_comparison(df, save_path=FIGURES_DIR / 'fig3_r2_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 4: F1 BY METHOD AND EQUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart comparing F1 by method and equation.\n",
    "    Uses dynamic equation list from data.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['wide'])\n",
    "    \n",
    "    # Get equations dynamically from data\n",
    "    equations = get_available_equations(core_df)\n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        means = []\n",
    "        stds = []\n",
    "        for equation in equations:\n",
    "            subset = core_df[(core_df['method'] == method) & (core_df['equation_name'] == equation)]\n",
    "            if len(subset) > 0:\n",
    "                means.append(subset['var_f1'].mean())\n",
    "                stds.append(subset['var_f1'].std())\n",
    "            else:\n",
    "                means.append(0)\n",
    "                stds.append(0)\n",
    "        \n",
    "        ax.bar(\n",
    "            x + i * width - width, means, width, yerr=stds,\n",
    "            label=METHOD_NAMES[method], color=METHOD_COLORS[method],\n",
    "            capsize=3, alpha=0.8,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection F1 by Method and Equation')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.15)\n",
    "    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig4 = plot_f1_comparison(df, save_path=FIGURES_DIR / 'fig4_f1_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 5: F1 BY EQUATION TYPE\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_by_equation_type(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart comparing F1 by method and equation type.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    # Get equation types dynamically\n",
    "    eq_types = [t for t in ['power_law', 'nested_transcendental'] if t in core_df['eq_type'].unique()]\n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    x = np.arange(len(eq_types))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        means = []\n",
    "        stds = []\n",
    "        for eq_type in eq_types:\n",
    "            subset = core_df[(core_df['method'] == method) & (core_df['eq_type'] == eq_type)]\n",
    "            if len(subset) > 0:\n",
    "                means.append(subset['var_f1'].mean())\n",
    "                stds.append(subset['var_f1'].std())\n",
    "            else:\n",
    "                means.append(0)\n",
    "                stds.append(0)\n",
    "        \n",
    "        bars = ax.bar(\n",
    "            x + i * width - width, means, width, yerr=stds,\n",
    "            label=METHOD_NAMES[method], color=METHOD_COLORS[method],\n",
    "            capsize=3, alpha=0.8,\n",
    "        )\n",
    "        \n",
    "        # Add value annotations\n",
    "        for bar, mean in zip(bars, means):\n",
    "            if mean > 0:\n",
    "                ax.annotate(\n",
    "                    f'{mean:.2f}', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                    xytext=(0, 3), textcoords='offset points', ha='center', fontsize=8,\n",
    "                )\n",
    "    \n",
    "    ax.set_xlabel('Equation Type')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection F1 by Equation Type')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_TYPE_NAMES.get(t, t) for t in eq_types])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.2)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig5 = plot_f1_by_equation_type(df, save_path=FIGURES_DIR / 'fig5_f1_by_type.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 6: DIMENSION BENEFIT\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_dims_benefit(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart showing performance with vs without dimensional info.\n",
    "    Uses dynamic equation list from data.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    physics_sr_df = core_df[core_df['method'] == 'physics_sr']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    # Get equations dynamically\n",
    "    equations = get_available_equations(physics_sr_df)\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.35\n",
    "    \n",
    "    with_dims_means = []\n",
    "    without_dims_means = []\n",
    "    for equation in equations:\n",
    "        subset_with = physics_sr_df[(physics_sr_df['equation_name'] == equation) & (physics_sr_df['with_dims'] == True)]\n",
    "        subset_without = physics_sr_df[(physics_sr_df['equation_name'] == equation) & (physics_sr_df['with_dims'] == False)]\n",
    "        with_dims_means.append(subset_with['var_f1'].mean() if len(subset_with) > 0 else 0)\n",
    "        without_dims_means.append(subset_without['var_f1'].mean() if len(subset_without) > 0 else 0)\n",
    "    \n",
    "    ax.bar(x - width/2, with_dims_means, width, label='With Dimensions', color='#2E86AB', alpha=0.8)\n",
    "    ax.bar(x + width/2, without_dims_means, width, label='Without Dimensions', color='#E94F37', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Benefit of Dimensional Information (Physics-SR)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.15)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig6 = plot_dims_benefit(df, save_path=FIGURES_DIR / 'fig6_dims_benefit.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 7: RUNTIME COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_runtime_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Bar chart comparing runtime by method.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    # Filter to methods that exist in data\n",
    "    methods = [m for m in methods if m in core_df['method'].unique()]\n",
    "    x = np.arange(len(methods))\n",
    "    \n",
    "    means = []\n",
    "    stds = []\n",
    "    colors = []\n",
    "    for method in methods:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        means.append(subset['runtime_seconds'].mean())\n",
    "        stds.append(subset['runtime_seconds'].std())\n",
    "        colors.append(METHOD_COLORS[method])\n",
    "    \n",
    "    bars = ax.bar(x, means, yerr=stds, color=colors, capsize=5, alpha=0.8)\n",
    "    \n",
    "    # Add value annotations\n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax.annotate(\n",
    "            f'{mean:.1f}s', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "            xytext=(0, 3), textcoords='offset points', ha='center', fontsize=10,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Runtime (seconds)')\n",
    "    ax.set_title('Computational Cost Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([METHOD_NAMES[m] for m in methods])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig7 = plot_runtime_comparison(df, save_path=FIGURES_DIR / 'fig7_runtime.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 8: COMPREHENSIVE HEATMAP\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_comprehensive_heatmap(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Heatmap showing F1 scores across all experimental conditions.\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    # Create pivot table\n",
    "    core_df['row_label'] = core_df['equation_name'].map(EQUATION_NAMES) + ' | ' + core_df['method'].map(METHOD_NAMES)\n",
    "    core_df['col_label'] = (\n",
    "        'Noise=' + (core_df['noise_level'] * 100).astype(int).astype(str) + '%, ' +\n",
    "        'Dummy=' + core_df['n_dummy'].astype(str) + ', ' +\n",
    "        'Dims=' + core_df['with_dims'].map({True: 'T', False: 'F'})\n",
    "    )\n",
    "    \n",
    "    # Pivot\n",
    "    pivot = core_df.pivot_table(\n",
    "        index='row_label',\n",
    "        columns='col_label',\n",
    "        values='var_f1',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Sort rows by equation then method\n",
    "    equations = [EQUATION_NAMES.get(eq, eq) for eq in get_available_equations(core_df)]\n",
    "    methods = ['Physics-SR', 'PySR-Only', 'LASSO+PySR']\n",
    "    row_order = [f\"{eq} | {m}\" for eq in equations for m in methods]\n",
    "    pivot = pivot.reindex([r for r in row_order if r in pivot.index])\n",
    "    \n",
    "    # Sort columns\n",
    "    col_order = sorted(pivot.columns)\n",
    "    pivot = pivot[col_order]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['large'])\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='RdYlGn',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'F1 Score'},\n",
    "        annot_kws={'size': 8},\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Experimental Condition')\n",
    "    ax.set_ylabel('Equation | Method')\n",
    "    ax.set_title('Variable Selection F1 Score Across All Conditions')\n",
    "    \n",
    "    # Rotate x labels\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig8 = plot_comprehensive_heatmap(df, save_path=FIGURES_DIR / 'fig8_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Supplementary Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 9: SAMPLE SIZE SENSITIVITY (PHYSICS-SR ONLY)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_sample_size_sensitivity(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing performance vs sample size for Physics-SR.\n",
    "    \"\"\"\n",
    "    # Filter Physics-SR only (relaxed filter to include all conditions)\n",
    "    physics_sr_df = df[df['method'] == 'physics_sr'].copy()\n",
    "    \n",
    "    # Check if we have multiple sample sizes\n",
    "    if 'n_samples' not in physics_sr_df.columns or len(physics_sr_df['n_samples'].unique()) < 2:\n",
    "        print(\"Warning: Not enough sample size variation for sensitivity plot.\")\n",
    "        fig, ax = plt.subplots(figsize=FIGURE_SIZES['wide'])\n",
    "        ax.text(0.5, 0.5, 'Insufficient sample size variation', ha='center', va='center', fontsize=14)\n",
    "        ax.set_title('Sample Size Sensitivity (Not Available)')\n",
    "        return fig\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=FIGURE_SIZES['wide'])\n",
    "    equations = ['coulomb', 'newton', 'ideal_gas', 'damped']\n",
    "    \n",
    "    # Different styles for each equation to maximize distinction\n",
    "    styles = {\n",
    "        'coulomb':   {'marker': 'o', 'markersize': 14, 'linewidth': 5.0, 'color': '#2E86AB'},  # Blue, large circle\n",
    "        'newton':    {'marker': 's', 'markersize': 8,  'linewidth': 2.0, 'color': '#E94F37'},  # Red, medium square\n",
    "        'ideal_gas': {'marker': '^', 'markersize': 6,  'linewidth': 1.5, 'color': '#4CAF50'},  # Green, triangle\n",
    "        'damped':    {'marker': 'd', 'markersize': 7,  'linewidth': 1.5, 'color': '#9C27B0'},  # Purple, small diamond\n",
    "    }\n",
    "    \n",
    "    # F1 vs sample size\n",
    "    ax1 = axes[0]\n",
    "    for equation in equations:\n",
    "        subset = physics_sr_df[physics_sr_df['equation_name'] == equation]\n",
    "        if len(subset) > 0 and len(subset['n_samples'].unique()) > 1:\n",
    "            means = subset.groupby('n_samples')['var_f1'].mean()\n",
    "            style = styles[equation]\n",
    "            ax1.plot(\n",
    "                means.index,\n",
    "                means.values,\n",
    "                marker=style['marker'],\n",
    "                markersize=style['markersize'],\n",
    "                linewidth=style['linewidth'],\n",
    "                color=style['color'],\n",
    "                label=EQUATION_NAMES.get(equation, equation),\n",
    "            )\n",
    "    \n",
    "    ax1.set_xlabel('Sample Size (n)')\n",
    "    ax1.set_ylabel('Variable Selection F1 Score')\n",
    "    ax1.set_title('F1 Score vs Sample Size')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # R2 vs sample size\n",
    "    ax2 = axes[1]\n",
    "    for equation in equations:\n",
    "        subset = physics_sr_df[physics_sr_df['equation_name'] == equation]\n",
    "        if len(subset) > 0 and len(subset['n_samples'].unique()) > 1:\n",
    "            # Filter out NaN/negative R2 values\n",
    "            subset_clean = subset[subset['test_r2'] > 0]\n",
    "            if len(subset_clean) > 0:\n",
    "                means = subset_clean.groupby('n_samples')['test_r2'].mean()\n",
    "                style = styles[equation]\n",
    "                ax2.plot(\n",
    "                    means.index,\n",
    "                    means.values,\n",
    "                    marker=style['marker'],\n",
    "                    markersize=style['markersize'],\n",
    "                    linewidth=style['linewidth'],\n",
    "                    color=style['color'],\n",
    "                    label=EQUATION_NAMES.get(equation, equation),\n",
    "                )\n",
    "    \n",
    "    ax2.set_xlabel('Sample Size (n)')\n",
    "    ax2.set_ylabel('Test R$^2$')\n",
    "    ax2.set_title('Test R$^2$ vs Sample Size')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig9 = plot_sample_size_sensitivity(df, save_path=FIGURES_DIR / 'fig9_sample_size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 10: PER-EQUATION BREAKDOWN\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_per_equation_breakdown(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Multi-panel figure showing detailed results per equation.\n",
    "    Uses dynamic equation list from data.\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    # Get equations dynamically\n",
    "    equations = get_available_equations(core_df)\n",
    "    n_equations = len(equations)\n",
    "    \n",
    "    # Determine grid size\n",
    "    n_cols = min(2, n_equations)\n",
    "    n_rows = (n_equations + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "    if n_equations == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    \n",
    "    for idx, equation in enumerate(equations):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Filter by equation\n",
    "        eq_df = core_df[core_df['equation_name'] == equation]\n",
    "        \n",
    "        # Box plot of F1 scores\n",
    "        data_for_boxplot = []\n",
    "        labels = []\n",
    "        colors = []\n",
    "        for m in methods:\n",
    "            method_data = eq_df[eq_df['method'] == m]['var_f1'].values\n",
    "            if len(method_data) > 0:\n",
    "                data_for_boxplot.append(method_data)\n",
    "                labels.append(METHOD_NAMES[m])\n",
    "                colors.append(METHOD_COLORS[m])\n",
    "        \n",
    "        if len(data_for_boxplot) > 0:\n",
    "            bp = ax.boxplot(\n",
    "                data_for_boxplot,\n",
    "                labels=labels,\n",
    "                patch_artist=True,\n",
    "            )\n",
    "            \n",
    "            # Color boxes\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "            \n",
    "            # Add mean markers\n",
    "            for i, data in enumerate(data_for_boxplot, 1):\n",
    "                ax.scatter(i, np.mean(data), color='black', marker='D', s=50, zorder=3)\n",
    "        \n",
    "        ax.set_ylabel('Variable Selection F1')\n",
    "        ax.set_title(f'{EQUATION_NAMES.get(equation, equation)}')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_equations, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Variable Selection Performance by Equation', fontsize=FONTSIZE_TITLE + 2, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig10 = plot_per_equation_breakdown(df, save_path=FIGURES_DIR / 'fig10_per_equation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 11: EXACT MATCH RATES\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_exact_match_rates(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Bar chart showing exact match rates by method.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=FIGURE_SIZES['wide'])\n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    # Filter to methods that exist\n",
    "    methods = [m for m in methods if m in core_df['method'].unique()]\n",
    "    x = np.arange(len(methods))\n",
    "    \n",
    "    # All equations\n",
    "    ax1 = axes[0]\n",
    "    rates_all = [core_df[core_df['method'] == m]['selected_correct'].mean() * 100 for m in methods]\n",
    "    \n",
    "    bars1 = ax1.bar(x, rates_all, color=[METHOD_COLORS[m] for m in methods], alpha=0.8)\n",
    "    ax1.set_title('All Equations')\n",
    "    ax1.set_ylabel('Exact Match Rate (%)')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([METHOD_NAMES[m] for m in methods])\n",
    "    ax1.set_ylim(0, 110)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, rate in zip(bars1, rates_all):\n",
    "        ax1.annotate(f'{rate:.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                     xytext=(0, 3), textcoords='offset points', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Power-law only\n",
    "    ax2 = axes[1]\n",
    "    power_law_df = core_df[core_df['eq_type'] == 'power_law']\n",
    "    if len(power_law_df) > 0:\n",
    "        rates_pl = [power_law_df[power_law_df['method'] == m]['selected_correct'].mean() * 100 \n",
    "                    if len(power_law_df[power_law_df['method'] == m]) > 0 else 0 \n",
    "                    for m in methods]\n",
    "        \n",
    "        bars2 = ax2.bar(x, rates_pl, color=[METHOD_COLORS[m] for m in methods], alpha=0.8)\n",
    "        ax2.set_title('Power-Law Equations Only')\n",
    "        ax2.set_ylabel('Exact Match Rate (%)')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels([METHOD_NAMES[m] for m in methods])\n",
    "        ax2.set_ylim(0, 110)\n",
    "        ax2.axhline(y=100, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        for bar, rate in zip(bars2, rates_pl):\n",
    "            ax2.annotate(f'{rate:.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                         xytext=(0, 3), textcoords='offset points', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Exact Variable Selection Match Rates', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig11 = plot_exact_match_rates(df, save_path=FIGURES_DIR / 'fig11_exact_match.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: LaTeX Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 1: MAIN RESULTS SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_main_results_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for main results.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Main Benchmark Results (Core Experiments)}\n",
    "\\label{tab:main_results}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "Method & Variable Selection F1 & Test $R^2$ & Runtime (s) \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        \n",
    "        f1_mean = subset['var_f1'].mean()\n",
    "        f1_std = subset['var_f1'].std()\n",
    "        r2_mean = subset['test_r2'].mean()\n",
    "        r2_std = subset['test_r2'].std()\n",
    "        rt_mean = subset['runtime_seconds'].mean()\n",
    "        rt_std = subset['runtime_seconds'].std()\n",
    "        \n",
    "        method_display = METHOD_NAMES.get(method, method)\n",
    "        \n",
    "        latex += f\"{method_display} & {f1_mean:.3f} $\\\\pm$ {f1_std:.3f} & \"\n",
    "        latex += f\"{r2_mean:.3f} $\\\\pm$ {r2_std:.3f} & \"\n",
    "        latex += f\"{rt_mean:.1f} $\\\\pm$ {rt_std:.1f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table1_latex = generate_main_results_table(df)\n",
    "print(\"TABLE 1: Main Results Summary\")\n",
    "print(\"=\"*60)\n",
    "print(table1_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 2: VARIABLE SELECTION METRICS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_variable_selection_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for variable selection metrics.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Variable Selection Performance}\n",
    "\\label{tab:var_selection}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "Method & Precision & Recall & F1 & Exact Match \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        prec = subset['var_precision'].mean()\n",
    "        rec = subset['var_recall'].mean()\n",
    "        f1 = subset['var_f1'].mean()\n",
    "        exact = subset['selected_correct'].mean() * 100\n",
    "        \n",
    "        method_display = METHOD_NAMES.get(method, method)\n",
    "        latex += f\"{method_display} & {prec:.3f} & {rec:.3f} & {f1:.3f} & {exact:.1f}\\\\% \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table2_latex = generate_variable_selection_table(df)\n",
    "print(\"\\nTABLE 2: Variable Selection Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(table2_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 3: PREDICTION BY EQUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_prediction_by_equation_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for prediction accuracy by equation.\n",
    "    Uses dynamic equation list.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Prediction Accuracy by Equation}\n",
    "\\label{tab:prediction_by_eq}\n",
    "\\begin{tabular}{llcc}\n",
    "\\toprule\n",
    "Equation & Method & Test $R^2$ & Test RMSE \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    # Get equations dynamically\n",
    "    equations = get_available_equations(core_df)\n",
    "    \n",
    "    for equation in equations:\n",
    "        eq_df = core_df[core_df['equation_name'] == equation]\n",
    "        eq_display = EQUATION_NAMES.get(equation, equation)\n",
    "        \n",
    "        for i, method in enumerate(['physics_sr', 'pysr_only', 'lasso_pysr']):\n",
    "            subset = eq_df[eq_df['method'] == method]\n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "            r2 = subset['test_r2'].mean()\n",
    "            rmse = subset['test_rmse'].mean()\n",
    "            \n",
    "            method_display = METHOD_NAMES.get(method, method)\n",
    "            \n",
    "            if i == 0:\n",
    "                latex += f\"{eq_display} & {method_display} & {r2:.3f} & {rmse:.4f} \\\\\\\\\\n\"\n",
    "            else:\n",
    "                latex += f\" & {method_display} & {r2:.3f} & {rmse:.4f} \\\\\\\\\\n\"\n",
    "        \n",
    "        if equation != equations[-1]:\n",
    "            latex += \"\\\\midrule\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table3_latex = generate_prediction_by_equation_table(df)\n",
    "print(\"\\nTABLE 3: Prediction by Equation\")\n",
    "print(\"=\"*60)\n",
    "print(table3_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 4: DIMENSIONAL INFORMATION BENEFIT\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_dims_benefit_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for dimensional information benefit.\n",
    "    Uses dynamic equation list.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    physics_sr_df = core_df[core_df['method'] == 'physics_sr']\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Benefit of Dimensional Information (Physics-SR)}\n",
    "\\label{tab:dims_benefit}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "Equation & With Dims (F1) & Without Dims (F1) & Improvement \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    # Get equations dynamically\n",
    "    equations = get_available_equations(physics_sr_df)\n",
    "    \n",
    "    for equation in equations:\n",
    "        eq_df = physics_sr_df[physics_sr_df['equation_name'] == equation]\n",
    "        with_dims = eq_df[eq_df['with_dims'] == True]['var_f1'].mean()\n",
    "        without_dims = eq_df[eq_df['with_dims'] == False]['var_f1'].mean()\n",
    "        \n",
    "        # Handle NaN\n",
    "        if pd.isna(with_dims):\n",
    "            with_dims = 0.0\n",
    "        if pd.isna(without_dims):\n",
    "            without_dims = 0.0\n",
    "        \n",
    "        if without_dims > 0:\n",
    "            improvement = (with_dims - without_dims) / without_dims * 100\n",
    "        else:\n",
    "            improvement = 0.0\n",
    "        \n",
    "        eq_display = EQUATION_NAMES.get(equation, equation)\n",
    "        latex += f\"{eq_display} & {with_dims:.3f} & {without_dims:.3f} & {improvement:+.1f}\\\\% \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table4_latex = generate_dims_benefit_table(df)\n",
    "print(\"\\nTABLE 4: Dimensional Information Benefit\")\n",
    "print(\"=\"*60)\n",
    "print(table4_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 5: POWER-LAW EQUATIONS SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_power_law_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for power-law equations only.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    power_law_df = core_df[core_df['eq_type'] == 'power_law']\n",
    "    \n",
    "    if len(power_law_df) == 0:\n",
    "        return \"% No power-law equations in data\"\n",
    "    \n",
    "    latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Power-Law Equations Performance Summary}\n",
    "\\label{tab:power_law_summary}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "Method & Precision & Recall & F1 & Exact Match \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = power_law_df[power_law_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        prec = subset['var_precision'].mean()\n",
    "        rec = subset['var_recall'].mean()\n",
    "        f1 = subset['var_f1'].mean()\n",
    "        exact = subset['selected_correct'].mean() * 100\n",
    "        \n",
    "        method_display = METHOD_NAMES.get(method, method)\n",
    "        latex += f\"{method_display} & {prec:.3f} & {rec:.3f} & {f1:.3f} & {exact:.1f}\\\\% \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table5_latex = generate_power_law_table(df)\n",
    "print(\"\\nTABLE 5: Power-Law Equations Summary\")\n",
    "print(\"=\"*60)\n",
    "print(table5_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPORT ALL TABLES\n",
    "# ==============================================================================\n",
    "\n",
    "def export_all_tables(\n",
    "    df: pd.DataFrame,\n",
    "    tables_dir: Path = TABLES_DIR\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Export all LaTeX tables to files.\n",
    "    \"\"\"\n",
    "    tables = {\n",
    "        'table1_main_results.tex': generate_main_results_table(df),\n",
    "        'table2_var_selection.tex': generate_variable_selection_table(df),\n",
    "        'table3_prediction_by_eq.tex': generate_prediction_by_equation_table(df),\n",
    "        'table4_dims_benefit.tex': generate_dims_benefit_table(df),\n",
    "        'table5_power_law_summary.tex': generate_power_law_table(df),\n",
    "    }\n",
    "    \n",
    "    for filename, latex_content in tables.items():\n",
    "        filepath = tables_dir / filename\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(latex_content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "\n",
    "\n",
    "# Export tables\n",
    "export_all_tables(df)\n",
    "print(\"\\nAll LaTeX tables exported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# KEY FINDINGS SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_key_findings(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate key findings from the benchmark results.\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "    physics_sr_df = core_df[core_df['method'] == 'physics_sr']\n",
    "    power_law_df = core_df[core_df['eq_type'] == 'power_law']\n",
    "    \n",
    "    findings = {}\n",
    "    \n",
    "    # 1. Overall best method\n",
    "    method_f1 = core_df.groupby('method')['var_f1'].mean()\n",
    "    if len(method_f1) > 0:\n",
    "        best_method = method_f1.idxmax()\n",
    "        findings['best_method'] = {\n",
    "            'method': METHOD_NAMES.get(best_method, best_method),\n",
    "            'f1': method_f1[best_method],\n",
    "        }\n",
    "    \n",
    "    # 2. Improvement over baseline\n",
    "    if 'physics_sr' in method_f1.index and 'pysr_only' in method_f1.index:\n",
    "        physics_sr_f1 = method_f1['physics_sr']\n",
    "        pysr_only_f1 = method_f1['pysr_only']\n",
    "        if pysr_only_f1 > 0:\n",
    "            improvement = (physics_sr_f1 - pysr_only_f1) / pysr_only_f1 * 100\n",
    "            findings['improvement_over_baseline'] = improvement\n",
    "    \n",
    "    # 3. Dimension benefit\n",
    "    if len(physics_sr_df) > 0:\n",
    "        with_dims_f1 = physics_sr_df[physics_sr_df['with_dims'] == True]['var_f1'].mean()\n",
    "        without_dims_f1 = physics_sr_df[physics_sr_df['with_dims'] == False]['var_f1'].mean()\n",
    "        if not pd.isna(without_dims_f1) and without_dims_f1 > 0:\n",
    "            dims_benefit = (with_dims_f1 - without_dims_f1) / without_dims_f1 * 100\n",
    "            findings['dimension_benefit'] = dims_benefit\n",
    "    \n",
    "    # 4. Noise robustness\n",
    "    if 0.0 in core_df['noise_level'].values and 0.05 in core_df['noise_level'].values:\n",
    "        clean_f1 = core_df[core_df['noise_level'] == 0.0].groupby('method')['var_f1'].mean()\n",
    "        noisy_f1 = core_df[core_df['noise_level'] == 0.05].groupby('method')['var_f1'].mean()\n",
    "        noise_degradation = {}\n",
    "        for m in clean_f1.index:\n",
    "            if m in noisy_f1.index and clean_f1[m] > 0:\n",
    "                noise_degradation[m] = (clean_f1[m] - noisy_f1[m]) / clean_f1[m] * 100\n",
    "        findings['noise_degradation'] = noise_degradation\n",
    "    \n",
    "    # 5. Exact match rates\n",
    "    exact_match_rates = core_df.groupby('method')['selected_correct'].mean() * 100\n",
    "    findings['exact_match_rates'] = exact_match_rates.to_dict()\n",
    "    \n",
    "    # 6. Hardest equation\n",
    "    eq_f1 = core_df.groupby('equation_name')['var_f1'].mean()\n",
    "    if len(eq_f1) > 0:\n",
    "        hardest_eq = eq_f1.idxmin()\n",
    "        findings['hardest_equation'] = {\n",
    "            'equation': EQUATION_NAMES.get(hardest_eq, hardest_eq),\n",
    "            'f1': eq_f1[hardest_eq],\n",
    "        }\n",
    "    \n",
    "    # 7. Power-law performance\n",
    "    if len(power_law_df) > 0:\n",
    "        pl_physics_sr = power_law_df[power_law_df['method'] == 'physics_sr']\n",
    "        if len(pl_physics_sr) > 0:\n",
    "            findings['power_law_performance'] = {\n",
    "                'f1': pl_physics_sr['var_f1'].mean(),\n",
    "                'exact_match': pl_physics_sr['selected_correct'].mean() * 100,\n",
    "            }\n",
    "    \n",
    "    return findings\n",
    "\n",
    "\n",
    "# Generate findings\n",
    "findings = generate_key_findings(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'best_method' in findings:\n",
    "    print(f\"\\n1. BEST METHOD: {findings['best_method']['method']}\")\n",
    "    print(f\"   - Average F1 Score: {findings['best_method']['f1']:.3f}\")\n",
    "\n",
    "if 'improvement_over_baseline' in findings:\n",
    "    print(f\"\\n2. IMPROVEMENT OVER BASELINE (PySR-Only):\")\n",
    "    print(f\"   - Physics-SR improves F1 by {findings['improvement_over_baseline']:+.1f}%\")\n",
    "\n",
    "if 'dimension_benefit' in findings:\n",
    "    print(f\"\\n3. DIMENSIONAL INFORMATION BENEFIT:\")\n",
    "    print(f\"   - Using dimensional info improves F1 by {findings['dimension_benefit']:+.1f}%\")\n",
    "\n",
    "if 'noise_degradation' in findings:\n",
    "    print(f\"\\n4. NOISE ROBUSTNESS (% degradation from 0% to 5% noise):\")\n",
    "    for method, degradation in findings['noise_degradation'].items():\n",
    "        print(f\"   - {METHOD_NAMES.get(method, method)}: {degradation:.1f}% degradation\")\n",
    "\n",
    "if 'exact_match_rates' in findings:\n",
    "    print(f\"\\n5. EXACT MATCH RATES:\")\n",
    "    for method, rate in findings['exact_match_rates'].items():\n",
    "        print(f\"   - {METHOD_NAMES.get(method, method)}: {rate:.1f}%\")\n",
    "\n",
    "if 'hardest_equation' in findings:\n",
    "    print(f\"\\n6. HARDEST EQUATION: {findings['hardest_equation']['equation']}\")\n",
    "    print(f\"   - Average F1 Score: {findings['hardest_equation']['f1']:.3f}\")\n",
    "\n",
    "if 'power_law_performance' in findings:\n",
    "    print(f\"\\n7. POWER-LAW EQUATIONS (Physics-SR):\")\n",
    "    print(f\"   - Average F1 Score: {findings['power_law_performance']['f1']:.3f}\")\n",
    "    print(f\"   - Exact Match Rate: {findings['power_law_performance']['exact_match']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY TABLES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY TABLE - ALL EQUATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create publication-ready summary\n",
    "core_df = df[df['n_samples'] == 500].copy() if 'n_samples' in df.columns else df.copy()\n",
    "power_law_df = core_df[core_df['eq_type'] == 'power_law']\n",
    "\n",
    "summary_data = []\n",
    "for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "    subset = core_df[core_df['method'] == method]\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "    summary_data.append({\n",
    "        'Method': METHOD_NAMES[method],\n",
    "        'Precision': f\"{subset['var_precision'].mean():.3f}\",\n",
    "        'Recall': f\"{subset['var_recall'].mean():.3f}\",\n",
    "        'F1': f\"{subset['var_f1'].mean():.3f}\",\n",
    "        'Test R2': f\"{subset['test_r2'].mean():.3f}\",\n",
    "        'Runtime (s)': f\"{subset['runtime_seconds'].mean():.1f}\",\n",
    "        'Exact Match': f\"{subset['selected_correct'].mean()*100:.1f}%\",\n",
    "    })\n",
    "\n",
    "summary_table = pd.DataFrame(summary_data)\n",
    "print(\"\\n\")\n",
    "display(summary_table)\n",
    "\n",
    "if len(power_law_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY TABLE - POWER-LAW EQUATIONS ONLY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    pl_summary_data = []\n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = power_law_df[power_law_df['method'] == method]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        pl_summary_data.append({\n",
    "            'Method': METHOD_NAMES[method],\n",
    "            'Precision': f\"{subset['var_precision'].mean():.3f}\",\n",
    "            'Recall': f\"{subset['var_recall'].mean():.3f}\",\n",
    "            'F1': f\"{subset['var_f1'].mean():.3f}\",\n",
    "            'Test R2': f\"{subset['test_r2'].mean():.3f}\",\n",
    "            'Runtime (s)': f\"{subset['runtime_seconds'].mean():.1f}\",\n",
    "            'Exact Match': f\"{subset['selected_correct'].mean()*100:.1f}%\",\n",
    "        })\n",
    "    \n",
    "    pl_summary_table = pd.DataFrame(pl_summary_data)\n",
    "    print(\"\\n\")\n",
    "    display(pl_summary_table)\n",
    "    \n",
    "    # Save summary tables\n",
    "    pl_summary_table.to_csv(RESULTS_DIR / 'summary_table_power_law.csv', index=False)\n",
    "\n",
    "summary_table.to_csv(RESULTS_DIR / 'summary_table_all.csv', index=False)\n",
    "print(f\"\\nSummary tables saved to {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ANALYSIS COMPLETE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[Generated Files]\")\n",
    "print(\"\\nFigures:\")\n",
    "for fig_file in sorted(FIGURES_DIR.glob('*.png')):\n",
    "    print(f\"  - {fig_file.name}\")\n",
    "\n",
    "print(\"\\nTables:\")\n",
    "for table_file in sorted(TABLES_DIR.glob('*.tex')):\n",
    "    print(f\"  - {table_file.name}\")\n",
    "\n",
    "print(\"\\nData:\")\n",
    "for csv_file in sorted(RESULTS_DIR.glob('*.csv')):\n",
    "    print(f\"  - {csv_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Ready for paper writing!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Quick Reference\n",
    "\n",
    "### Output Files Generated\n",
    "\n",
    "**Figures (`results/figures/`):**\n",
    "1. `fig1_f1_vs_noise.png` - F1 vs Noise Level\n",
    "2. `fig2_f1_vs_dummy.png` - F1 vs Dummy Features\n",
    "3. `fig3_r2_comparison.png` - Test R2 by Method and Equation\n",
    "4. `fig4_f1_comparison.png` - F1 by Method and Equation\n",
    "5. `fig5_f1_by_type.png` - F1 by Equation Type\n",
    "6. `fig6_dims_benefit.png` - Dimensional Information Benefit\n",
    "7. `fig7_runtime.png` - Runtime Comparison\n",
    "8. `fig8_heatmap.png` - Comprehensive F1 Heatmap\n",
    "9. `fig9_sample_size.png` - Sample Size Sensitivity\n",
    "10. `fig10_per_equation.png` - Per-Equation Breakdown\n",
    "11. `fig11_exact_match.png` - Exact Match Rates\n",
    "\n",
    "**Tables (`results/tables/`):**\n",
    "1. `table1_main_results.tex` - Main Results Summary\n",
    "2. `table2_var_selection.tex` - Variable Selection Metrics\n",
    "3. `table3_prediction_by_eq.tex` - Prediction by Equation\n",
    "4. `table4_dims_benefit.tex` - Dimension Benefit Analysis\n",
    "5. `table5_power_law_summary.tex` - Power-Law Summary\n",
    "\n",
    "**Data (`results/`):**\n",
    "- `summary_table_all.csv` - Final summary table (all equations)\n",
    "- `summary_table_power_law.csv` - Final summary table (power-law only)\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| var_f1 | Variable Selection F1 Score |\n",
    "| var_precision | Precision in variable selection |\n",
    "| var_recall | Recall in variable selection |\n",
    "| selected_correct | Exact match with ground truth |\n",
    "| test_r2 | Test set R-squared |\n",
    "| runtime_seconds | Total execution time |\n",
    "| eq_type | Equation type (power_law or nested_transcendental) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
