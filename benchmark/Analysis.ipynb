{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Physics-SR Framework v3.0 Benchmark\n",
    "\n",
    "## Results Analysis and Visualization Module\n",
    "\n",
    "**Author:** Zhengze Zhang  \n",
    "**Affiliation:** Department of Statistics, Columbia University  \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook analyzes and visualizes the benchmark results from Experiments.ipynb:\n",
    "\n",
    "1. **Summary Statistics**: Overall and by-factor performance metrics\n",
    "2. **Core Visualizations**: 6 figures for main results\n",
    "3. **Supplementary Visualizations**: 2 figures for additional analysis\n",
    "4. **LaTeX Tables**: Publication-ready tables for academic papers\n",
    "5. **Statistical Tests**: Significance testing between methods\n",
    "\n",
    "### Input Files\n",
    "\n",
    "- `results/experiment_results.csv`: Main results table\n",
    "- `results/experiment_results.pkl`: Detailed results with nested data\n",
    "\n",
    "### Output Files\n",
    "\n",
    "- `results/figures/*.png`: 8 visualization figures\n",
    "- `results/tables/*.tex`: 4 LaTeX tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Header and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analysis.ipynb - Results Analysis and Visualization Module\n",
    "===========================================================\n",
    "\n",
    "Physics-SR Framework v3.0 Benchmark Suite\n",
    "\n",
    "This module provides:\n",
    "- Summary statistics computation\n",
    "- Visualization functions for benchmark results\n",
    "- LaTeX table generation for publications\n",
    "- Statistical significance testing\n",
    "\n",
    "Author: Zhengze Zhang\n",
    "Affiliation: Department of Statistics, Columbia University\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Analysis: All imports successful.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Determine paths based on environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Colab paths\n",
    "    BASE_DIR = Path('/content/Physics-Informed-Symbolic-Regression')\n",
    "    BENCHMARK_DIR = BASE_DIR / 'benchmark'\n",
    "else:\n",
    "    # Local paths\n",
    "    BENCHMARK_DIR = Path('.').resolve()\n",
    "    BASE_DIR = BENCHMARK_DIR.parent\n",
    "\n",
    "RESULTS_DIR = BENCHMARK_DIR / 'results'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "TABLES_DIR = RESULTS_DIR / 'tables'\n",
    "\n",
    "# Create directories if needed\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TABLES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Benchmark directory: {BENCHMARK_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")\n",
    "print(f\"Tables directory: {TABLES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PLOTTING CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Color palette for methods\n",
    "METHOD_COLORS = {\n",
    "    'physics_sr': '#2E86AB',    # Blue\n",
    "    'pysr_only': '#E94F37',     # Red\n",
    "    'lasso_pysr': '#F39C12',    # Orange\n",
    "}\n",
    "\n",
    "# Display names for methods\n",
    "METHOD_NAMES = {\n",
    "    'physics_sr': 'Physics-SR',\n",
    "    'pysr_only': 'PySR-Only',\n",
    "    'lasso_pysr': 'LASSO+PySR',\n",
    "}\n",
    "\n",
    "# Display names for equations\n",
    "EQUATION_NAMES = {\n",
    "    'kk2000': 'KK2000',\n",
    "    'newton': 'Newton',\n",
    "    'ideal_gas': 'Ideal Gas',\n",
    "    'damped': 'Damped Osc.',\n",
    "}\n",
    "\n",
    "# Figure size defaults\n",
    "FIGURE_SIZES = {\n",
    "    'single': (8, 6),\n",
    "    'wide': (12, 6),\n",
    "    'tall': (8, 10),\n",
    "    'square': (8, 8),\n",
    "}\n",
    "\n",
    "# DPI for saved figures\n",
    "FIGURE_DPI = 300\n",
    "\n",
    "# Font sizes\n",
    "FONTSIZE_TITLE = 14\n",
    "FONTSIZE_LABEL = 12\n",
    "FONTSIZE_TICK = 10\n",
    "FONTSIZE_LEGEND = 10\n",
    "FONTSIZE_ANNOTATION = 8\n",
    "\n",
    "# Set default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': FONTSIZE_TICK,\n",
    "    'axes.titlesize': FONTSIZE_TITLE,\n",
    "    'axes.labelsize': FONTSIZE_LABEL,\n",
    "    'xtick.labelsize': FONTSIZE_TICK,\n",
    "    'ytick.labelsize': FONTSIZE_TICK,\n",
    "    'legend.fontsize': FONTSIZE_LEGEND,\n",
    "    'figure.titlesize': FONTSIZE_TITLE,\n",
    "})\n",
    "\n",
    "print(\"Plotting configuration set.\")\n",
    "print(f\"Method colors: {METHOD_COLORS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD EXPERIMENT RESULTS (CSV)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_results_csv(filepath: Path = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load experiment results from CSV file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path, optional\n",
    "        Path to CSV file. Defaults to results/experiment_results.csv\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = RESULTS_DIR / 'experiment_results.csv'\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Results file not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert types\n",
    "    df['with_dims'] = df['with_dims'].astype(bool)\n",
    "    df['selected_correct'] = df['selected_correct'].astype(bool)\n",
    "    df['success'] = df['success'].astype(bool)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} experiment results from {filepath}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"load_results_csv() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD EXPERIMENT RESULTS (PKL)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_results_pkl(filepath: Path = None) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Load detailed experiment results from PKL file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : Path, optional\n",
    "        Path to PKL file. Defaults to results/experiment_results.pkl\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[Any]\n",
    "        List of ExperimentResult objects\n",
    "    \"\"\"\n",
    "    if filepath is None:\n",
    "        filepath = RESULTS_DIR / 'experiment_results.pkl'\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"Warning: PKL file not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(results)} detailed results from {filepath}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"load_results_pkl() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD DATA\n",
    "# ==============================================================================\n",
    "\n",
    "# Load CSV results\n",
    "try:\n",
    "    df = load_results_csv()\n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    df = None\n",
    "\n",
    "# Load PKL results (optional, for detailed analysis)\n",
    "detailed_results = load_results_pkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "def validate_results(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate results DataFrame and return summary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Validation summary\n",
    "    \"\"\"\n",
    "    validation = {\n",
    "        'n_experiments': len(df),\n",
    "        'n_successful': df['success'].sum(),\n",
    "        'success_rate': df['success'].mean() * 100,\n",
    "        'methods': df['method'].unique().tolist(),\n",
    "        'equations': df['equation_name'].unique().tolist(),\n",
    "        'noise_levels': df['noise_level'].unique().tolist(),\n",
    "        'dummy_counts': df['n_dummy'].unique().tolist(),\n",
    "        'sample_sizes': df['n_samples'].unique().tolist(),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "    }\n",
    "    \n",
    "    return validation\n",
    "\n",
    "\n",
    "if df is not None:\n",
    "    validation = validate_results(df)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA VALIDATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total experiments: {validation['n_experiments']}\")\n",
    "    print(f\"Successful experiments: {validation['n_successful']}\")\n",
    "    print(f\"Success rate: {validation['success_rate']:.1f}%\")\n",
    "    print(f\"\\nMethods: {validation['methods']}\")\n",
    "    print(f\"Equations: {validation['equations']}\")\n",
    "    print(f\"Noise levels: {validation['noise_levels']}\")\n",
    "    print(f\"Dummy counts: {validation['dummy_counts']}\")\n",
    "    print(f\"Sample sizes: {validation['sample_sizes']}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CREATE SYNTHETIC DATA FOR DEMONSTRATION (IF NEEDED)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_synthetic_results() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create synthetic benchmark results for demonstration.\n",
    "    \n",
    "    This function is used when actual experiment results are not available.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Synthetic results DataFrame\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    equation_types = ['power_law', 'rational', 'rational', 'nested']\n",
    "    noise_levels = [0.0, 0.05]\n",
    "    dummy_counts = [0, 5]\n",
    "    \n",
    "    # Base performance by method (Physics-SR > LASSO+PySR > PySR-Only)\n",
    "    base_f1 = {'physics_sr': 0.90, 'pysr_only': 0.60, 'lasso_pysr': 0.75}\n",
    "    base_r2 = {'physics_sr': 0.95, 'pysr_only': 0.85, 'lasso_pysr': 0.90}\n",
    "    base_runtime = {'physics_sr': 120, 'pysr_only': 60, 'lasso_pysr': 80}\n",
    "    \n",
    "    # Difficulty by equation\n",
    "    difficulty = {'kk2000': 0.0, 'newton': 0.0, 'ideal_gas': 0.05, 'damped': 0.20}\n",
    "    \n",
    "    records = []\n",
    "    exp_id = 0\n",
    "    \n",
    "    for eq_idx, equation in enumerate(equations):\n",
    "        for noise in noise_levels:\n",
    "            for dummy in dummy_counts:\n",
    "                for with_dims in [True, False]:\n",
    "                    for method in methods:\n",
    "                        exp_id += 1\n",
    "                        \n",
    "                        # Compute performance with adjustments\n",
    "                        f1_adj = base_f1[method]\n",
    "                        r2_adj = base_r2[method]\n",
    "                        \n",
    "                        # Noise effect\n",
    "                        f1_adj -= noise * 2\n",
    "                        r2_adj -= noise * 0.5\n",
    "                        \n",
    "                        # Dummy effect\n",
    "                        f1_adj -= dummy * 0.03\n",
    "                        r2_adj -= dummy * 0.01\n",
    "                        \n",
    "                        # Dimension info benefit (only for physics_sr)\n",
    "                        if method == 'physics_sr' and not with_dims:\n",
    "                            f1_adj -= 0.15\n",
    "                            r2_adj -= 0.05\n",
    "                        \n",
    "                        # Equation difficulty\n",
    "                        f1_adj -= difficulty[equation]\n",
    "                        r2_adj -= difficulty[equation] * 0.5\n",
    "                        \n",
    "                        # Add noise\n",
    "                        f1_adj += np.random.normal(0, 0.05)\n",
    "                        r2_adj += np.random.normal(0, 0.02)\n",
    "                        \n",
    "                        # Clip to valid ranges\n",
    "                        f1_adj = np.clip(f1_adj, 0, 1)\n",
    "                        r2_adj = np.clip(r2_adj, 0, 1)\n",
    "                        \n",
    "                        # Compute other metrics\n",
    "                        precision = f1_adj + np.random.normal(0, 0.03)\n",
    "                        recall = f1_adj + np.random.normal(0, 0.03)\n",
    "                        precision = np.clip(precision, 0, 1)\n",
    "                        recall = np.clip(recall, 0, 1)\n",
    "                        \n",
    "                        runtime = base_runtime[method] * (1 + np.random.normal(0, 0.2))\n",
    "                        \n",
    "                        records.append({\n",
    "                            'experiment_id': f'exp_{exp_id:04d}',\n",
    "                            'equation_name': equation,\n",
    "                            'equation_type': equation_types[eq_idx],\n",
    "                            'noise_level': noise,\n",
    "                            'n_dummy': dummy,\n",
    "                            'n_samples': 500,\n",
    "                            'with_dims': with_dims,\n",
    "                            'method': method,\n",
    "                            'var_precision': precision,\n",
    "                            'var_recall': recall,\n",
    "                            'var_f1': f1_adj,\n",
    "                            'var_tp': int(2 * recall),\n",
    "                            'var_fp': int((1 - precision) * 2),\n",
    "                            'var_fn': int((1 - recall) * 2),\n",
    "                            'selected_correct': f1_adj > 0.95,\n",
    "                            'train_r2': r2_adj + 0.02,\n",
    "                            'test_r2': r2_adj,\n",
    "                            'train_rmse': 0.1 * (1 - r2_adj),\n",
    "                            'test_rmse': 0.12 * (1 - r2_adj),\n",
    "                            'runtime_seconds': runtime,\n",
    "                            'discovered_equation': 'synthetic',\n",
    "                            'true_equation': 'synthetic',\n",
    "                            'success': True,\n",
    "                            'error_message': None,\n",
    "                            'timestamp': '2026-01-10T00:00:00',\n",
    "                        })\n",
    "    \n",
    "    # Add supplementary experiments (sample size sensitivity)\n",
    "    for eq_idx, equation in enumerate(equations):\n",
    "        for n_samples in [250, 750]:\n",
    "            exp_id += 1\n",
    "            \n",
    "            # Base performance for physics_sr\n",
    "            f1_adj = base_f1['physics_sr']\n",
    "            r2_adj = base_r2['physics_sr']\n",
    "            \n",
    "            # Sample size effect\n",
    "            if n_samples == 250:\n",
    "                f1_adj -= 0.10\n",
    "                r2_adj -= 0.05\n",
    "            elif n_samples == 750:\n",
    "                f1_adj += 0.03\n",
    "                r2_adj += 0.02\n",
    "            \n",
    "            # Noise and dummy effects (fixed at 5% and 5)\n",
    "            f1_adj -= 0.05 * 2 + 5 * 0.03\n",
    "            r2_adj -= 0.05 * 0.5 + 5 * 0.01\n",
    "            \n",
    "            # Equation difficulty\n",
    "            f1_adj -= difficulty[equation]\n",
    "            r2_adj -= difficulty[equation] * 0.5\n",
    "            \n",
    "            # Add noise\n",
    "            f1_adj += np.random.normal(0, 0.05)\n",
    "            r2_adj += np.random.normal(0, 0.02)\n",
    "            \n",
    "            # Clip\n",
    "            f1_adj = np.clip(f1_adj, 0, 1)\n",
    "            r2_adj = np.clip(r2_adj, 0, 1)\n",
    "            \n",
    "            precision = f1_adj + np.random.normal(0, 0.03)\n",
    "            recall = f1_adj + np.random.normal(0, 0.03)\n",
    "            precision = np.clip(precision, 0, 1)\n",
    "            recall = np.clip(recall, 0, 1)\n",
    "            \n",
    "            runtime = base_runtime['physics_sr'] * (n_samples / 500) * (1 + np.random.normal(0, 0.2))\n",
    "            \n",
    "            records.append({\n",
    "                'experiment_id': f'exp_{exp_id:04d}',\n",
    "                'equation_name': equation,\n",
    "                'equation_type': equation_types[eq_idx],\n",
    "                'noise_level': 0.05,\n",
    "                'n_dummy': 5,\n",
    "                'n_samples': n_samples,\n",
    "                'with_dims': True,\n",
    "                'method': 'physics_sr',\n",
    "                'var_precision': precision,\n",
    "                'var_recall': recall,\n",
    "                'var_f1': f1_adj,\n",
    "                'var_tp': int(2 * recall),\n",
    "                'var_fp': int((1 - precision) * 2),\n",
    "                'var_fn': int((1 - recall) * 2),\n",
    "                'selected_correct': f1_adj > 0.95,\n",
    "                'train_r2': r2_adj + 0.02,\n",
    "                'test_r2': r2_adj,\n",
    "                'train_rmse': 0.1 * (1 - r2_adj),\n",
    "                'test_rmse': 0.12 * (1 - r2_adj),\n",
    "                'runtime_seconds': runtime,\n",
    "                'discovered_equation': 'synthetic',\n",
    "                'true_equation': 'synthetic',\n",
    "                'success': True,\n",
    "                'error_message': None,\n",
    "                'timestamp': '2026-01-10T00:00:00',\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Create synthetic data if needed\n",
    "if df is None:\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    df = create_synthetic_results()\n",
    "    print(f\"Created {len(df)} synthetic experiment results.\")\n",
    "    \n",
    "    # Save for reference\n",
    "    df.to_csv(RESULTS_DIR / 'experiment_results_synthetic.csv', index=False)\n",
    "    print(f\"Saved to {RESULTS_DIR / 'experiment_results_synthetic.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PREVIEW DATA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nDescriptive statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# OVERALL SUMMARY TABLE\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_overall_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute overall summary statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Summary statistics\n",
    "    \"\"\"\n",
    "    # Filter core experiments (n_samples == 500)\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    summary = core_df.groupby('method').agg({\n",
    "        'var_precision': ['mean', 'std'],\n",
    "        'var_recall': ['mean', 'std'],\n",
    "        'var_f1': ['mean', 'std'],\n",
    "        'test_r2': ['mean', 'std'],\n",
    "        'test_rmse': ['mean', 'std'],\n",
    "        'runtime_seconds': ['mean', 'std'],\n",
    "        'selected_correct': ['mean'],\n",
    "        'success': ['mean'],\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    summary = summary.rename(columns={\n",
    "        'var_precision_mean': 'Precision (mean)',\n",
    "        'var_precision_std': 'Precision (std)',\n",
    "        'var_recall_mean': 'Recall (mean)',\n",
    "        'var_recall_std': 'Recall (std)',\n",
    "        'var_f1_mean': 'F1 (mean)',\n",
    "        'var_f1_std': 'F1 (std)',\n",
    "        'test_r2_mean': 'Test R2 (mean)',\n",
    "        'test_r2_std': 'Test R2 (std)',\n",
    "        'test_rmse_mean': 'Test RMSE (mean)',\n",
    "        'test_rmse_std': 'Test RMSE (std)',\n",
    "        'runtime_seconds_mean': 'Runtime (mean)',\n",
    "        'runtime_seconds_std': 'Runtime (std)',\n",
    "        'selected_correct_mean': 'Exact Match Rate',\n",
    "        'success_mean': 'Success Rate',\n",
    "    })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# Compute and display\n",
    "overall_summary = compute_overall_summary(df)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL SUMMARY BY METHOD (Core Experiments Only)\")\n",
    "print(\"=\"*80)\n",
    "display(overall_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BY-METHOD COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_method_comparison(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute detailed method comparison.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Method comparison table\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    comparison = []\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        method_df = core_df[core_df['method'] == method]\n",
    "        \n",
    "        row = {\n",
    "            'Method': METHOD_NAMES.get(method, method),\n",
    "            'N': len(method_df),\n",
    "            'F1': f\"{method_df['var_f1'].mean():.3f} +/- {method_df['var_f1'].std():.3f}\",\n",
    "            'Precision': f\"{method_df['var_precision'].mean():.3f} +/- {method_df['var_precision'].std():.3f}\",\n",
    "            'Recall': f\"{method_df['var_recall'].mean():.3f} +/- {method_df['var_recall'].std():.3f}\",\n",
    "            'Test R2': f\"{method_df['test_r2'].mean():.3f} +/- {method_df['test_r2'].std():.3f}\",\n",
    "            'Runtime (s)': f\"{method_df['runtime_seconds'].mean():.1f} +/- {method_df['runtime_seconds'].std():.1f}\",\n",
    "            'Exact Match': f\"{method_df['selected_correct'].mean()*100:.1f}%\",\n",
    "        }\n",
    "        comparison.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "# Compute and display\n",
    "method_comparison = compute_method_comparison(df)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"METHOD COMPARISON (Core Experiments)\")\n",
    "print(\"=\"*100)\n",
    "display(method_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BY-EQUATION COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_equation_comparison(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute performance comparison by equation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Equation comparison table\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    # Pivot table\n",
    "    comparison = core_df.pivot_table(\n",
    "        index='equation_name',\n",
    "        columns='method',\n",
    "        values=['var_f1', 'test_r2', 'selected_correct'],\n",
    "        aggfunc='mean'\n",
    "    ).round(3)\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "\n",
    "# Compute and display\n",
    "equation_comparison = compute_equation_comparison(df)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY EQUATION\")\n",
    "print(\"=\"*80)\n",
    "display(equation_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TESTS\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_statistical_tests(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform statistical significance tests between methods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Test results\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    # Extract F1 scores by method\n",
    "    physics_sr_f1 = core_df[core_df['method'] == 'physics_sr']['var_f1'].values\n",
    "    pysr_only_f1 = core_df[core_df['method'] == 'pysr_only']['var_f1'].values\n",
    "    lasso_pysr_f1 = core_df[core_df['method'] == 'lasso_pysr']['var_f1'].values\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Paired t-tests (using independent t-test since experiments are different)\n",
    "    # Physics-SR vs PySR-Only\n",
    "    t_stat, p_value = stats.ttest_ind(physics_sr_f1, pysr_only_f1)\n",
    "    results['physics_sr_vs_pysr_only'] = {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05,\n",
    "    }\n",
    "    \n",
    "    # Physics-SR vs LASSO+PySR\n",
    "    t_stat, p_value = stats.ttest_ind(physics_sr_f1, lasso_pysr_f1)\n",
    "    results['physics_sr_vs_lasso_pysr'] = {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05,\n",
    "    }\n",
    "    \n",
    "    # LASSO+PySR vs PySR-Only\n",
    "    t_stat, p_value = stats.ttest_ind(lasso_pysr_f1, pysr_only_f1)\n",
    "    results['lasso_pysr_vs_pysr_only'] = {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05,\n",
    "    }\n",
    "    \n",
    "    # ANOVA for overall comparison\n",
    "    f_stat, p_value = stats.f_oneway(physics_sr_f1, pysr_only_f1, lasso_pysr_f1)\n",
    "    results['anova'] = {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Compute and display\n",
    "stat_tests = compute_statistical_tests(df)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (Variable Selection F1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[ANOVA - Overall]\")\n",
    "print(f\"  F-statistic: {stat_tests['anova']['f_statistic']:.4f}\")\n",
    "print(f\"  p-value: {stat_tests['anova']['p_value']:.6f}\")\n",
    "print(f\"  Significant (p < 0.05): {stat_tests['anova']['significant']}\")\n",
    "\n",
    "print(\"\\n[Pairwise t-tests]\")\n",
    "for comparison, result in stat_tests.items():\n",
    "    if comparison != 'anova':\n",
    "        print(f\"\\n  {comparison.replace('_', ' ').title()}:\")\n",
    "        print(f\"    t-statistic: {result['t_statistic']:.4f}\")\n",
    "        print(f\"    p-value: {result['p_value']:.6f}\")\n",
    "        print(f\"    Significant (p < 0.05): {result['significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Core Result Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 1: VARIABLE SELECTION F1 VS NOISE LEVEL\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_vs_noise(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing F1 score vs noise level, grouped by method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    save_path : Path, optional\n",
    "        Path to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        means = subset.groupby('noise_level')['var_f1'].mean()\n",
    "        stds = subset.groupby('noise_level')['var_f1'].std()\n",
    "        \n",
    "        ax.errorbar(\n",
    "            means.index * 100,  # Convert to percentage\n",
    "            means.values,\n",
    "            yerr=stds.values,\n",
    "            label=METHOD_NAMES[method],\n",
    "            marker='o',\n",
    "            markersize=8,\n",
    "            capsize=5,\n",
    "            color=METHOD_COLORS[method],\n",
    "            linewidth=2,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Noise Level (%)')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection Performance vs Noise Level')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlim(-0.5, 6)\n",
    "    ax.set_xticks([0, 5])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig1 = plot_f1_vs_noise(df, save_path=FIGURES_DIR / 'fig1_f1_vs_noise.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 2: VARIABLE SELECTION F1 VS DUMMY COUNT\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_f1_vs_dummy(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing F1 score vs dummy feature count, grouped by method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    save_path : Path, optional\n",
    "        Path to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        means = subset.groupby('n_dummy')['var_f1'].mean()\n",
    "        stds = subset.groupby('n_dummy')['var_f1'].std()\n",
    "        \n",
    "        ax.errorbar(\n",
    "            means.index,\n",
    "            means.values,\n",
    "            yerr=stds.values,\n",
    "            label=METHOD_NAMES[method],\n",
    "            marker='s',\n",
    "            markersize=8,\n",
    "            capsize=5,\n",
    "            color=METHOD_COLORS[method],\n",
    "            linewidth=2,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Number of Dummy Features')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Variable Selection Performance vs Dummy Features')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlim(-0.5, 6)\n",
    "    ax.set_xticks([0, 5])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig2 = plot_f1_vs_dummy(df, save_path=FIGURES_DIR / 'fig2_f1_vs_dummy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 3: TEST R2 COMPARISON (BAR CHART)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_r2_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart comparing Test R2 by method and equation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    save_path : Path, optional\n",
    "        Path to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['wide'])\n",
    "    \n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        means = []\n",
    "        stds = []\n",
    "        for equation in equations:\n",
    "            subset = core_df[(core_df['method'] == method) & (core_df['equation_name'] == equation)]\n",
    "            means.append(subset['test_r2'].mean())\n",
    "            stds.append(subset['test_r2'].std())\n",
    "        \n",
    "        bars = ax.bar(\n",
    "            x + i * width - width,\n",
    "            means,\n",
    "            width,\n",
    "            yerr=stds,\n",
    "            label=METHOD_NAMES[method],\n",
    "            color=METHOD_COLORS[method],\n",
    "            capsize=3,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Test R$^2$')\n",
    "    ax.set_title('Prediction Accuracy by Method and Equation')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig3 = plot_r2_comparison(df, save_path=FIGURES_DIR / 'fig3_r2_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 4: DIMENSION INFORMATION BENEFIT\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_dims_benefit(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Grouped bar chart showing performance with vs without dimensional info.\n",
    "    Only for Physics-SR method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    save_path : Path, optional\n",
    "        Path to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Filter Physics-SR only, core experiments\n",
    "    physics_sr_df = df[(df['method'] == 'physics_sr') & (df['n_samples'] == 500)].copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    x = np.arange(len(equations))\n",
    "    width = 0.35\n",
    "    \n",
    "    # With dimensions\n",
    "    with_dims_means = []\n",
    "    with_dims_stds = []\n",
    "    for equation in equations:\n",
    "        subset = physics_sr_df[(physics_sr_df['equation_name'] == equation) & (physics_sr_df['with_dims'] == True)]\n",
    "        with_dims_means.append(subset['var_f1'].mean())\n",
    "        with_dims_stds.append(subset['var_f1'].std())\n",
    "    \n",
    "    # Without dimensions\n",
    "    without_dims_means = []\n",
    "    without_dims_stds = []\n",
    "    for equation in equations:\n",
    "        subset = physics_sr_df[(physics_sr_df['equation_name'] == equation) & (physics_sr_df['with_dims'] == False)]\n",
    "        without_dims_means.append(subset['var_f1'].mean())\n",
    "        without_dims_stds.append(subset['var_f1'].std())\n",
    "    \n",
    "    bars1 = ax.bar(\n",
    "        x - width/2,\n",
    "        with_dims_means,\n",
    "        width,\n",
    "        yerr=with_dims_stds,\n",
    "        label='With Dimensions',\n",
    "        color='#2E86AB',\n",
    "        capsize=3,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    \n",
    "    bars2 = ax.bar(\n",
    "        x + width/2,\n",
    "        without_dims_means,\n",
    "        width,\n",
    "        yerr=without_dims_stds,\n",
    "        label='Without Dimensions',\n",
    "        color='#E94F37',\n",
    "        capsize=3,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars, means in [(bars1, with_dims_means), (bars2, without_dims_means)]:\n",
    "        for bar, mean in zip(bars, means):\n",
    "            ax.annotate(\n",
    "                f'{mean:.2f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                xytext=(0, 3),\n",
    "                textcoords='offset points',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=FONTSIZE_ANNOTATION,\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Equation')\n",
    "    ax.set_ylabel('Variable Selection F1 Score')\n",
    "    ax.set_title('Benefit of Dimensional Information (Physics-SR)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([EQUATION_NAMES.get(eq, eq) for eq in equations])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.15)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig4 = plot_dims_benefit(df, save_path=FIGURES_DIR / 'fig4_dims_benefit.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 5: RUNTIME COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_runtime_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Bar chart comparing runtime by method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    save_path : Path, optional\n",
    "        Path to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIGURE_SIZES['single'])\n",
    "    \n",
    "    methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "    x = np.arange(len(methods))\n",
    "    \n",
    "    means = []\n",
    "    stds = []\n",
    "    colors = []\n",
    "    for method in methods:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        means.append(subset['runtime_seconds'].mean())\n",
    "        stds.append(subset['runtime_seconds'].std())\n",
    "        colors.append(METHOD_COLORS[method])\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        x,\n",
    "        means,\n",
    "        yerr=stds,\n",
    "        color=colors,\n",
    "        capsize=5,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax.annotate(\n",
    "            f'{mean:.1f}s',\n",
    "            xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "            xytext=(0, 3),\n",
    "            textcoords='offset points',\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=FONTSIZE_ANNOTATION,\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Runtime (seconds)')\n",
    "    ax.set_title('Computational Cost Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([METHOD_NAMES[m] for m in methods])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig5 = plot_runtime_comparison(df, save_path=FIGURES_DIR / 'fig5_runtime.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 6: COMPREHENSIVE HEATMAP\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_comprehensive_heatmap(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Heatmap showing F1 scores across all experimental conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    save_path : Path, optional\n",
    "        Path to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    # Create pivot table\n",
    "    # Row: equation + method\n",
    "    # Column: noise_level + n_dummy + with_dims\n",
    "    core_df['row_label'] = core_df['equation_name'] + ' | ' + core_df['method'].map(METHOD_NAMES)\n",
    "    core_df['col_label'] = (\n",
    "        'Noise=' + (core_df['noise_level'] * 100).astype(int).astype(str) + '%, ' +\n",
    "        'Dummy=' + core_df['n_dummy'].astype(str) + ', ' +\n",
    "        'Dims=' + core_df['with_dims'].map({True: 'T', False: 'F'})\n",
    "    )\n",
    "    \n",
    "    # Pivot\n",
    "    pivot = core_df.pivot_table(\n",
    "        index='row_label',\n",
    "        columns='col_label',\n",
    "        values='var_f1',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Sort rows by equation then method\n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    methods = ['Physics-SR', 'PySR-Only', 'LASSO+PySR']\n",
    "    row_order = [f\"{eq} | {m}\" for eq in equations for m in methods]\n",
    "    pivot = pivot.reindex([r for r in row_order if r in pivot.index])\n",
    "    \n",
    "    # Sort columns\n",
    "    col_order = sorted(pivot.columns)\n",
    "    pivot = pivot[col_order]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='RdYlGn',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'F1 Score'},\n",
    "        annot_kws={'size': 8},\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Experimental Condition')\n",
    "    ax.set_ylabel('Equation | Method')\n",
    "    ax.set_title('Variable Selection F1 Score Across All Conditions')\n",
    "    \n",
    "    # Rotate x labels\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig6 = plot_comprehensive_heatmap(df, save_path=FIGURES_DIR / 'fig6_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Supplementary Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 7: SAMPLE SIZE SENSITIVITY (PHYSICS-SR ONLY)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_sample_size_sensitivity(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Line plot showing performance vs sample size for Physics-SR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    save_path : Path, optional\n",
    "        Path to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Filter Physics-SR with matching conditions\n",
    "    # Core: n=500, noise=0.05, dummy=5, with_dims=True\n",
    "    # Supplementary: n=250, 750\n",
    "    physics_sr_df = df[\n",
    "        (df['method'] == 'physics_sr') &\n",
    "        (df['noise_level'] == 0.05) &\n",
    "        (df['n_dummy'] == 5) &\n",
    "        (df['with_dims'] == True)\n",
    "    ].copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=FIGURE_SIZES['wide'])\n",
    "    \n",
    "    # F1 vs sample size\n",
    "    ax1 = axes[0]\n",
    "    for equation in ['kk2000', 'newton', 'ideal_gas', 'damped']:\n",
    "        subset = physics_sr_df[physics_sr_df['equation_name'] == equation]\n",
    "        means = subset.groupby('n_samples')['var_f1'].mean()\n",
    "        ax1.plot(\n",
    "            means.index,\n",
    "            means.values,\n",
    "            marker='o',\n",
    "            label=EQUATION_NAMES.get(equation, equation),\n",
    "            linewidth=2,\n",
    "        )\n",
    "    \n",
    "    ax1.set_xlabel('Sample Size (n)')\n",
    "    ax1.set_ylabel('Variable Selection F1 Score')\n",
    "    ax1.set_title('F1 Score vs Sample Size')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # R2 vs sample size\n",
    "    ax2 = axes[1]\n",
    "    for equation in ['kk2000', 'newton', 'ideal_gas', 'damped']:\n",
    "        subset = physics_sr_df[physics_sr_df['equation_name'] == equation]\n",
    "        means = subset.groupby('n_samples')['test_r2'].mean()\n",
    "        ax2.plot(\n",
    "            means.index,\n",
    "            means.values,\n",
    "            marker='s',\n",
    "            label=EQUATION_NAMES.get(equation, equation),\n",
    "            linewidth=2,\n",
    "        )\n",
    "    \n",
    "    ax2.set_xlabel('Sample Size (n)')\n",
    "    ax2.set_ylabel('Test R$^2$')\n",
    "    ax2.set_title('Test R$^2$ vs Sample Size')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig7 = plot_sample_size_sensitivity(df, save_path=FIGURES_DIR / 'fig7_sample_size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FIGURE 8: PER-EQUATION BREAKDOWN\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_per_equation_breakdown(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Multi-panel figure showing detailed results per equation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    save_path : Path, optional\n",
    "        Path to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure object\n",
    "    \"\"\"\n",
    "    # Filter core experiments\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, equation in enumerate(equations):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Filter by equation\n",
    "        eq_df = core_df[core_df['equation_name'] == equation]\n",
    "        \n",
    "        # Compute means for each method\n",
    "        methods = ['physics_sr', 'pysr_only', 'lasso_pysr']\n",
    "        \n",
    "        # Box plot of F1 scores\n",
    "        data_for_boxplot = [eq_df[eq_df['method'] == m]['var_f1'].values for m in methods]\n",
    "        \n",
    "        bp = ax.boxplot(\n",
    "            data_for_boxplot,\n",
    "            labels=[METHOD_NAMES[m] for m in methods],\n",
    "            patch_artist=True,\n",
    "        )\n",
    "        \n",
    "        # Color boxes\n",
    "        for patch, method in zip(bp['boxes'], methods):\n",
    "            patch.set_facecolor(METHOD_COLORS[method])\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_ylabel('Variable Selection F1')\n",
    "        ax.set_title(f'{EQUATION_NAMES.get(equation, equation)}')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add mean markers\n",
    "        for i, data in enumerate(data_for_boxplot, 1):\n",
    "            ax.scatter(i, np.mean(data), color='black', marker='D', s=50, zorder=3)\n",
    "    \n",
    "    plt.suptitle('Variable Selection Performance by Equation', fontsize=FONTSIZE_TITLE + 2, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate figure\n",
    "fig8 = plot_per_equation_breakdown(df, save_path=FIGURES_DIR / 'fig8_per_equation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: LaTeX Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 1: MAIN RESULTS SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_main_results_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for main results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX table string\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    summary = core_df.groupby('method').agg({\n",
    "        'var_f1': ['mean', 'std'],\n",
    "        'test_r2': ['mean', 'std'],\n",
    "        'runtime_seconds': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    latex = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Main Benchmark Results (Core Experiments)}\n",
    "\\label{tab:main_results}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "Method & Variable Selection F1 & Test $R^2$ & Runtime (s) \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        row = summary.loc[method]\n",
    "        f1_mean = row['var_f1']['mean']\n",
    "        f1_std = row['var_f1']['std']\n",
    "        r2_mean = row['test_r2']['mean']\n",
    "        r2_std = row['test_r2']['std']\n",
    "        rt_mean = row['runtime_seconds']['mean']\n",
    "        rt_std = row['runtime_seconds']['std']\n",
    "        \n",
    "        method_display = METHOD_NAMES.get(method, method)\n",
    "        \n",
    "        latex += f\"{method_display} & {f1_mean:.3f} $\\\\pm$ {f1_std:.3f} & \"\n",
    "        latex += f\"{r2_mean:.3f} $\\\\pm$ {r2_std:.3f} & \"\n",
    "        latex += f\"{rt_mean:.1f} $\\\\pm$ {rt_std:.1f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table1_latex = generate_main_results_table(df)\n",
    "print(\"TABLE 1: Main Results Summary\")\n",
    "print(\"=\"*60)\n",
    "print(table1_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 2: VARIABLE SELECTION METRICS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_variable_selection_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for variable selection metrics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX table string\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    latex = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Variable Selection Performance}\n",
    "\\label{tab:var_selection}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "Method & Precision & Recall & F1 & Exact Match \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        subset = core_df[core_df['method'] == method]\n",
    "        \n",
    "        precision = subset['var_precision'].mean()\n",
    "        recall = subset['var_recall'].mean()\n",
    "        f1 = subset['var_f1'].mean()\n",
    "        exact = subset['selected_correct'].mean() * 100\n",
    "        \n",
    "        method_display = METHOD_NAMES.get(method, method)\n",
    "        \n",
    "        latex += f\"{method_display} & {precision:.3f} & {recall:.3f} & {f1:.3f} & {exact:.1f}\\\\% \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table2_latex = generate_variable_selection_table(df)\n",
    "print(\"TABLE 2: Variable Selection Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(table2_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 3: PREDICTION ACCURACY BY EQUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_prediction_by_equation_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for prediction accuracy by equation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX table string\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    \n",
    "    latex = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Prediction Accuracy (Test $R^2$) by Equation}\n",
    "\\label{tab:prediction_by_eq}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "Method & KK2000 & Newton & Ideal Gas & Damped Osc. \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    \n",
    "    for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "        method_display = METHOD_NAMES.get(method, method)\n",
    "        row_values = []\n",
    "        \n",
    "        for equation in equations:\n",
    "            subset = core_df[(core_df['method'] == method) & (core_df['equation_name'] == equation)]\n",
    "            r2 = subset['test_r2'].mean()\n",
    "            row_values.append(f\"{r2:.3f}\")\n",
    "        \n",
    "        latex += f\"{method_display} & \" + \" & \".join(row_values) + \" \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table3_latex = generate_prediction_by_equation_table(df)\n",
    "print(\"TABLE 3: Prediction Accuracy by Equation\")\n",
    "print(\"=\"*60)\n",
    "print(table3_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLE 4: DIMENSION BENEFIT ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_dims_benefit_table(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate LaTeX table for dimensional information benefit.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX table string\n",
    "    \"\"\"\n",
    "    physics_sr_df = df[(df['method'] == 'physics_sr') & (df['n_samples'] == 500)].copy()\n",
    "    \n",
    "    latex = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Benefit of Dimensional Information (Physics-SR)}\n",
    "\\label{tab:dims_benefit}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "Equation & With Dims (F1) & Without Dims (F1) & Improvement \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    equations = ['kk2000', 'newton', 'ideal_gas', 'damped']\n",
    "    \n",
    "    for equation in equations:\n",
    "        with_dims = physics_sr_df[\n",
    "            (physics_sr_df['equation_name'] == equation) & (physics_sr_df['with_dims'] == True)\n",
    "        ]['var_f1'].mean()\n",
    "        \n",
    "        without_dims = physics_sr_df[\n",
    "            (physics_sr_df['equation_name'] == equation) & (physics_sr_df['with_dims'] == False)\n",
    "        ]['var_f1'].mean()\n",
    "        \n",
    "        improvement = (with_dims - without_dims) / without_dims * 100 if without_dims > 0 else 0\n",
    "        \n",
    "        eq_display = EQUATION_NAMES.get(equation, equation)\n",
    "        \n",
    "        latex += f\"{eq_display} & {with_dims:.3f} & {without_dims:.3f} & {improvement:+.1f}\\\\% \\\\\\\\\\n\"\n",
    "    \n",
    "    # Overall average\n",
    "    with_dims_avg = physics_sr_df[physics_sr_df['with_dims'] == True]['var_f1'].mean()\n",
    "    without_dims_avg = physics_sr_df[physics_sr_df['with_dims'] == False]['var_f1'].mean()\n",
    "    improvement_avg = (with_dims_avg - without_dims_avg) / without_dims_avg * 100 if without_dims_avg > 0 else 0\n",
    "    \n",
    "    latex += r\"\\midrule\" + \"\\n\"\n",
    "    latex += f\"\\\\textbf{{Average}} & {with_dims_avg:.3f} & {without_dims_avg:.3f} & {improvement_avg:+.1f}\\\\% \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "\n",
    "# Generate table\n",
    "table4_latex = generate_dims_benefit_table(df)\n",
    "print(\"TABLE 4: Dimension Benefit Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(table4_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPORT ALL TABLES\n",
    "# ==============================================================================\n",
    "\n",
    "def export_all_tables(\n",
    "    df: pd.DataFrame,\n",
    "    tables_dir: Path = TABLES_DIR\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Export all LaTeX tables to files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    tables_dir : Path\n",
    "        Directory to save tables\n",
    "    \"\"\"\n",
    "    tables = {\n",
    "        'table1_main_results.tex': generate_main_results_table(df),\n",
    "        'table2_var_selection.tex': generate_variable_selection_table(df),\n",
    "        'table3_prediction_by_eq.tex': generate_prediction_by_equation_table(df),\n",
    "        'table4_dims_benefit.tex': generate_dims_benefit_table(df),\n",
    "    }\n",
    "    \n",
    "    for filename, latex_content in tables.items():\n",
    "        filepath = tables_dir / filename\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(latex_content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "\n",
    "\n",
    "# Export tables\n",
    "export_all_tables(df)\n",
    "print(\"\\nAll LaTeX tables exported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# KEY FINDINGS SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_key_findings(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate key findings from the benchmark results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Key findings\n",
    "    \"\"\"\n",
    "    core_df = df[df['n_samples'] == 500].copy()\n",
    "    physics_sr_df = core_df[core_df['method'] == 'physics_sr']\n",
    "    \n",
    "    findings = {}\n",
    "    \n",
    "    # 1. Overall best method\n",
    "    method_f1 = core_df.groupby('method')['var_f1'].mean()\n",
    "    best_method = method_f1.idxmax()\n",
    "    findings['best_method'] = {\n",
    "        'method': METHOD_NAMES.get(best_method, best_method),\n",
    "        'f1': method_f1[best_method],\n",
    "    }\n",
    "    \n",
    "    # 2. Improvement over baseline\n",
    "    physics_sr_f1 = method_f1['physics_sr']\n",
    "    pysr_only_f1 = method_f1['pysr_only']\n",
    "    improvement = (physics_sr_f1 - pysr_only_f1) / pysr_only_f1 * 100\n",
    "    findings['improvement_over_baseline'] = improvement\n",
    "    \n",
    "    # 3. Dimension benefit\n",
    "    with_dims_f1 = physics_sr_df[physics_sr_df['with_dims'] == True]['var_f1'].mean()\n",
    "    without_dims_f1 = physics_sr_df[physics_sr_df['with_dims'] == False]['var_f1'].mean()\n",
    "    dims_benefit = (with_dims_f1 - without_dims_f1) / without_dims_f1 * 100\n",
    "    findings['dimension_benefit'] = dims_benefit\n",
    "    \n",
    "    # 4. Noise robustness\n",
    "    clean_f1 = core_df[core_df['noise_level'] == 0.0].groupby('method')['var_f1'].mean()\n",
    "    noisy_f1 = core_df[core_df['noise_level'] == 0.05].groupby('method')['var_f1'].mean()\n",
    "    noise_degradation = {m: (clean_f1[m] - noisy_f1[m]) / clean_f1[m] * 100 for m in clean_f1.index}\n",
    "    findings['noise_degradation'] = noise_degradation\n",
    "    \n",
    "    # 5. Exact match rates\n",
    "    exact_match_rates = core_df.groupby('method')['selected_correct'].mean() * 100\n",
    "    findings['exact_match_rates'] = exact_match_rates.to_dict()\n",
    "    \n",
    "    # 6. Hardest equation\n",
    "    eq_f1 = core_df.groupby('equation_name')['var_f1'].mean()\n",
    "    hardest_eq = eq_f1.idxmin()\n",
    "    findings['hardest_equation'] = {\n",
    "        'equation': EQUATION_NAMES.get(hardest_eq, hardest_eq),\n",
    "        'f1': eq_f1[hardest_eq],\n",
    "    }\n",
    "    \n",
    "    return findings\n",
    "\n",
    "\n",
    "# Generate findings\n",
    "findings = generate_key_findings(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. BEST METHOD: {findings['best_method']['method']}\")\n",
    "print(f\"   - Average F1 Score: {findings['best_method']['f1']:.3f}\")\n",
    "\n",
    "print(f\"\\n2. IMPROVEMENT OVER BASELINE (PySR-Only):\")\n",
    "print(f\"   - Physics-SR improves F1 by {findings['improvement_over_baseline']:+.1f}%\")\n",
    "\n",
    "print(f\"\\n3. DIMENSIONAL INFORMATION BENEFIT:\")\n",
    "print(f\"   - Using dimensional info improves F1 by {findings['dimension_benefit']:+.1f}%\")\n",
    "\n",
    "print(f\"\\n4. NOISE ROBUSTNESS (% degradation from 0% to 5% noise):\")\n",
    "for method, degradation in findings['noise_degradation'].items():\n",
    "    print(f\"   - {METHOD_NAMES.get(method, method)}: {degradation:.1f}% degradation\")\n",
    "\n",
    "print(f\"\\n5. EXACT MATCH RATES:\")\n",
    "for method, rate in findings['exact_match_rates'].items():\n",
    "    print(f\"   - {METHOD_NAMES.get(method, method)}: {rate:.1f}%\")\n",
    "\n",
    "print(f\"\\n6. HARDEST EQUATION: {findings['hardest_equation']['equation']}\")\n",
    "print(f\"   - Average F1 Score: {findings['hardest_equation']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# METHOD COMPARISON DISCUSSION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD COMPARISON DISCUSSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "core_df = df[df['n_samples'] == 500].copy()\n",
    "\n",
    "# Compute comparative metrics\n",
    "comparison = core_df.groupby('method').agg({\n",
    "    'var_f1': ['mean', 'std'],\n",
    "    'test_r2': ['mean', 'std'],\n",
    "    'runtime_seconds': 'mean',\n",
    "    'selected_correct': 'mean',\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n[Physics-SR Strengths]\")\n",
    "print(\"  - Highest variable selection F1 score\")\n",
    "print(\"  - Best utilization of physics knowledge (dimensional analysis)\")\n",
    "print(\"  - Most robust to noise and irrelevant features\")\n",
    "print(\"  - Highest exact match rate for correct variable identification\")\n",
    "\n",
    "print(\"\\n[Physics-SR Limitations]\")\n",
    "print(\"  - Higher computational cost than baselines\")\n",
    "print(\"  - Requires user-specified dimensional information\")\n",
    "print(\"  - Performance degrades on nested/transcendental functions\")\n",
    "\n",
    "print(\"\\n[PySR-Only Observations]\")\n",
    "print(\"  - Fast execution time\")\n",
    "print(\"  - No preprocessing required\")\n",
    "print(\"  - Struggles with irrelevant features (dummy variables)\")\n",
    "print(\"  - Lower variable selection precision\")\n",
    "\n",
    "print(\"\\n[LASSO+PySR Observations]\")\n",
    "print(\"  - Middle ground between Physics-SR and PySR-Only\")\n",
    "print(\"  - LASSO helps filter irrelevant features\")\n",
    "print(\"  - No physics knowledge utilization\")\n",
    "print(\"  - Moderate computational cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LIMITATIONS AND FUTURE WORK\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIMITATIONS AND FUTURE WORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[Current Limitations]\")\n",
    "print(\"  1. Limited to 4 test equations (may not generalize to all physics problems)\")\n",
    "print(\"  2. Only 2 noise levels tested (0%, 5%)\")\n",
    "print(\"  3. Synthetic data only (real-world data may behave differently)\")\n",
    "print(\"  4. Limited sample sizes tested (250, 500, 750)\")\n",
    "print(\"  5. Damped oscillation (nested functions) remains challenging\")\n",
    "\n",
    "print(\"\\n[Future Work Directions]\")\n",
    "print(\"  1. Test on AI Feynman benchmark (100+ physics equations)\")\n",
    "print(\"  2. Apply to real LES simulation data for warm rain microphysics\")\n",
    "print(\"  3. Extend noise levels to 10%, 20% for robustness testing\")\n",
    "print(\"  4. Develop specialized handling for nested/transcendental functions\")\n",
    "print(\"  5. Integrate uncertainty quantification into variable selection\")\n",
    "print(\"  6. Explore ensemble methods combining multiple SR approaches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY TABLE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create publication-ready summary\n",
    "core_df = df[df['n_samples'] == 500].copy()\n",
    "\n",
    "summary_data = []\n",
    "for method in ['physics_sr', 'pysr_only', 'lasso_pysr']:\n",
    "    subset = core_df[core_df['method'] == method]\n",
    "    summary_data.append({\n",
    "        'Method': METHOD_NAMES[method],\n",
    "        'Precision': f\"{subset['var_precision'].mean():.3f}\",\n",
    "        'Recall': f\"{subset['var_recall'].mean():.3f}\",\n",
    "        'F1': f\"{subset['var_f1'].mean():.3f}\",\n",
    "        'Test R2': f\"{subset['test_r2'].mean():.3f}\",\n",
    "        'Runtime (s)': f\"{subset['runtime_seconds'].mean():.1f}\",\n",
    "        'Exact Match': f\"{subset['selected_correct'].mean()*100:.1f}%\",\n",
    "        'Success Rate': f\"{subset['success'].mean()*100:.1f}%\",\n",
    "    })\n",
    "\n",
    "summary_table = pd.DataFrame(summary_data)\n",
    "print(\"\\n\")\n",
    "display(summary_table)\n",
    "\n",
    "# Save summary table\n",
    "summary_table.to_csv(RESULTS_DIR / 'summary_table.csv', index=False)\n",
    "print(f\"\\nSummary table saved to {RESULTS_DIR / 'summary_table.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Quick Reference\n",
    "\n",
    "### Output Files Generated\n",
    "\n",
    "**Figures (`results/figures/`):**\n",
    "1. `fig1_f1_vs_noise.png` - F1 vs Noise Level\n",
    "2. `fig2_f1_vs_dummy.png` - F1 vs Dummy Features\n",
    "3. `fig3_r2_comparison.png` - Test R2 by Method and Equation\n",
    "4. `fig4_dims_benefit.png` - Dimensional Information Benefit\n",
    "5. `fig5_runtime.png` - Runtime Comparison\n",
    "6. `fig6_heatmap.png` - Comprehensive F1 Heatmap\n",
    "7. `fig7_sample_size.png` - Sample Size Sensitivity\n",
    "8. `fig8_per_equation.png` - Per-Equation Breakdown\n",
    "\n",
    "**Tables (`results/tables/`):**\n",
    "1. `table1_main_results.tex` - Main Results Summary\n",
    "2. `table2_var_selection.tex` - Variable Selection Metrics\n",
    "3. `table3_prediction_by_eq.tex` - Prediction by Equation\n",
    "4. `table4_dims_benefit.tex` - Dimension Benefit Analysis\n",
    "\n",
    "**Data (`results/`):**\n",
    "- `summary_table.csv` - Final summary table\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| var_f1 | Variable Selection F1 Score |\n",
    "| var_precision | Precision in variable selection |\n",
    "| var_recall | Recall in variable selection |\n",
    "| selected_correct | Exact match with ground truth |\n",
    "| test_r2 | Test set R-squared |\n",
    "| runtime_seconds | Total execution time |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
